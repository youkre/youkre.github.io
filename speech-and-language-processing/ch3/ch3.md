---
title: "第3章 N元语言模型"
date: '2025-07-19T23:55:38+08:00'
---

<figure>
“你总是那么迷人！”他微笑着说道，这时我偶尔会鞠个躬，他们也注意到了一辆四匹马拉的马车，心生向往。
<figcaption>由简·奥斯汀语料的三元模型生成的随机句子</figcaption>
</figure>

正如一句老话所说：“预测很难——尤其是预测未来。”但如果尝试预测一些看起来简单得多的事情呢？比如一个人接下来会说什么词。举个例子，下面这句话接下来可能会出现什么词：

```
瓦尔登湖的水是如此美丽地……
```

你可能会觉得，可能的词是“蓝”、“绿”或者“清澈”，但不太可能是“冰箱”或“这”。在本章中，我们将通过引入**语言模型**（Language Models，简称LMs）来形式化这种直觉。语言模型能够为每一个可能的下一个词分配一个**概率**。它不仅可以为一个完整的句子分配概率，还能告诉我们，以下这段话在文本中出现的概率：

```
突然之间，我注意到三个家伙站在人行道上。
``

比下面这个词语顺序被打乱的版本出现的可能性要大得多：

```
在家伙们突然之间我注意到人行道三个的站立
```

那我们为什么要去预测下一个词，或者去计算一个句子的概率呢？其中一个原因是**生成文本**时需要选择更符合语境的词。例如，我们可以纠正一些语法或拼写错误，比如 `Their are two midterms` 中，“There”被误写成了“Their”，或者“`Everything has improve`”中，“improve”应为“improved”。因为“`There are`”比“`Their are`”更常见，“`has improved`”也比“`has improve`”更有可能出现，所以语言模型可以帮助用户选择更符合语法的表达。再比如，语音识别系统要判断你说的是“我很快就回来（I will be back soonish）”而不是“我将成为低音管盘（I will be bassoon dish）”，就需要知道“back soonish”这个组合更有可能。语言模型还可以帮助**增强和替代交流系统**（AAC）（Trnka 等，2007；Kane 等，2017）。一些身体不便、无法说话或打手语的人可以通过注视或其它动作从菜单中选择词语，语言模型可以辅助这类系统预测可能的词。

此外，词预测在自然语言处理中还有另一个重要意义：**大型语言模型**其实就是通过训练来预测词语而构建的！正如我们在第7到第9章将看到的那样，大型语言模型仅通过预测上下文中的下一个词，就能从大量数据中学到丰富的语言知识。

在本章中，我们将介绍最简单的语言模型类型：**n元语言模型**。n元（n-gram）是指由 *n* 个词组成的序列：比如两个词组成的2元（我们称为**双词模型**或**bigram**），像“水的”或“的湖”；三个词组成的3元（**三词模型**或**trigram**），如“The water of”或“water of Walden”。我们也用“n-gram”这个词来指代一种概率模型(这造成了术语上有一点模糊)，它可以根据前面的 n-1 个词来估计下一个词的概率，从而为整个词序列分配概率。

在后续章节中，我们将介绍更强大的**基于Transformer架构的神经网络大型语言模型**（见第9章）。但由于n元模型的形式化非常清晰且易于理解，我们将用它来介绍大型语言模型的一些核心概念，包括**训练集与测试集、困惑度（perplexity）、采样（sampling）以及插值（interpolation）**等。
