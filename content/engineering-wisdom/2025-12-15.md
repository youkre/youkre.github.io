---
title: "熵与概率"
summary: ""
date: 2025-12-15
---

在深度学习和自然语言处理中，我们经常会遇到“熵”（entropy）这个概念。那么，熵到底是什么呢？

**熵是信息量的一种度量**。但“信息量”又是什么？

在信息论中，香农（Shannon）提出： 

> 一个事件 $x$ 发生时所携带的信息量（也称为“自信息”，self-information）定义为：

$$
I(x) = -\log_2 p(x)
$$

### 直观理解

- 如果某事几乎必然发生（$p(x) \approx 1$），那么它发生时**不带来新信息** → $I(x) \approx 0$；
- 如果某事极不可能发生（$p(x) \approx 0$），一旦发生就**非常令人惊讶** → $I(x)$ 很大。

因此：**越意外的事件，信息量越大**。

既然熵是一种可计算的量，那么它的计算公式是什么？在介绍熵的计算之前，我们需要先理解概率论中的**随机变量**及其**期望**。

## 随机变量及其期望

在概率论中，**随机变量**（random variable）并不是一个“普通变量”，而是一个**函数**。具体来说：

- 设有一个**样本空间** $\Omega$（即所有可能实验结果的集合）；
- 随机变量 $X$ 是一个从 $\Omega$ 到某个可数集合 $\mathcal{X}$（通常是实数集或离散符号集）的函数：

$$
X: \Omega \to \mathcal{X}
$$

### 举个例子

抛两枚公平的硬币，用 H 表示正面，T 表示反面，样本空间为：

- $(H, H)$
- $(H, T)$
- $(T, H)$
- $(T, T)$

设随机变量 X 表示在一次抛掷两枚硬币的试验中，正面朝上的硬币数量，则：

- $X(H,H) = 2$
- $X(H,T) = X(T,H) = 1$
- $X(T,T) = 0$

于是，$X$ 的取值为 $\{0, 1, 2\}$

**注意**：当我们说“随机变量 X 表示正面出现的次数”，指的是在一次具体的试验中（如抛两枚硬币得到 (H,T)），X 的取值为该次试验中 H 的数量。这不是指“样本空间中有多少个结果包含 H”——后者是一个固定常数，不是随机变量。这一点对于理解概率论的后续内容非常重要。

我们可以对随机变量的可能取值指定概率。
对于上述抛硬币的例子，我们假设两枚硬币都是公平的（即 $P(H) = P(T)= \frac{1}{2}$），且两次抛掷相互独立，则每个基本结果的概率为各自概率的乘积：

$$
\begin{aligned}
P(X = 0) &= P((T,T)) \\
&= P(T) cdot T(T) \\
&= \frac{1}{2} \times \frac{1}{2} \\
&= \frac{1}{4}
\end{aligned}
$$

$$
\begin{aligned}
P(X = 1) &= P((H,T), (T,H)) \\
&= P(H, T) + P(T, H) \\
&= \frac{1}{2} \times \frac{1}{2} + \frac{1}{2} \times \frac{1}{2} \\
&= \frac{2}{4}
\end{aligned}
$$

$$
\begin{aligned}
P(X = 2) &= P((H,H)) \\
&= \frac{1}{2} \times \frac{1}{2} \\
&= \frac{1}{4}
\end{aligned}
$$

随机变量的**期望**（expected value）是其所有可能取值的**加权平均**，权重即为对应取值的概率：

$$
\mathbb{E}[X] = \sum_{x \in \mathcal{X}} p(x) \cdot x
$$

## 熵的计算

> 熵 = 信息量的期望值（即加权平均）

随机变量 $X$ 的熵 $H(X)$ 定义为其自信息 $I(X)$ 的期望：

$$
\begin{aligned}
H(X) &= \mathbb{E}[I(X)] \\
&= \sum_{x \in \mathcal{X}} p(x) \cdot I(x) \\
&= \sum_{x \in \mathcal{X}} p(x) \cdot \big( -\log_2 p(x) \big) \\
&= -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
\end{aligned}
$$

换句话说，**熵是以概率 $p(x)$ 为权重，对每个结果的信息量 $-\log_2 p(x)$ 所做的加权平均**。

这与算术平均 $\frac{1}{n} \sum x_i$ 不同——**高频事件对熵的贡献更大**。

### 举个例子：抛硬币

#### 情况一：公平硬币

- $p(\text{H}) = p(\text{T}) = 0.5$
- 信息量：$I(\text{H}) = I(\text{T}) = -\log_2 0.5 = 1$ 比特
- 熵：$H(X) = 0.5 \times 1 + 0.5 \times 1 = 1$ 比特

平均每次抛硬币带来 **1 比特**信息。

#### 情况二：偏置硬币

- $p(\text{H}) = 0.9,\; p(\text{T}) = 0.1$
- $I(\text{H}) = -\log_2 0.9 \approx 0.15$ 比特（不意外）
- $I(\text{T}) = -\log_2 0.1 \approx 3.32$ 比特（很意外）
- 熵：$H(X) = 0.9 \times 0.15 + 0.1 \times 3.32 \approx 0.47$ 比特

尽管“出现 T”的信息量很大，但由于它极少发生，**平均信息量反而更低**。

这正是“加权平均”的体现：**高频事件主导了熵的大小**。

## 小结

- 熵不是单个事件的信息量，而是**整个概率分布的平均不确定性**；
- 它衡量的是：**如果你根据这个分布不断抽样，平均每次能获得多少“新信息”**；
- 在语言模型中，熵越低，说明模型对下一个词越“确定”——这正是困惑度（perplexity）的理论基础。


<nav class="pagination justify-content-between">
<a href="../2025-12-11">为什么困惑度要用 N 次方根？</a>
<a href="../">目录</a>
<span></span>
</nav>

