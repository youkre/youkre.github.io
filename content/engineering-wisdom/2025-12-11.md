---
title: "为什么困惑度要用 N 次方根？"
summary: ""
date: 2025-12-11T15:47:05+08:00
---

翻译经典教材《Speech and Language Processing》第3版（简称 SLP）时，第三章关于语言模型困惑度（perplexity）的一段描述让我停顿了一下：

> The **perplexity** (sometimes abbreviated as PP or PPL) of a language model on aperplexity test set is the inverse probability of the test set (one over the probability of the test set), normalized by the number of words (or tokens). For this reason it’s sometimes called the per-word or per-token perplexity. We normalize by the number of words N by taking the Nth root.

翻译出来就是：

> 某个测试集上语言模型的**困惑度**（有时缩写为PP或PPL），是指测试集概率的倒数（即1除以测试集的概率），并按词数进行归一化。正因如此，它有时也被称为**每词困惑度**或**每词元困惑度**。为了按词数 N 归一化，我们对概率取 N 次方根。

初看这句“We normalize by the number of words N by taking the Nth root”，我一度困惑：为什么要“取 N 次方根”？直接除以 N 不行吗？而且，“对词数归一化”是什么意思？

后来意识到：这里其实是在计算几何平均（geometric mean）——一旦联系到这个统计概念，一切豁然开朗。今天就梳理一下，希望能帮到同样被这句话“卡住”的读者。

## 困惑度的本质：衡量“平均每词”的预测难度

语言模型给一个句子 $w_1, w_2, \dots, w_N$ 打分的方式是计算其联合概率：

$$
P(w_1, w_2, \dots, w_N) = \prod_{i=1}^N P(w_i \mid w_1, \dots, w_{i-1})
$$

这是一个连乘积。问题来了：句子越长，这个乘积就越小（因为每个因子都 ≤1），导致无法公平比较不同长度的句子。

所以我们需要一个与长度无关的指标，反映模型“平均每一步预测得有多好”。

## 为什么是“取 N 次方根”？因为这是几何平均

在乘法结构中（比如概率连乘、增长率等），“平均”应该用几何平均，而不是算术平均。

几何平均的定义是：

$$
\left( \prod_{i=1}^N x_i \right)^{1/N}
$$

应用到语言模型中，就是：

$$
\left( \prod_{i=1}^N P(w_i \mid \text{context}) \right)^{1/N}
$$

这正是原文所说的 “taking the $N$th root” —— **对整个联合概率开 $N$ 次方根**，从而得到“每词平均概率”。

归一化的对象不是“词数 N”，而是联合概率；N 只是用来做几何平均的指数。

## 为什么不能“除以 N”？

假设你错误地定义“平均概率”为 $\frac{1}{N} \cdot P(\text{sentence})$，那会怎样？

举个例子：

- 短句（2 词）：$P = 0.1 \times 0.1 = 0.01$
- 长句（4 词）：$P = 0.1^4 = 0.0001$

如果除以 $N$：

- 短句：$0.01 / 2 = 0.005$
- 长句：$0.0001 / 4 = 0.000025$ → 仍差两个数量级！

但如果用几何平均：

- 短句：$(0.01)^{1/2} = 0.1$
- 长句：$(0.0001)^{1/4} = 0.1$ → **完全一致**！

这说明：两者的“每词预测难度”其实相同，而只有几何平均能正确反映这一点。

## 小结：技术写作中的“隐含知识”

SLP 作为权威教材，行文高度凝练，常默认读者能自动识别像“取 $N$ 次方根 = 几何平均”这样的统计常识。但对初学者而言，这种“跳跃”容易造成理解断层。

主动追问一句：“这在统计学中对应什么？”——往往就是打通任督二脉的关键。

希望这篇笔记能让你下次看到 “take the $N$th root” 时，立刻想到：

> 哦，这是在算几何平均！

---

参考：Jurafsky & Martin, Speech and Language Processing, 3rd ed., Chapter 3.

标签：#自然语言处理 #语言模型 #困惑度 #几何平均 #SLP #阅读笔记


<nav class="pagination justify-content-between">
<a href="../2025-11-13">和AI聊天，其实是和自己对话</a>
<a href="../">目录</a>
<a href="../2025-12-15">熵与概率</a>
</nav>

