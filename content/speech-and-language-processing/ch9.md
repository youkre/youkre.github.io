---
title: "第 9 章 掩码语言模型"
summary: ""
date: 2025-12-19T14:08:00+08:00
---

> Larvatus prodeo（戴上面具，我继续前行）  
> ——笛卡尔

在前两章中，我们介绍了 Transformer 架构，并展示了如何将 Transformer 语言模型以**因果型**（causal）或从左至右的方式进行预训练。
本章将引入预训练语言模型的另一种范式——**双向 Transformer**（bidrectional transformer）编码器，以及其中应用最为广泛的模型：**BERT**（Devlin 等，2019）。
该模型通过**掩码语言建模**（masked language modeling）进行训练：不同于预测下一个词，而是将句子中的某个词进行掩码处理，要求模型根据其左右两侧的上下文来预测被掩码的词。
因此，这种方法使模型能够同时利用左侧和右侧的上下文信息。

我们在上一章已初步介绍了**微调**（finetuning）的概念。
本章将进一步描述一种新的微调方式：我们将这些预训练模型所学习到的 Transformer 网络作为基础，在其顶层之后添加一个神经网络分类器，并在额外的标注数据上进行训练，以完成某项下游任务，例如命名实体识别或自然语言推理。
其核心思想与之前一致：预训练阶段学习到的语言模型能够生成对词语语义的丰富表征，从而使得模型更容易学习（即“被微调以满足”）特定下游语言理解任务的需求。
这种“预训练–微调”范式正是机器学习中所谓**迁移学习**（transfer learning）的一个具体实例，即从一个任务或领域中获取知识，并将其应用于（迁移至）解决新任务。

本章引入的第二个关键概念是**上下文嵌入**（contextual embeddings）：即基于上下文的词表示方法。
第 5 章中介绍的方法（如 word2vec 或 GloVe）为词汇表中的每个唯一词 $w$ 学习一个固定的向量嵌入。
相比之下，上下文嵌入（例如由 BERT 等掩码语言模型所学习到的表示）则为同一个词 $w$ 在不同上下文中生成不同的向量表示。
虽然第 8 章中的因果语言模型也使用了上下文嵌入，但掩码语言模型所产生的嵌入在作为语义表示方面表现尤为出色。


<nav class="pagination justify-content-between">
<a href="../ch8-09">8.9 Transformer 的可解释性</a>
<a href="../">目录</a>
<a href="../ch9-01">9.1 双向 Transformer 编码器</a>
</nav>

