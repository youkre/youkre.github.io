---
title: "5.5 交叉熵损失函数"
summary: ""
date: 2025-10-06T09:59:00+08:00
---

我们需要一个损失函数，用于衡量对于一个观测样本 $x$，分类器的输出（$\hat{y} = \sigma(\mathbf{w} \cdot \mathbf{x} + b)$）与正确输出（$y$，即0或1）的接近程度。我们将这个函数表示为：

$$
L(\hat{y}, y) = \hat{y} \text{ 与真实标签 } y \text{ 的差异程度}
\tag{5.20}
$$

通过一个损失函数来实现这一点，该函数倾向于让训练样本的正确类别标签具有更高的可能性。这种方法被称为**条件最大似然估计**（conditional maximum likelihood estimation）：我们选择一组参数 $\mathbf{w}$ 和 $b$，以**最大化在给定观测值 $x$ 的条件下，训练数据中真实标签 $y$ 的对数概率**。由此得到的损失函数称为**负对数似然损失**（negative log likelihood loss），通常简称为**交叉熵损失**（cross-entropy loss）。

我们来推导这个应用于单个观测样本 $x$ 的损失函数。目标是学习一组权重，使其最大化正确标签的概率 $p(y|x)$。由于只有两种离散结果（1或0），这是一个伯努利分布（Bernoulli distribution）。可以将分类器对单个样本产生的概率 $p(y|x)$ 表示如下（注意：当 $y=1$ 时，公式 (5.21) 简化为 $\hat{y}$；当 $y=0$ 时，简化为 $1-\hat{y}$）：

$$
p(y|x) = \hat{y}^y (1 - \hat{y})^{1-y}
\tag{}
$$

(5.21)
{class="text-end"}

现在对等式两边取对数。这在数学上非常方便，且不会影响结果；因为能使概率最大化的参数值，同样也能使该概率的对数最大化：

$$
\begin{aligned}
\log p(y|x) &= \log[\hat{y}^y (1 - \hat{y})^{1-y}] \\
&= y \log \hat{y} + (1 - y) \log(1 - \hat{y})
\end{aligned}
\tag{}
$$

(5.22)
{class="text-end"}

公式 (5.22) 描述了一个应被最大化的对数似然。为了将其转换为一个损失函数（即我们需要最小化的量），只需对公式 (5.22) 取负号。结果就是交叉熵损失 $L_{CE}$：

$$
\begin{aligned}
L_{CE}(\hat{y}, y) &= -\log p(y|x) \\
&= -[y \log \hat{y} + (1 - y) \log(1 - \hat{y})]
\end{aligned}
\tag{}
$$

(5.23)
{class="text-end"}

最后，可以将 $\hat{y} = \sigma(\mathbf{w} \cdot \mathbf{x} + b)$ 的定义代入：

$$
L_{CE}(\hat{y}, y) = -[y \log \sigma(\mathbf{w} \cdot \mathbf{x} + b) + (1 - y) \log(1 - \sigma(\mathbf{w} \cdot \mathbf{x} + b))]
\tag{}
$$

(5.24)
{class="text-end"}

我们验证一下，这个损失函数是否对图5.2中的例子起到了正确的作用。我们希望当模型的预测接近正确时，损失值较小；而当模型判断错误时，损失值较大。

首先，假设图5.2中情感例子的真实标签是正面的，即 $y = 1$。在这种情况下，模型表现良好，因为根据公式 (5.8)，它确实赋予该样本更高的正面概率（0.70），高于负面概率（0.30）。如果我们将 $\sigma(\mathbf{w} \cdot \mathbf{x} + b) = 0.70$ 和 $y = 1$ 代入公式 (5.24)，等式右侧项将消失，得到以下损失值（当未指定底数时，我们用 $\log$ 表示自然对数）：

$$
\begin{aligned}
L_{CE}(\hat{y}, y) &= -[y \log \sigma(\mathbf{w} \cdot \mathbf{x} + b) + (1 - y) \log(1 - \sigma(\mathbf{w} \cdot \mathbf{x} + b))] \\
&= -[\log \sigma(\mathbf{w} \cdot \mathbf{x} + b)] \\
&= -\log(0.70) \\
&\approx 0.36
\end{aligned}
$$

相比之下，假设图5.2中的例子实际上是负面的，即 $y = 0$（也许评论者接着写道：“但归根结底，这部电影很糟糕！我恳求你不要去看！”）。在这种情况下，模型判断错误，我们希望损失值更高。现在，我们将 $y = 0$ 和 $1 - \sigma(\mathbf{w} \cdot \mathbf{x} + b) = 0.30$（来自公式 5.8）代入公式 (5.24)，等式左侧项将消失：

$$
\begin{aligned}
L_{CE}(\hat{y}, y) &= -[y \log \sigma(\mathbf{w} \cdot \mathbf{x} + b) + (1 - y) \log(1 - \sigma(\mathbf{w} \cdot \mathbf{x} + b))] \\
&= -[\log(1 - \sigma(\mathbf{w} \cdot \mathbf{x} + b))] \\
&= -\log(0.30) \\
&\approx 1.20
\end{aligned}
$$

结果确实如此：第一个分类器的损失（0.36）小于第二个分类器的损失（1.20）。

为什么最小化这个负对数概率能达到我们的目的呢？一个完美的分类器会将正确结果（$y = 1$ 或 $y = 0$）的概率设为1，错误结果的概率设为0。这意味着，如果 $y = 1$，$\hat{y}$ 越高（越接近1），分类器表现越好；$\hat{y}$ 越低（越接近0），表现越差。如果 $y = 0$，则 $1 - \hat{y}$ 越高（越接近1），分类器表现越好。$\hat{y}$ 的负对数（当真实 $y = 1$ 时）或 $1 - \hat{y}$ 的负对数（当真实 $y = 0$ 时）是一个方便的损失度量，因为它从0（1的负对数，无损失）变化到无穷大（0的负对数，无限损失）。这个损失函数还确保了在最大化正确答案概率的同时，最小化了错误答案的概率；因为两者之和为1，正确答案概率的任何增加都来自于错误答案概率的减少。它被称为交叉熵损失，因为公式 (5.22) 也是**交叉熵**（cross-entropy）的计算公式，即真实概率分布 $y$ 与我们估计的分布 $\hat{y}$ 之间的交叉熵。


<nav class="pagination justify-content-between">
<a href="../ch5-04">5.4 逻辑回归中的学习</a>
<a href="../">目录</a>
<a href="../ch5-06">5.6 梯度下降</a>
</nav>

