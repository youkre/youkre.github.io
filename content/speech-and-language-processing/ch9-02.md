---
title: "9.2 Transformer 模块"
summary: ""
date: 2025-11-02T01:02:08+08:00
---

自注意力计算是“Transformer 模块”（transformer block）的核心。
一个 Transformer 模块除了自注意力层外，还包括另外三种类型的层：(1) 前馈层（feedforward layer），(2) 残差连接（residual connections），以及 (3) 归一化层（normalizing layers，俗称“层归一化”或“layer norm”）。

图9.6 展示了一个 Transformer 模块的架构，描绘了一种理解该模块的常见方式 -- 即被称为**残差流**（residual stream）（Elhage 等人，2021）的概念。
在残差流的视角下，我们将一个单独的词元 $i$ 在 Transformer 模块中的处理过程，看作是一条贯穿始终的、针对该词元位置 $i$ 的 $d$ 维表示流。
这条残差流始于原始的输入向量，模块中的各个组件从这条流中读取输入，并将它们的输出结果加回到这条流中。

**图9.6** 展示了 Transformer 模块的架构，突出了**残差流**。此图展示的是该架构的**预归一化**（prenorm）版本，即层归一化（layer norm）发生在注意力层和前馈层**之前**，而非之后。

残差流底部的输入是一个词元的嵌入向量，其维度为 $d$。这个初始嵌入向量通过**残差连接**（residual connections）向上传递，并被 Transformer 的其他组件逐步添加信息：我们已经了解的**注意力层**，以及即将介绍的**前馈层**。
在注意力层和前馈层之前，会进行一个称为**层归一化**（layer norm）的计算。

因此，初始向量首先经过一个层归一化和注意力层，然后其结果被加回到残差流中，此处是加到原始输入向量 $\mathbf{x}_i$ 上。接着，这个求和后的向量再次经过另一个层归一化和一个前馈层，它们的输出也被加回到残差流中。我们将用 $\mathbf{h}_i$ 来表示 Transformer 模块为词元 $i$ 产生的最终输出。（在早期的描述中，残差流常被用“残差连接”这一比喻来描述，即把一个组件的输入与其输出相加；但残差流是一种更清晰的方式来可视化 Transformer 的工作过程。）

我们已经了解了注意力层，现在让我们在处理词元位置 $i$ 的单个输入 $\mathbf{x}_i$ 的背景下，介绍前馈层和层归一化的计算。

**前馈层**（Feedforward layer） 前馈层是一个全连接的两层网络，即包含一个隐藏层和两个权重矩阵，这在第7章中已有介绍。权重在每个词元位置 $i$ 上是相同的，但在不同的 Transformer 层之间是不同的。通常，前馈网络隐藏层的维度 $d_{ff}$ 会大于模型的维度 $d$。（例如，在最初的 Transformer 模型中，$d = 512$ 而 $d_{ff} = 2048$。）

$$
FFN(\mathbf{x}_i) = \text{ReLU}(\mathbf{x}_i\mathbf{W}_1 + \mathbf{b}_1)\mathbf{W}_2 + \mathbf{b}_2 \tag{9.20}
$$

**层归一化**（Layer Norm） 在 Transformer 模块的两个阶段，我们会对向量进行**归一化**（Ba 等人，2016）。这个过程称为**层归一化**（layer norm，全称 layer normalization），是多种归一化形式中的一种，通过将隐藏层的值保持在一个有利于基于梯度训练的范围内，来提升深度神经网络的训练性能。

层归一化是统计学中**z-score**（标准分数）的一种变体，应用于隐藏层中的单个向量。也就是说，“层归一化”这个术语有点令人困惑；它**不是**对整个 Transformer 层进行操作，而只是对单个词元的嵌入向量进行操作。因此，层归一化的输入是一个维度为 $d$ 的单个向量，输出是该向量的归一化版本，维度仍为 $d$。层归一化的第一步是计算待归一化向量各元素的均值 $\mu$ 和标准差 $\sigma$。给定一个维度为 $d$ 的嵌入向量 $\mathbf{x}$，这些值的计算方式如下：

$$
\mu = \frac{1}{d}\sum^d_{i=1} \mathbf{x}_i
\tag{9.21}
$$

$$
\sigma = \sqrt{\frac{1}{d}\sum^d_{i=1}(\mathbf{x}_i − \mu)^2}
\tag{9.22}
$$

有了这些值后，通过从每个向量分量中减去均值并除以标准差来对它们进行归一化。此计算的结果是一个新的、均值为零、标准差为一的向量。
$$
\hat{\mathbf{x}} = \frac{(\mathbf{x} − \mu)}{\sigma}
\tag{9.23}
$$

最后，在层归一化的标准实现中，引入了两个可学习的参数 $\gamma$ 和 $\beta$，分别代表缩放（gain）和偏移（offset）值。

$$
\text{LayerNorm}(\mathbf{x}) = \frac{\gamma (\mathbf{x} − \mu)}{\sigma} + \beta
\tag{9.24}
$$

**整合所有部分** 一个 Transformer 模块所计算的函数，可以通过将其分解为每个组件计算的单个方程来表达。我们用 **t**（形状为 $[1 \times d]$）来代表 Transformer，并用上标来标记模块内的每一次计算：

$$
\mathbf{t}^1_i = \text{LayerNorm}(\mathbf{x}_i) \tag{9.25}
$$
$$
\mathbf{t}^2_i = \text{MultiHeadAttention}(\mathbf{t}^1_i ,[\mathbf{x}_1^1,\cdots,\mathbf{x}_1^N]) \tag{9.26}
$$
$$
\mathbf{t}^3_i = \mathbf{t}^2_i +\mathbf{x}_i \tag{9.27}
$$
$$
\mathbf{t}^4_i = \text{LayerNorm}(\mathbf{t}^3_i ) \tag{9.28}
$$
$$
\mathbf{t}^5_i = \text{FFN}(\mathbf{t}^4_i ) \tag{9.29}
$$
$$
\mathbf{h}_i = \mathbf{t}^5_i +\mathbf{t}^3_i \tag{9.30}
$$

请注意，唯一一个接收来自其他词元（其他残差流）信息作为输入的组件是多头注意力，它（如公式 (9.27) 所示）会查看上下文中所有相邻的词元。然而，注意力的输出随后被加回到当前词元的嵌入流中。事实上，Elhage 等人（2021）指出，我们可以将注意力头视为字面意义上将信息从一个相邻词元的残差流移动到当前词元的残差流。因此，每个位置的高维嵌入空间既包含了关于当前词元的信息，也包含了关于相邻词元的信息，尽管这些信息位于向量空间的不同子空间中。图9.7 展示了这种信息移动的可视化。

**图9.7** 一个注意力头可以将信息从词元 A 的残差流移动到词元 B 的残差流。

至关重要的是，Transformer 模块的输入和输出维度是匹配的，以便它们可以被堆叠。每个输入到模块的词元向量 $\mathbf{x}_i$ 的维度为 $d$，输出 $\mathbf{h}_i$ 的维度也为 $d$。大型语言模型的 Transformer 会堆叠许多这样的模块，从 12 层（用于 T5 或 GPT-3-small 语言模型）到 96 层（用于 GPT-3 large），甚至更新的模型层数更多。我们稍后会再讨论这种堆叠问题。

公式 (9.27) 及其后续公式仅描述了一个单层 Transformer 模块，但残差流的比喻贯穿了所有的 Transformer 层，从第一个模块一直到一个 12 层 Transformer 的第 12 个模块。在较早的 Transformer 模块中，残差流主要表示当前词元。而在最顶层的 Transformer 模块中，残差流通常表示的是下一个词元，因为最终它被训练用来预测下一个词元。

一旦我们堆叠了许多模块，还有一个额外的要求：在最后一个（最顶层）的 Transformer 模块的末尾，会对每个词元流的最后一个 $\mathbf{h}_i$ 再运行一次额外的层归一化（就位于我们即将定义的语言模型头部层之下）。

[^3] 需要注意的是，我们这里使用的是当前最常见的一种 Transformer 架构，称为**预归一化**（prenorm）架构。Vaswani 等人（2017）最初定义的 Transformer 使用了一种称为**后归一化**（postnorm）的替代架构，即层归一化发生在注意力层和前馈层（FFN）**之后**；事实证明，将层归一化提前效果更好，但这确实需要在最后额外增加一个层归一化层。


<nav class="pagination justify-content-between">
<a href="../ch9-01">9.1 注意力机制</a>
<a href="../">目录</a>
<a href="../ch9-03">9.3 使用单个矩阵 X 并行化计算</a>
</nav>

