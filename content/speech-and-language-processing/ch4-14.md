---
title: "4.14 进阶：正则化"
summary: ""
date: 2025-10-09T10:19:00+08:00
---

> Numquam ponenda est pluralitas sine necessitate  
> “如无必要，勿增实体”
>
> —— 奥卡姆的威廉

要学习到使模型完美匹配训练数据的权重存在一个问题。
如果某个特征恰好只出现在一个类别中，并因此能完美预测结果，那么它将被赋予一个非常高的权重。
特征的权重会试图完美拟合训练集的细节，实际上这种拟合过于完美，以至于模型会学习到那些只是偶然与类别相关的噪声因素。
这个问题被称为**过拟合**（overfitting）。
一个好的模型应该能够从训练数据**泛化**（generalize）到未见过的测试集上，而一个过拟合的模型泛化能力会很差。

为了避免过拟合，我们在公式 (4.25) 的损失函数中添加一个新的**正则化项**（regularization term）$R(\theta)$，从而得到针对 $m$ 个样本批次的以下损失函数（此处稍作改写，从最小化损失变为最大化对数概率，并移除了不影响 argmax 结果的 $\frac{1}{m}$ 项）：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \sum_{i=1}^m \log P(y^{(i)}|x^{(i)}) - \alpha R(\theta)
\tag{4.48}
$$

新的正则化项 $R(\theta)$ 用于惩罚过大的权重。
因此，如果一个参数设置完美匹配了训练数据，但使用了许多高权重值，而一种设置对数据的拟合稍差、但使用了更小权重，那么前者受到的惩罚会大于后者。
正则化强度参数 $\alpha$ 的值越大，模型权重的值就会越小，降低模型对训练数据的依赖。

计算正则化项 $R(\theta)$ 有两种常用方法。
**L2 正则化**（L2 regularization）是权重值的二次函数，之所以这样命名，是因为它使用了权重值的 L2 范数（的平方）。
L2 范数 $||\theta||^2$ 等同于向量 $\theta$ 到原点的欧几里得距离。
如果 $\theta$ 包含 $n$ 个权重，则：

$$
R(\theta) = ||\theta||_2^2 = \sum_{j=1}^n \theta_j^2
\tag{4.49}
$$

L2 正则化的损失函数变为：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \left[ \sum_{i=1}^m \log P(y^{(i)}|x^{(i)}) \right] - \alpha \sum_{j=1}^n \theta_j^2
\tag{4.50}
$$

**L1 正则化**（L1 regularization）是权重值的线性函数，得名于 L1 范数 $||\theta||_1$，即权重绝对值之和，也称为**曼哈顿距离**（Manhattan distance）（曼哈顿距离是指在像纽约这样具有网格状街道的城市中，两点之间需要步行的实际距离）：

$$
R(\theta) = ||\theta||_1 = \sum_{j=1}^n |\theta_j|
\tag{4.51}
$$

L1 正则化的损失函数变为：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \left[ \sum_{i=1}^m \log P(y^{(i)}|x^{(i)}) \right] - \alpha \sum_{j=1}^n |\theta_j|
\tag{4.52}
$$

这类正则化方法源于统计学，其中 L1 正则化被称为**套索回归**（lasso regression）（Tibshirani, 1996），而 L2 正则化被称为**岭回归**（ridge regression），两者在语言处理中都很常用。
由于 L2 正则化具的导数简单（$\theta^2$ 的导数就是 $2\theta$），因此更容易优化；而 L1 正则化则更复杂（$|\theta|$ 的导数在零点不连续）。
然而，L2 正则化倾向于产生包含许多小权重的权重向量，而 L1 正则化则倾向于产生稀疏解，即少数较大的权重和大量为零的权重。
因此，L1 正则化会产生更稀疏的权重向量，也就是有效特征的数量更少。

L1 和 L2 正则化都可以从贝叶斯角度解释为对权重分布先验的约束。
L1 正则化可以看作是对权重施加了拉普拉斯先验（Laplace prior）。L2 正则化则对应于假设权重服从均值为 $µ = 0$ 的高斯分布（Gaussian distribution）。
在高斯（或正态）分布中，一个值离均值越远，其概率越低（由方差 $\sigma$ 缩放）。
通过对权重使用高斯先验，我们实际上是在表达权重更倾向于为 0 的偏好。
权重 $\theta_j$ 的高斯分布形式为：

$$
\frac{1}{\sqrt{2\pi\sigma^2_j}} \exp\left(-\frac{(\theta_j - \mu_j)^2}{2\sigma^2_j}\right)
\tag{4.53}
$$

如果我们为每个权重乘上一个关于该权重的高斯先验，那么我们实际上是在最大化以下约束：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \prod_{i=1}^m P(y^{(i)}|x^{(i)}) \times \prod_{j=1}^n \frac{1}{\sqrt{2\pi\sigma_j^2}} \exp \left(-\frac{(\theta_j - \mu_j)^2}{2\sigma_j^2}\right)
\tag{4.54}
$$

在对数空间中，假设均值 $µ = 0$ 且 $2\sigma^2 = 1$，上式等价于：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \sum_{i=1}^m \log P(y^{(i)}|x^{(i)}) - \alpha \sum_{j=1}^n \theta_j^2
\tag{4.55}
$$

这与公式 4.50 的形式相同。


<nav class="pagination justify-content-between">
<a href="../ch4-13">4.13 模型的解释</a>
<a href="../">目录</a>
<a href="../ch4-15">4.15 进阶：梯度公式的推导</a>
</nav>

