---
title: "8.6 关于采样方法的更多讨论"
summary: ""
date: 2025-12-28T11:04:00+08:00
---

下面介绍的采样方法都包含可调节的参数，用于在生成过程中权衡两个关键因素：**质量**（quality）与**多样性**（diversity）。
倾向于选择高概率词的方法，通常生成的文本被人类评价为更准确、更连贯、更符合事实，但也更容易显得枯燥、重复。
而给予中等概率词稍高权重的方法，则往往更具创造性与多样性，但可能牺牲事实性，甚至导致语义混乱或整体质量下降。

### 8.6.1 Top-k 采样（Top-k Sampling）

**Top-k 采样**是对贪心解码（greedy decoding）的一种简单推广。
它不再只选择概率最高的单个词，而是先将整个词汇表的概率分布截断，仅保留概率最高的 $k$ 个词；对这 $k$ 个词的概率重新归一化，形成一个合法的概率分布；然后根据这个新分布从中随机采样一个词。
更正式地描述如下：

1. 预先选定一个整数 $k$；
2. 对词汇表 $V$ 中的每个词，使用语言模型计算其在当前上下文下的条件概率 $p(w_t \mid \mathbf{w}_{\lt t})$；
3. 将所有词按概率从高到低排序，丢弃排名不在前 $k$ 的词；
4. 将剩余 $k$ 个词的概率值重新归一化（使其和为 1）；
5. 根据归一化后的概率，从这 $k$ 个词中随机采样一个作为输出。

当 $k = 1$ 时，top-$k$ 采样就退化为贪心解码。
而当 $k > 1$ 时，模型有时会选择并非最可能但仍然合理的词，从而在保持文本质量的同时提升生成结果的多样性。

### 8.6.2 核采样（Nucleus Sampling）或 Top-$p$ 采样

Top-$k$ 采样的一个主要问题是：$k$ 是固定值，但不同上下文中词的概率分布形状差异很大。  
例如，若设 $k = 10$，在某些上下文中，前 10 个词可能集中了绝大部分概率质量（如 95%），此时截断影响不大；但在另一些上下文中，概率分布可能非常平坦，前 10 个词加起来可能只占 30% 的概率质量，强行保留它们会引入大量低质量候选词。

为解决这一问题，Holtzman 等人（2020）提出了 **Top-p 采样**（也称**核采样**，nucleus sampling）。其核心思想不是保留固定的 $k$ 个词，而是保留累积概率达到 $p$ 的最小词集（即覆盖至少 $p$ 比例概率质量的“核心”（nucleus）部分）。
目标不变：剔除极不可能的词。
但通过基于概率质量而非词数来动态调整候选集大小，使得该方法在不同上下文中更稳健，候选池会自动扩大或缩小。

具体而言，给定条件分布 $P(w_t \mid \mathbf{w}_{\lt t})$，我们将所有词按概率从高到低排序，找到最小的词集 $V^{(p)}$，使得这些词的累积概率不小于预设阈值 $p$：

$$
\sum_{w \in V^{(p)}} P(w \mid \mathbf{w}_{\lt t}) \geq p.
\tag{8.48}
$$


<nav class="pagination justify-content-between">
<a href="../ch8-05">8.5 语言建模头部</a>
<a href="../">目录</a>
<a href="../ch8-07">8.7 训练</a>
</nav>

