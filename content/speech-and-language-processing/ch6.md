---
title: "第 6 章 神经网络"
summary: ""
date: 2025-12-17T08:23:00+08:00
---

> “当单元数量很大时，这类机器可以表现出极为复杂的行为。”
>
> ——艾伦·图灵（Alan Turing, 1948），《智能机器》，第6页

神经网络是语言处理的一项基础计算工具，其历史也相当悠久。
之所以称为“神经”网络，是因为它起源于**McCulloch-Pitts神经元**（McCulloch 和 Pitts，1943）。这是一种基于生物神经元的简化模型，，将神经元视为一种计算单元，并可用命题逻辑来描述。
然而，现代语言处理中使用的神经网络已不再依赖这些早期的生物学启发。

如今的神经网络是由大量小型计算单元组成的网络。每个单元接收一个输入值向量，并输出一个单一数值。
本章将介绍用于分类任务的神经网络。
我们所采用的架构称为**前馈网络**（feedforward network），因为其计算过程是从一层单元逐层向前传递到下一层。
现代神经网络通常被称为**深度学习**（deep learning），这是因为现代网络往往是**深层的**（即包含许多层）。

神经网络与逻辑回归在数学上有很多共通之处。
但神经网络是一种比逻辑回归更强大的分类器。事实上，即使是最小的神经网络（从技术上讲，仅含一个“隐藏层”的网络）也能被证明可以学习任意函数。

神经网络分类器与逻辑回归还存在另一重要区别。
在逻辑回归中，我们通过基于领域知识设计丰富多样的特征模板，将该分类器应用于多种不同任务。
而在使用神经网络时，则通常避免大量依赖人工构造的复杂特征，转而构建直接以原始词（raw words）作为输入的网络，并在学习分类的过程中自动习得特征表示。
我们在第6章中已经看到过这种表示学习（representation learning）的典型例子——词嵌入（embeddings）。
特别地，深层网络在表示学习方面表现尤为出色。
正因如此，深度神经网络成为那些拥有充足数据、足以支持自动特征学习的任务的理想工具。

神经网络分类器与逻辑回归还有另一点不同。
使用逻辑回归时，我们通常基于领域知识设计出丰富多样的特征模板，从而将其应用于多种任务。
而使用神经网络时，则更倾向于避免大量手工构造的复杂特征。取而代之的是，构建直接以原始词语作为输入的神经网络，并让网络在学习分类的过程中自动学习特征表示。
我们在第 5 章中已经看到过这种表示学习的例子——词嵌入（embeddings），一旦开始学些深度 transformer 网络，我们会看到大量此类例子。
特别地，非常深的网络在表示学习方面表现尤为出色。
因此，对于拥有足够数据、能够自动学习特征的任务，深度神经网络正是合适的工具。

本章将介绍作为分类器的前馈网络，并将其应用于一个简单的语言建模任务：为词序列分配概率，并预测下一个词。
在后续章节中，我们将进一步探讨神经模型的诸多其他方面，例如**循环神经网络**（第8章）、**Transformer**（第9章）以及**掩码语言建模**（第11章）。

本章将介绍使用前馈网络的分类器，首先使用手工构造的特征，然后使用使用第 5 章所学的词嵌入。
在后续章节中，我们将介绍更多类型的神经网络模型，其中最重要的是**Transformer**和**注意力机制**（第8章），此外还包括**循环神经网络**（第13章）和**卷积神经网络**（第15章）。
下一章则将引入神经大语言模型这一范式。


<nav class="pagination justify-content-between">
<a href="../ch5-09">5.9 向量模型的评估</a>
<a href="../">目录</a>
<a href="../ch6-01">6.1 单元</a>
</nav>

