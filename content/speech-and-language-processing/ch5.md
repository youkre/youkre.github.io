---
title: ""
summary: ""
date: 2025-10-03T08:29:00+08:00
---

> “你怎么知道这些漂亮的秋海棠不也同样重要呢？”
> ——阿加莎·克里斯蒂《斯泰尔斯庄园奇案》中赫尔克里·波洛的台词

侦探小说中充斥着线索，正如文本中充斥着词语。然而，对普通读者而言，如何权衡作者埋下的线索，以完成关键的分类任务——判断“凶手是谁”，却可能极具挑战。

在本章中，我们将介绍一种非常适合揭示特征（或线索）与特定结果之间关联的算法：**逻辑回归（logistic regression）**。事实上，逻辑回归是社会科学和自然科学领域中最重要的分析工具之一。在自然语言处理中，逻辑回归是分类任务的基线监督学习算法，并且与神经网络有着非常紧密的联系。正如我们将在第7章看到的那样，神经网络可以被看作是一系列堆叠在一起的逻辑回归分类器。因此，本章介绍的分类与机器学习技术将在本书后续内容中发挥重要作用。

逻辑回归可用于将一个观测样本划分到两个类别之一（例如“正面情感”和“负面情感”），也可用于多类别分类。由于两类情况下的数学推导更为简单，我们将在接下来的几节中首先介绍这种二分类逻辑回归，然后在第5.3节简要总结用于多于两类情况的**多项逻辑回归（multinomial logistic regression）**。我们将在接下来的几节中介绍逻辑回归的数学原理，但首先让我们从一些高层次的概念开始。

**生成式与判别式分类器**：朴素贝叶斯与逻辑回归之间最重要的区别在于，逻辑回归是一种**判别式分类器（discriminative classifier）**，而朴素贝叶斯是一种**生成式分类器（generative classifier）**。

这是构建机器学习模型的两种截然不同的框架。让我们用一个视觉比喻来理解：假设我们试图区分狗的图片和猫的图片。一个生成式模型的目标是理解狗长什么样、猫长什么样。你甚至可以要求这样的模型“生成”（即画出）一只狗。当面对一张测试图片时，系统会判断是“猫模型”还是“狗模型”能更好地拟合（即对图片“惊讶程度更低”）这张图，并据此选择标签。

相比之下，一个判别式模型只关心如何**区分**这两个类别（可能并不真正深入理解它们）。例如，训练数据中的所有狗都戴着项圈，而猫没有。如果这一特征能完美地区分两类，模型就满足了。如果你问这样的模型它对猫的了解，它唯一能回答的就是“猫不戴项圈”。

更正式地说，回顾一下，朴素贝叶斯并不是直接计算 $P(c|d)$ 来为文档 $d$ 分配类别 $c$，而是通过计算似然和先验：

$$
\hat{c} = \underset{c \in C}{\mathrm{argmax}}\, \overbrace{P(d|c)}^{\text{似然}} \overbrace{P(c)}^{\text{先验}}
$$

(5.1)
{class="text-center"}

像朴素贝叶斯这样的**生成式模型**会利用这个**似然项**，它表达了“如果我们知道文档属于类别 $c$，该如何生成文档的特征”。

相比之下，在文本分类场景中，一个**判别式模型**会尝试**直接**计算 $P(c|d)$。它可能会学习为那些能直接提升其**区分**不同类别能力的文档特征赋予高权重，即使它无法生成某一类别的示例。

**概率机器学习分类器的组成部分**：与朴素贝叶斯类似，逻辑回归也是一种利用监督学习的概率分类器。机器学习分类器需要一个包含 $m$ 个输入/输出对 $(x^{(i)}, y^{(i)})$ 的训练语料库。（我们使用带括号的上标来指代训练集中的单个实例——对于情感分类任务，每个实例可能就是一篇待分类的文档。）一个用于分类的机器学习系统通常包含以下四个组成部分：

1. **输入的特征表示（feature representation）**：对于每个输入观测 $x^{(i)}$，这将是一个特征向量 $[x_1, x_2, ..., x_n]$。我们通常将输入 $x^{(j)}$ 的第 $i$ 个特征表示为 $x_i^{(j)}$（有时简写为 $x_i$），但也可能看到记号 $f_i$、$f_i^{(x)}$，或在多类别分类中表示为 $f_i(c,x)$。

2. **分类函数（classification function）**：该函数通过计算 $p(y|x)$ 来得出预测类别 $\hat{y}$。在下一节中，我们将介绍用于分类的**Sigmoid函数**和**Softmax函数**。

3. **目标函数（objective function）**：这是我们希望在学习过程中优化的目标，通常涉及最小化一个对应于训练样本误差的**损失函数（loss function）**。我们将引入**交叉熵损失函数（cross-entropy loss function）**。

4. **优化目标函数的算法**：我们将介绍**随机梯度下降（stochastic gradient descent）**算法。

逻辑回归包含两个阶段：

**训练阶段**：我们使用随机梯度下降算法和交叉熵损失函数来训练系统（具体来说，是训练下文将介绍的权重 $w$ 和偏置 $b$）。

**测试阶段**：给定一个测试样本 $x$，我们计算 $p(y|x)$，并返回概率较高的类别标签 $y = 1$ 或 $y = 0$。


<nav class="pagination justify-content-between">
<a href="../ch4-03">4.3 实例演示</a>
<a href="../">目录</a>
<a href="../ch5-01"></a>
</nav>

