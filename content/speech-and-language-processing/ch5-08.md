---
title: "5.8 多项逻辑回归中的学习"
summary: ""
date: 2025-10-10T08:46:00.000Z
---

多项逻辑回归的损失函数将二元逻辑回归的损失函数从 2 个类别推广到了 $K$ 个类别。回顾一下，二元逻辑回归的交叉熵损失函数（重复自公式 5.23）为：

$$
L_{CE}(\hat{y}, y) = -\log p(y|x) = -[y \log \hat{y} + (1 - y)\log(1 - \hat{y})]
\tag{5.44}
$$

多项逻辑回归的损失函数将公式 (5.44) 中的两项（一项在 $y = 1$ 时非零，另一项在 $y = 0$ 时非零）推广为 $K$ 项。如上所述，对于多项回归，将 $y$ 和 $\hat{y}$ 都表示为向量。真实标签 $y$ 是一个包含 $K$ 个元素的向量，每个元素对应一个类别，当正确类别为 $c$ 时，$y_c = 1$，而 $y$ 的其余所有元素均为 0。分类器将生成一个包含 $K$ 个元素的估计向量 $\hat{y}$，其中每个元素 $\hat{y}_k$ 表示估计的概率 $p(y_k = 1|x)$。

把二元逻辑回归推广，可以得到单个样本 $x$ 的损失函数，它是 $K$ 个输出类别的对数之和，每个项由指示函数 $y_k$ 加权（公式 5.45）。这实际上就等于正确类别 $c$ 的负对数概率（公式 5.46）：

$$
\begin{aligned}
L_{CE}(\hat{y}, y) = -\sum_{k=1}^K y_k \log \hat{y}_k \tag{5.45} \\
= -\log \hat{y}_c \quad \text{（其中 } c \text{ 是正确类别）} \tag{5.46} \\
= -\log \hat{p}(y_c = 1|x) \quad \text{（其中 } c \text{ 是正确类别）} \\
= -\log \frac{\exp(\mathbf{w}_c \cdot \mathbf{x} + b_c)}{\sum_{j=1}^K \exp(\mathbf{w}_j \cdot \mathbf{x} + b_j)} \quad \text{（} c \text{ 是正确类别）}  \tag{5.47}
\end{aligned}
$$

我们是如何从公式 (5.45) 得到公式 (5.46) 的呢？因为只有一个类别（记为 $c$）是正确的，向量 $\mathbf{y}$ 仅在 $k = c$ 时取值为 1，即 $y_c = 1$，而对于所有 $j \neq c$，有 $y_j = 0$。这意味着公式 (5.45) 求和中的所有项都为 0，除了对应正确类别 $c$ 的那一项。因此，交叉熵损失就是对应正确类别的输出概率的对数的负值，因此也将公式 (5.46) 称为**负对数似然损失**（negative log likelihood loss）。

当然，对于梯度下降，我们不需要损失本身，而是需要它的梯度。单个样本的梯度与公式 (5.30) 中看到的二元逻辑回归梯度 $(\hat{y} - y)x$ 非常相似。考虑梯度的一部分，即对单个权重的偏导数。对于每个类别 $k$，输入 $x$ 的第 $i$ 个元素的权重为 $w_{k,i}$。损失函数对 $w_{k,i}$ 的偏导数是多少？这个导数实际上是类别 $k$ 的真实值（为 1 或 0）与分类器输出的类别 $k$ 的概率之间的差值，再乘以对应于类别 $k$ 的权重向量中第 $i$ 个元素的输入值 $x_i$：

$$
\begin{aligned}
\frac{\partial L_{CE}}{\partial w_{k,i}} &= - (y_k - \hat{y}_k) x_i \\
&= - (y_k - p(y_k = 1|x)) x_i \\
&= - \left(y_k - \frac{\exp(\mathbf{w}_k \cdot \mathbf{x} + b_k)}{\sum_{j=1}^K \exp(\mathbf{w}_j \cdot \mathbf{x} + b_j)}\right) x_i
\end{aligned}
\tag{5.48}
$$

第7章介绍神经网络时，将再次讨论 softmax 回归的梯度，介绍该梯度的推导过程（见公式 7.33–公式 7.41）。


<nav class="pagination justify-content-between">
<a href="../ch5-07">5.7 正则化</a>
<a href="../">目录</a>
<a href="../ch5-09">5.9 模型的解释</a>
</nav>

