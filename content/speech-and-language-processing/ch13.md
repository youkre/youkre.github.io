---
title: "第 13 章 循环神经网络与长短期记忆网络"
summary: ""
date: 2025-12-18T07:51:00+08:00
---

> 时间自会说明一切。
>
> ——简·奥斯汀，《劝导》

语言本质上是一种时间性现象。
口语是一连串随时间展开的声学事件，而我们在理解和生成口语及书面语时，也都是将其视为一个顺序输入流。
我们所使用的隐喻也体现了语言的时间特性：例如，我们谈论“对话的流动”、“新闻推送”和“推文流”，这些说法都强调了语言是一种随时间逐步展开的序列。

本章将介绍一种深度学习架构：循环神经网络（Recurrent Neural Networks, RNN）及其变体，如长短期记忆网络（Long Short-Term Memory networks, LSTM），它们表示时间的方式与前馈网络和 transformer 网络不通过。
RNN 有一种机制可以直接处理语言的序列特性，使其无需依赖任意设定的固定窗口即可应对语言的时间性。
循环网络通过其**循环连接**提供了一种全新的方式来表示先前的上下文信息，从而使模型的决策能够依赖于数百个词之前的上下文。
我们将看到如何将该模型应用于语言建模、序列标注任务（如词性标注）以及文本分类任务（如情感分析）。


<nav class="pagination justify-content-between">
<a href="../ch12-07">12.7 偏见与伦理问题</a>
<a href="../">目录</a>
<a href="../ch13-01">13.1 循环神经网络</a>
</nav>

