---
title: "11.5 问答系统的评估"
summary: ""
date: 2025-12-30T11:52:00+08:00
---

评估问答（Question Answering, QA）系统通常采用三种技术，具体选择取决于问题类型和问答场景。
对于 **多项选择题**（如 MMLU 中的问题），我们使用精确匹配（Exact Match）作为评价指标：

> **精确匹配**（Exact Match）：预测答案与标准答案完全一致的百分比。

对于具有**自由文本答案**的问题（如 Natural Questions），通常采用**词元 F₁ 分数**（token F₁ score）来粗略衡量预测答案与参考答案之间的**部分字符串重叠程度**：

> **F₁ 分数**（F₁ Score）：预测答案与标准答案之间的平均词元重叠度。将预测答案和标准答案分别视为词元袋（bag of tokens），为每个问题计算 F₁ 值，再对所有问题取平均。

最后，在某些场景中，问答系统会返回多个**排序后的答案**。
此时，我们采用**平均倒数排名**（Mean Reciprocal Rank, MRR）进行评估（Voorhees, 1999）。
MRR 适用于那些为每个测试问题返回一个**短排序列表**（如候选答案或段落）的系统，并可将该列表与人工标注的正确答案进行比较。
具体而言，对每个测试问题，其得分是首个正确答案所在排名的倒数。
例如，若系统为某问题返回了五个答案，但前三个均错误（即排名最高的正确答案位于第 4 位），则该问题的倒数排名得分为 $\frac{1}{4}$。
若系统未返回任何正确答案，则得分为 0。
系统的 MRR 即为所有测试问题得分的平均值。
在某些 MRR 的变体中，得分为 0 的问题会被排除在平均计算之外。
更形式化地，设系统对测试集 $Q$ 中的每个问题返回一个排序答案列表（或在变体中，令 $Q$ 表示测试集中得分非零的问题子集），则 MRR 定义为：

$$
MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
\tag{11.20}
$$


<nav class="pagination justify-content-between">
<a href="../ch11-04">11.4 问答数据集</a>
<a href="../">目录</a>
<a href="../ch12">第 12 章 机器翻译</a>
</nav>

