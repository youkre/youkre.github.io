---
title: "6.10 Semantic properties of embeddings"
summary: ""
date: 
---

In this section we brieﬂy summarize some of the semantic properties of embeddings
that have been studied.

**Different types of similarity or association**: One parameter of vector semantic
models that is relevant to both sparse tf-idf vectors and dense word2vec vectors is
the size of the context window used to collect counts. This is generally between 1
and 10 words on each side of the target word (for a total context of 2-20 words).

The choice depends on the goals of the representation. Shorter context windows
tend to lead to representations that are a bit more syntactic, since the information is
coming from immediately nearby words. When the vectors are computed from short
context windows, the most similar words to a target word w tend to be semantically
similar words with the same parts of speech. When vectors are computed from long
context windows, the highest cosine words to a target word w tend to be words that
are topically related but not similar.

For example Levy and Goldberg (2014a) showed that using skip-gram with a
window of ±2, the most similar words to the word *Hogwarts* (from the *Harry Potter*
series) were names of other ﬁctional schools: *Sunnydale* (from *Buffy the Vampire*
Slayer) or *Evernight* (from a vampire series). With a window of±5, the most similar
words to *Hogwarts* were other words topically related to the *Harry Potter* series:
*Dumbledore*, *Malfoy*, and *half-blood*.

It’s also often useful to distinguish two kinds of similarity or association between
words (Sch ¨utze and Pedersen, 1993). Two words have **ﬁrst-order co-occurrence**
(sometimes called **syntagmatic association**) if they are typically nearby each other.
Thus wrote is a ﬁrst-order associate of *book* or *poem*. Two words have **second-order co-occurrence**
(sometimes called **paradigmatic association**) if they have similar
neighbors. Thus *wrote* is a second-order associate of words like said or remarked.

**Analogy/Relational** Similarity: Another semantic property of embeddings is their
ability to capture relational meanings. In an important early vector space model of
cognition, Rumelhart and Abrahamson (1973) proposed the **parallelogram model**
for solving simple analogy problems of the form *a is to b as a\* is to what?* . In
such problems, a system is given a problem like *apple:tree::grape:?*, i.e., 
*apple is to tree as grape is to __*, and must ﬁll in the word vine. In the parallelogram
model, illustrated in Fig. 6.15, the vector from the word *apple* to the word *tree* 
(= $\overrightarrow{tree}$ −- $\overrightarrow{apple}$) is added to the vector for *grape* ($\overrightarrow{grape}$); the nearest word to that point
is returned.

In early work with sparse embeddings, scholars showed that sparse vector mod-
els of meaning could solve such analogy problems (Turney and Littman, 2005),
but the parallelogram method received more modern attention because of its suc-
cess with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and Goldberg
2014b, Pennington et al. 2014). For example, the result of the expression $\overrightarrow{king}$ −
$\overrightarrow{man}$ + $\overrightarrow{woman}$ is a vector close to $\overrightarrow{queen}$. Similarly, $\overrightarrow{Paris}$ − $\overrightarrow{France}$ + $\overrightarrow{Italy}$ results
in a vector that is close to $\overrightarrow{Rome}$. The embedding model thus seems to be extract-
ing representations of relations like MALE -FEMALE , or CAPITAL -CITY-OF, or even
COMPARATIVE /SUPERLATIVE , as shown in Fig. 6.16 from GloVe.

Figure 6.15 The parallelogram model for analogy problems (Rumelhart and Abrahamson,
1973): the location of $\overrightarrow{vine}$ can be found by subtracting $\overrightarrow{apple}$ from $\overrightarrow{tree}$ and adding $\overrightarrow{grape}$.

Figure 6.16 Relational properties of the GloVe vector space, shown by projecting vectors onto two dimensions. (a) $\overrightarrow{king}$ − $\overrightarrow{man}$ + $\overrightarrow{woman}$ is close to $\overrightarrow{queen}$. (b) offsets seem to capture comparative and superlative morphology (Pennington et al., 2014).

For a  $a : b::a^* : b^*$ problem, meaning the algorithm is given vectors **a**, **b**, and
$a^*$ and must ﬁnd $b^*$, the parallelogram method is thus:

$$
\hat{b^*}= argmin_x distance(x, b−a+a^*)
\tag{6.41}
$$

with some distance function, such as Euclidean distance.

There are some caveats. For example, the closest value returned by the paral-
lelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact
$b^*$ but one of the 3 input words or their morphological variants (i.e., *cherry:red :: potato:x*
returns *potato* or *potatoes* instead of *brown*), so these must be explicitly
excluded. Furthermore while embedding spaces perform well if the task involves
frequent words, small distances, and certain relations (like relating countries with
their capitals or verbs/nouns with their inﬂected forms), the parallelogram method
with embeddings doesn’t work as well for other relations (Linzen 2016, Gladkova
et al. 2016, Schluter 2018, Ethayarajh et al. 2019a), and indeed Peterson et al. (2020)
argue that the parallelogram method is in general too simple to model the human
cognitive process of forming analogies of this kind.

### 6.10.1 Embeddings and Historical Semantics

Embeddings can also be a useful tool for studying how meaning changes over time,
by computing multiple embedding spaces, each from texts written in a particular
time period. For example Fig. 6.17 shows a visualization of changes in meaning in
English words over the last two centuries, computed by building separate embed-
ding spaces for each decade from historical corpora like Google n-grams (Lin et al.,
2012b) and the Corpus of Historical American English (Davies, 2012).

**Figure 6.17** A t-SNE visualization of the semantic change of 3 words in English using
word2vec vectors. The modern sense of each word, and the grey context words, are com-
puted from the most recent (modern) time-point embedding space. Earlier points are com-
puted from earlier historical embedding spaces. The visualizations show the changes in the
word *gay* from meanings related to “cheerful” or “frolicsome” to referring to homosexuality,
the development of the modern “transmission” sense of *broadcast* from its original sense of
sowing seeds, and the pejoration of the word *awful* as it shifted from meaning “full of awe”
to meaning “terrible or appalling” (Hamilton et al., 2016b).


<nav class="pagination justify-content-between">
<a href="../ch6-09">6.9 嵌入向量的可视化</a>
<a href="../">目录</a>
<a href="../ch6-11">6.11 Bias and Embeddings</a>
</nav>

