---
title: "6.3 词语与向量"
summary: ""
date: 2025-10-13T09:57:00+08:00
---

> “三维空间中向量最重要的属性是：{位置，位置，位置}”
>
> ——Randall Munroe，https://xkcd.com/2358/

向量或分布模型的意义通常基于**共现矩阵**（co-occurrence matrix），这是一种表示词语共现频率的方式。我们将介绍两种常用的矩阵：**词-文档矩阵**（term-document matrix）和**词-词矩阵**（term-term matrix）。

### 6.3.1 向量与文档

在**词-文档矩阵**中，每一行代表词汇表中的一个词，每一列代表某个文档集合中的一篇文档。图6.2展示了一个词-文档矩阵的小型示例，显示了四个词在莎士比亚四部戏剧中的出现次数。该矩阵中的每个单元格表示某个特定词（由行定义）在某篇特定文档（由列定义）中出现的次数。因此，“fool”（傻瓜）在《第十二夜》（Twelfth Night）中出现了58次。

| | 皆大欢喜 (As You Like It) | 第十二夜 (Twelfth Night) | 凯撒大帝 (Julius Caesar) | 亨利五世 (Henry V) |
| :--- | :--- | :--- | :--- | :--- |
| battle（战斗） | 1 | 0 | 7 | 13 |
| good（好） | 114 | 80 | 62 | 89 |
| fool（傻瓜） | 36 | 58 | 1 | 4 |
| wit（机智） | 20 | 15 | 2 | 3 |

**图6.2** 四个词在莎士比亚四部戏剧中的词-文档矩阵。每个单元格包含（行）词在（列）文档中出现的次数。

图6.2中的词-文档矩阵最初是作为**信息检索的向量空间模型**（vector space model of information retrieval）的一部分提出的（Salton, 1971）。在这个模型中，一篇文档被表示为一个计数向量，即图6.3中的每一列。

回顾一些基本的线性代数知识：**向量**本质上只是一组数字的列表或数组。因此，《皆大欢喜》被表示为列表 `[1,114,36,20]`（图6.3中的第一个**列向量**），而《凯撒大帝》则被表示为列表 `[7,62,1,2]`（第三个列向量）。**向量空间**是一组向量的集合，其特征由其**维度**决定。三维向量空间中的向量在每个维度上都有一个元素。我们通常将四维空间中的向量称为“四维向量”，它在每个维度上都有一个元素。在图6.3的示例中，我们选择将文档向量设为四维，只是为了便于在页面上展示；在实际的词-文档矩阵中，文档向量的维度是 $|V|$，即词汇表的大小。

向量空间中数字的顺序表示文档在不同维度上的变化。这两个向量的第一个维度都对应于“battle”（战斗）一词的出现次数，我们可以比较每个维度，例如注意到《皆大欢喜》和《第十二夜》的向量在第一个维度上的值相似（分别为1和0）。

![](/images/speech-and-language-processing/slp-fig-6-3.png)

**图6.3** 莎士比亚四部戏剧的词-文档矩阵。红色框表示每篇文档被表示为一个长度为四的列向量。

我们可以将文档的向量看作是 $|V|$ 维空间中的一个点；因此，图6.3中的文档是四维空间中的点。由于四维空间难以可视化，图6.4展示了一个二维的可视化图；我们任意选择了对应于“battle”（战斗）和“fool”（傻瓜）这两个词的维度。

词-文档矩阵最初被定义为一种寻找相似文档的方法，用于**信息检索**（information retrieval）任务。两篇相似的文档往往包含相似的词语，如果两篇文档包含相似的词语，那么它们的列向量也会相似。喜剧《皆大欢喜》`[1,114,36,20]` 和《第十二夜》`[0,80,58,15]` 的向量看起来非常相似（傻瓜和机智多，战斗少），而与《凯撒大帝》`[7,62,1,2]` 或《亨利五世》`[13,89,4,3]` 相比则差异很大。仅从原始数字就能看出这一点：在第一个维度（战斗）上，喜剧的数值较低，而其他两部剧的数值较高，这一点在图6.4中也能直观地看到；我们很快就会介绍如何更正式地量化这种直觉。

![](/images/speech-and-language-processing/slp-fig-6-4.png)

**图6.4** 四部莎士比亚戏剧文档向量的空间可视化，仅展示两个维度，分别对应“battle”（战斗）和“fool”（傻瓜）两个词。喜剧在“fool”维度上值较高，在“battle”维度上值较低。

当然，真实的词-文档矩阵不会只有4行4列，更不用说2维了。更一般地说，词-文档矩阵有 $|V|$ 行（词汇表中每个词型对应一行）和 $D$ 列（文档集合中每篇文档对应一列）；正如我们将看到的，词汇表大小通常在数万级别，而文档数量可能极为庞大（想想网络上的所有网页）。

**信息检索**（IR）的任务是从某个文档集合的 $D$ 篇文档中，找到最匹配查询 $q$ 的文档 $d$。因此，在信息检索中，我们也会用一个向量来表示查询，该向量长度也为 $|V|$，并且需要一种方法来比较两个向量以确定它们的相似程度。（进行信息检索还需要高效地存储和操作这些向量，这得益于这些向量是**稀疏**的，即大部分元素为零这一便利特性。）

在本章后面，我们将介绍这种向量比较过程的一些组成部分：tf-idf 词项加权和余弦相似度度量。

### 6.3.2 词语作为向量：以文档为维度

我们已经看到，文档可以在向量空间中被表示为向量。但向量语义也可以用来表示**词语**的意义。通过将每个词与一个词向量相关联来实现这一点——这次是**行向量**而非列向量，因此其维度也不同，如图6.5所示。“fool”一词的向量 `[36,58,1,4]` 的四个维度分别对应四部莎士比亚戏剧。在相同的四个维度上，用词频计数构成其他三个词的向量：“wit”为 `[20,15,2,3]`；“battle”为 `[1,0,7,13]`；“good”为 `[114,80,62,89]`。

![](/images/speech-and-language-processing/slp-fig-6-5.png)

**图6.5** 莎士比亚四部戏剧的词-文档矩阵。红色框表示每个词被表示为一个长度为四的行向量。

对于文档，我们看到相似的文档具有相似的向量，因为相似的文档往往包含相似的词语。同样的原理也适用于词语：相似的词语具有相似的向量，因为它们倾向于出现在相似的文档中。因此，词-文档矩阵使我们能够通过一个词常出现的文档来表示其意义。

### 6.3.3 词语作为向量：以词语为维度

除了使用词-文档矩阵将词语表示为文档计数的向量外，另一种方法是使用**词-词矩阵**（term-term matrix），也称为**词-词矩阵**（word-word matrix）或**词-上下文矩阵**（term-context matrix），其中列由词语而非文档标记。因此，该矩阵的维度为 $|V| \times |V|$，每个单元格记录行（目标）词和列（上下文）词在某个训练语料库的某个上下文中共同出现的次数。上下文可以是整篇文档，此时单元格表示两个词在同一篇文档中出现的次数。但更常见的是使用更小的上下文，通常是词周围的窗口，例如左右各4个词，此时单元格表示（在某个训练语料库中）列词在行词周围±4个词的窗口内出现的次数。以下是四个词语及其上下文窗口的例子：

> is traditionally followed by **cherry** pie, a traditional dessert
>
> often mixed, such as **strawberry** rhubarb pie. Apple pie
>
> computer peripherals and personal **digital** assistants. These devices usually
>
> a computer. This includes **information** available on the internet

如果我们将每个词（例如 **strawberry**）的每次出现都提取出来，并统计其周围的上下文词，就可以得到一个词-词共现矩阵。图6.6展示了一个从维基百科语料库（Davies, 2015）计算出的词-词共现矩阵的简化子集。

| | aardvark ... | computer | data | result | pie | sugar | ... |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **cherry** | 0 ... | 2 | 8 | 9 | 442 | 25 | ... |
| **strawberry** | 0 ... | 0 | 0 | 1 | 60 | 19 | ... |
| **digital** | 0 ... | 1670 | 1683 | 85 | 5 | 4 | ... |
| **information** | 0 ... | 3325 | 3982 | 378 | 5 | 13 | ... |

**图6.6** 维基百科语料库中四个词的共现向量，展示了六个维度（为教学目的手动挑选）。"digital" 的向量用红色标出。请注意，真实的向量会有更多维度，因此会更加稀疏。

注意图6.6中，“cherry”和“strawberry”彼此更相似（“pie”和“sugar”都倾向于出现在它们的窗口中），而与其他词（如“digital”）相比则差异较大；相反，“digital”和“information”彼此更相似，而与“strawberry”等词相比则差异较大。图6.7展示了一个空间可视化图。

![](/images/speech-and-language-processing/slp-fig-6-7.png)

**图6.7** “digital”和“information”的词向量空间可视化，仅展示两个维度，分别对应“data”和“computer”两个词。

请注意，向量的维度 $|V|$ 通常是词汇表的大小，通常在10,000到50,000个词之间（使用训练语料库中最常见的词；保留频率在50,000左右之后的词通常没有帮助）。由于这些数字大多数为零，因此这是一种**稀疏**的向量表示；存在高效的算法用于存储和计算稀疏矩阵。

现在我们已经有了一些直观认识，接下来让我们深入探讨计算词语相似性的细节。之后，我们将讨论对矩阵单元格进行加权的方法。


<nav class="pagination justify-content-between">
<a href="../ch6-02">6.2 向量语义</a>
<a href="../">目录</a>
<a href="../ch6-04">6.4 用余弦度量相似性</a>
</nav>

