---
title: "第11章 信息检索与检索增强生成"
summary: ""
date: 2025-12-29T17:03:00+08:00
---

> 有两次，我被人问到：“请问，巴贝奇先生，如果您把错误的数字输入机器，它会输出正确的答案吗？”……我实在无法理解，究竟是怎样的思想混乱才会引发这样的问题。
>
> ——巴贝奇（1864）

人们需要获取信息。
因此，几乎从计算机诞生之初，我们就开始向它们提问了。
早在 1961 年，就已出现能回答美国棒球统计数据问题的系统，比如：“洋基队在七月打了多少场比赛？”（Green 等，1961）。
甚至在 20 世纪 70 年代的虚构作品中，也有类似的计算机。道格拉斯·亚当斯在《银河系漫游指南》中创造的“深思”（Deep Thought）计算机，就曾回答过“生命、宇宙以及一切的终极问题”。[^1]
由于大量知识都以文本形式存在，早在大语言模型（LLM）出现之前，问答系统就已达到人类水平的表现。例如，IBM 的 Watson 系统在 2011 年赢得了电视智力竞赛节目《危险边缘》（*Jeopardy!*），在回答如下问题时超越了人类选手：

> 威廉·威尔金森所著《瓦拉几亚与摩尔多瓦公国纪事》启发了这位作家创作其最著名的小说 [^2]

[^1]: 答案是42，但遗憾的是，那个终极问题本身从未被揭示。
[^2]: 答案当然是“布拉姆·斯托克是谁？”，那部小说就是《德古拉》。

因此很自然地，大语言模型的一项重要功能，就是通过回答用户的问题来满足**人类的信息需求**。
由于大量信息存在于网络上，问答任务与网络信息检索密切相关——后者正是搜索引擎所执行的任务。
事实上，这两者的界限正变得越来越模糊：现代搜索引擎已与大语言模型深度集成。

考虑一些简单的信息需求，例如可以用简短文本中的事实直接回答的**事实型问题**（factoid questions）：

(11.1) 卢浮宫博物馆位于哪里？

(11.2) 核爆炸的能量来自何处？

(11.3) 在 LaTeX 中如何打出花体字母 l？

要让大语言模型回答这些问题，我们只需对其进行提示即可！
例如，一个已在问答任务上经过指令微调（参见第9章）的预训练大语言模型，可以直接对以下问题进行条件生成：

> 卢浮宫博物馆位于哪里？

并将其生成的回复作为答案。
这种方法之所以有效，是因为大语言模型在其预训练数据中接触过大量事实信息（包括卢浮宫的位置），并将这些信息编码到了模型参数中。
这类事实性知识似乎主要存储在 Transformer 模型中规模庞大的前馈层连接之中（Geva 等，2021；Meng 等，2022）。

仅靠提示（prompting）大语言模型，对许多事实型问题而言确实是一种有效的方法。
但知识被存储在大语言模型前馈层的权重中，这一特性使得单纯依靠提示来准确回答事实性问题会带来若干问题。

第一个、也是最主要的问题是：大语言模型经常对事实性问题给出错误答案！
大语言模型会**产生幻觉**（hallucinate）。
所谓“幻觉”，是指模型生成的内容与现实世界的事实不符。
也就是说，当被提问时，大语言模型有时会编造听起来合理、但实际上错误的答案。
例如，Dahl 等人（2024）发现，在回答法律领域的问题（如特定法律案件）时，大语言模型产生幻觉的比例高达 69% 到 88%！
即使正确的事实其实已经存储在模型参数中，大语言模型有时仍会给出错误答案。这似乎是因为前馈层未能成功调用其参数中已有的知识（Jiang 等，2024）。

更麻烦的是，我们往往难以判断大语言模型是否正在产生幻觉。部分原因在于，大语言模型的**置信度校准**（calibration）效果很差。
在一个**校准良好**（well-calibrated）的系统中，系统对其答案正确性的置信度应与该答案实际正确的概率高度相关。
也就是说，如果一个校准良好的系统答错了，它至少可能会使用模糊措辞，或建议用户去查阅其他来源。
但大语言模型缺乏良好的校准能力，常常以十足的把握给出完全错误的答案（Zhou 等，2024）。

使用简单提示方法回答问题的第二个问题是：通过提示大语言模型仅依靠其预训练参数来作答，无法让我们就专有数据（proprietary data）进行提问。
我们希望使用语言模型来回答关于专有数据的事实性问题，例如个人电子邮件。
或者，在医疗健康应用中，我们可能希望将语言模型应用于医疗记录。
又或者，公司可能拥有包含客户服务或内部使用答案的内部文档。
再或者，律师事务所需要就法律证据开示（legal discovery）中涉及的的专有文档提出问题。
所有这些数据（希望如此）都不在大语言模型预训练所用的大规模网络语料库中。

最后一个问题是：大语言模型是静态的——它们只在某个特定时间点完成一次预训练。
这意味着，模型无法回答涉及快速变化信息的问题（比如“上周发生了什么？”），因为它们不具备预训练截止日期之后的最新知识。

针对使用简单提示回答事实性问题所带来的所有这些问题，一种解决方案是为语言模型提供外部知识来源，例如专有文本，如医疗或法律记录、个人电子邮件或公司文档，并在回答问题时使用这些文档。
这种方法被称为**检索增强生成**（retrieval-augmented generation）或**RAG**，本章将聚焦于这一方法。
在 RAG 中，我们使用**信息检索**（IR）技术来检索那些可能包含有助于回答问题的信息的文档。
然后，我们利用大语言模型，基于这些文档生成答案。

基于检索到的文档生成答案，可以解决简单地使用提示回答问题时存在的一些问题。
首先，它有助于确保答案所依据的事实来自某个经过整理的数据集。
此外，系统在向用户提供答案的同时，还可以附上该答案所来源的段落或文档的上下文。
这些信息可以帮助用户对答案的准确性建立信心（或者帮助他们发现答案何时出错！）。
而且，这些检索技术可以应用于我们所需的任何专有数据，例如在相关应用中的法律或医疗数据。

本章将首先介绍信息检索，即根据用户表达的信息需求（查询），从文档集合中选出最相关文档的任务。
我们会讲解经典方法：基于稀疏 tf-idf 向量的余弦相似度计算；也会介绍现代神经“稠密”检索器（dense retrievers），它们使用 BERT 或其他语言模型对查询和文档进行神经表示。
接着，我们将引入基于检索器的问答系统（retriever-based question answering）以及检索增强生成范式（RAG）。

最后，我们会讨论多种包含问题与答案的数据集。这些数据集既可用于对大语言模型进行指令微调（instruction tuning），也可作为评估模型性能的基准（benchmarks）。


<nav class="pagination justify-content-between">
<a href="../ch9-05">9.5 用于序列标注的微调：命名实体识别</a>
<a href="../">目录</a>
<a href="../ch11-01">11.1 信息检索</a>
</nav>

