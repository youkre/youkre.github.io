---
title: "5.2 使用逻辑回归进行分类"
summary: ""
date: 2025-10-04T09:48:00+08:00
---

前一节中的 Sigmoid 函数为我们提供了一种方法：对一个输入样本 $x$，计算其属于正类的概率 $P(y = 1|x)$。

那么，如何决定将哪个类别分配给一个测试样本 $x$ 呢？对于给定的 $x$，如果概率 $P(y = 1|x)$ 大于 0.5，就判定为“是”（即类别 1），否则判定为“否”（即类别 0）。我们将 0.5 称为**决策边界（decision boundary）**：

$$
\text{decision}(x) =
\begin{cases}
1 & \text{if } P(y = 1|x) > 0.5 \\
0 & \text{otherwise}
\end{cases}
$$

接下来，我们通过一些语言任务的例子，来展示如何使用逻辑回归作为分类器。

### 5.2.1 情感分类

假设我们正在对电影评论文本进行二元情感分类，需要判断是否将情感类别“+”或“−”分配给一篇评论文档 `doc`。我们将每个输入样本表示为下表所示的 6 个特征 $x_1$ 到 $x_6$；图 5.2 展示了一个小型测试文档及其提取出的特征向量。

| 变量 | 定义 | 图 5.2 中的值 |
| --- | --- | --- |
| $x_1$ | 文档中正面情感词典词的数量 | 3 |
| $x_2$ | 文档中负面情感词典词的数量 | 2 |
| $x_3$ | $\begin{cases} 1 & \text{if “no” in doc} \\ 0 & \text{otherwise} \end{cases}$ | 1 |
| $x_4$ | 第一和第二人称代词的数量 | 3 |
| $x_5$ | $\begin{cases} 1 & \text{if “!” in doc} \\ 0 & \text{otherwise} \end{cases}$ | 0 |
| $x_6$ | 文档词数的自然对数（ln） | $\ln(66) = 4.19$ |

![](/images/speech-and-language-processing/slp-fig-5-2.png)

**图 5.2** 一个小型测试文档示例，展示了提取出的特征向量 $x$。

我们暂时假设已经为这些特征学习到了对应的实数值权重，且这 6 个特征的权重为 `[2.5, −5.0, −1.2, 0.5, 2.0, 0.7]`，偏置项 $b = 0.1$。（下一节将介绍权重是如何学习的。）例如，权重 $w_1$ 表示正面情感词（如 *great*、*nice*、*enjoyable* 等）的数量对判断为正面情感的重要性，而 $w_2$ 则表示负面情感词的重要性。注意 $w_1 = 2.5$ 为正值，而 $w_2 = −5.0$ 为负值，这意味着负面词与正面情感的判定呈负相关，并且其重要性大约是正面词的两倍。

给定这 6 个特征和输入评论 $x$，我们可以使用公式 (5.5) 计算 $P(+|x)$ 和 $P(−|x)$：

$$
\begin{aligned}
p(+|x) = P(y = 1|x) &= \sigma(\mathbf{w} \cdot \mathbf{x} + b) \\
&= \sigma([2.5, -5.0, -1.2, 0.5, 2.0, 0.7] \cdot [3, 2, 1, 3, 0, 4.19] + 0.1) \\
&= \sigma(0.833) \\
&= 0.70 \\
p(−|x) &= P(y = 0|x) = 1 - \sigma(\mathbf{w} \cdot \mathbf{x} + b) \\
&= 0.30
\end{aligned}
$$

(5.8)
{class="text-end"}

由于 $P(+|x) = 0.70 > 0.5$，分类器将该评论判定为正面情感（+）。

### 5.2.2 其他分类任务与特征

逻辑回归被广泛应用于各种自然语言处理（NLP）任务中，输入的任何属性都可以作为特征。考虑**句号消歧（period disambiguation）**任务：通过将每个句号分类为两类之一——EOS（句子结束）或非 EOS——来判断句号是句子的结尾还是单词的一部分。我们可以使用如下所示的特征 $x_1$，表示当前词为小写，该特征可能具有正值权重。或者使用一个特征表示当前词出现在我们的缩写词典中（如“Prof.”），该特征可能具有负值权重。特征也可以表达多个属性的组合。例如，大写词后的句号更可能是 EOS，但如果该词本身是“St.”且前一个词也是大写，则该句号很可能属于“street”一词的缩写，而非句子结尾。

$$
x_1 =
\begin{cases} 
1 & \text{if “Case}(w_i) = \text{Lower”} \\
0 & \text{otherwise}
\end{cases}
\quad \\
x_2 =
\begin{cases}
1 & \text{if “}w_i \in \text{AcronymDict”} \\
0 & \text{otherwise}
\end{cases}
\quad \\
x_3 =
\begin{cases}
1 & \text{if “}w_i = \text{St. \& Case}(w_{i-1}) = \text{Upper”} \\
0 & \text{otherwise}
\end{cases}
$$

**特征的设计与学习**：在传统模型中，特征通常由人工设计，设计过程需要仔细分析训练集，结合语言学直觉、相关文献以及系统早期版本在训练集上的错误分析结果。我们还可以考虑**特征交互（feature interactions）**，即由更基础的特征组合而成的复杂特征。上面句号消歧的例子中就包含这样一个特征：当“St.”前面的词是大写时，其后的句号更不可能是句子的结尾。特征也可以通过**特征模板（feature templates）** 自动生成，特征模板是对特征的抽象描述。例如，用于句号消歧的二元语法（bigram）模板可能会为训练集中每个出现在句号前的词对创建一个特征。因此，特征空间是稀疏的，因为我们只需为训练集中实际出现在该位置的 n-gram 创建特征。特征通常通过对字符串描述进行哈希处理来生成。用户描述的一个特征，如“bigram(American breakfast)”，会被哈希为一个唯一的整数 $i$，该整数即作为特征编号 $f_i$。

从上一段可以看出，手工设计特征需要大量的人工投入。因此，近年来的 NLP 系统倾向于避免使用人工设计的特征，转而专注于**表示学习（representation learning）**：即从输入数据中以无监督方式自动学习特征的方法。我们将在第6章和第7章介绍表示学习的相关方法。

**输入特征的缩放**：当不同的输入特征具有极不相同的数值范围时，通常需要对它们进行重新缩放，使其具有可比的范围。我们通过**标准化（standardize）** 输入值，使其均值为0，标准差为1（这种变换有时称为 z-score 变换）。具体来说，如果 $µ_i$ 是输入数据集中 $m$ 个样本上特征 $x_i$ 的均值，$\sigma_i$ 是特征 $x_i$ 在整个输入数据集上的标准差，我们可以用如下方式计算得到的新特征 $x_i'$ 来替代原始特征 $x_i$：

$$
\begin{aligned}
µ_i &= \frac{1}{m} \sum_{j=1}^m x_i^{(j)} \\
\sigma_i &= \sqrt{\frac{1}{m}\sum_{j=1}^m (x_i^{(j)} − µ_i)^2} \\
x'_i &= \frac{x_i − µ_i}{\sigma_i}
\end{aligned}
$$

(5.9)
{class="text-end"}

另一种方法是将输入特征值归一化到 0 和 1 之间：

$$
x'_i = \frac{x_i − \min(x_i)}{\max(x_i) − \min(x_i)}
\tag{5.10}
$$

使输入数据具有相近的数值范围，有助于在不同特征之间进行比较。数据缩放在大型神经网络中尤为重要，因为它有助于加速梯度下降的收敛过程。

### 5.2.3 同时处理多个样本

我们之前展示的逻辑回归公式是针对单个样本的。但在实际应用中，我们通常需要一次性处理包含多个样本的整个测试集。假设我们有一个测试集，包含 $m$ 个待分类的测试样本。我们将继续使用第78页的记号，其中带括号的上标表示数据集（无论是训练集还是测试集）中的样本索引。因此，每个测试样本 $x^{(i)}$ 都有一个特征向量 $\mathbf{x}^{(i)}$，其中 $1 \leq i \leq m$。（与之前一样，我们用粗体表示向量和矩阵。）

一种计算每个输出值 $\hat{y}^{(i)}$ 的方法是使用一个 for 循环，逐个计算每个测试样本：

$$
\text{for each } x^{(i)} \text{ in input } [x^{(1)}, x^{(2)}, ..., x^{(m)}] \\
\hat{y}^{(i)} = \sigma(\mathbf{w} \cdot \mathbf{x}^{(i)} + b)
$$

(5.11)
{class="text-end"}

那么，对于前3个测试样本，我们将分别计算预测值 $\hat{y}^{(i)}$ 如下：

$$
\begin{aligned}
P(y^{(1)} = 1|x^{(1)}) &= \sigma(\mathbf{w} \cdot \mathbf{x}^{(1)} + b) \\
P(y^{(2)} = 1|x^{(2)}) &= \sigma(\mathbf{w} \cdot \mathbf{x}^{(2)} + b) \\
P(y^{(3)} = 1|x^{(3)}) &= \sigma(\mathbf{w} \cdot \mathbf{x}^{(3)} + b)
\end{aligned}
$$

但事实上，可以对原始公式 (5.5) 稍作修改，以更高效地完成这一计算。我们将使用**矩阵运算**，通过一次矩阵操作为所有样本分配类别！

首先，我们将每个输入样本 $x$ 的所有输入特征向量打包成一个单一的输入矩阵 $\mathbf{X}$，其中每一行 $i$ 是一个行向量，由输入样本 $x^{(i)}$ 的特征向量（即向量 $\mathbf{x}^{(i)}$）构成。假设每个样本有 $f$ 个特征和对应的权重，则 $\mathbf{X}$ 将是一个形状为 $[m \times f]$ 的矩阵，如下所示：

$$
\mathbf{X} =
\begin{bmatrix}
x_1^{(1)} & x_2^{(1)} & \cdots & x_f^{(1)} \\
x_1^{(2)} & x_2^{(2)} & \cdots & x_f^{(2)} \\
x_1^{(3)} & x_2^{(3)} & \cdots & x_f^{(3)} \\
\vdots & \vdots & \ddots & \vdots
\end{bmatrix}
$$

(5.12)
{class="text-end"}

现在，将偏置项 $b$ 表示为一个长度为 $m$ 的向量 $\mathbf{b}$，其中每个元素都是标量 $b$，即 $\mathbf{b} = [b, b, \cdots, b]$；将输出向量表示为 $\hat{\mathbf{y}} = [\hat{y}^{(1)}, \hat{y}^{(2)}, \cdots, \hat{y}^{(m)}]$（每个输入 $x^{(i)}$ 及其特征向量 $\mathbf{x}^{(i)}$ 对应一个标量输出 $\hat{y}^{(i)}$）；并将权重向量 $\mathbf{w}$ 表示为一个列向量。这样，就可以通过一次矩阵乘法和一次加法计算所有输出：

$$
\hat{\mathbf{y}} = \mathbf{X}\mathbf{w} + \mathbf{b}
$$

(5.13)
{class="text-end"}

你应该验证一下，公式 (5.13) 计算的结果与公式 (5.11) 中的 for 循环是相同的。例如，输出向量 $\hat{\mathbf{y}}$ 的第一个元素 $\hat{y}^{(1)}$ 将正确地计算为：

$$
\hat{y}^{(1)} = [x_1^{(1)}, x_2^{(1)}, \cdots, x_f^{(1)}] \cdot [w_1, w_2, ..., w_f] + b
$$

(5.14)
{class="text-end"}

请注意，为了使矩阵乘法能够正确进行，我们必须调整 $\mathbf{X}$ 和 $\mathbf{w}$ 在公式 (5.5) 中的顺序。下面是再次写出的公式 (5.13)，并标出了各矩阵/向量的形状：

$$
\begin{aligned}
\hat{\mathbf{y}} &= \mathbf{X} \mathbf{w} + \mathbf{b} \\
(m \times 1) &= (m \times f) (f \times 1) + (m \times 1)
\end{aligned}
$$

(5.15)
{class="text-end"}

现代编译器和计算硬件可以非常高效地执行这种矩阵运算，从而使计算速度大大加快，这在处理非常大的训练或测试数据集时尤为重要。

顺便提一下，如果我们选择将 $\mathbf{X}$ 定义为一个由列向量构成的矩阵（每个输入样本对应一个列向量），而不是行向量，那么我们本可以保持 $\mathbf{X}$ 和 $\mathbf{w}$ 在原始公式中的顺序（即 $\hat{\mathbf{y}} = \mathbf{X}\mathbf{w} + \mathbf{b}$），此时 $\mathbf{X}$ 的形状将是 $[f \times m]$。但通常情况下，我们习惯将输入表示为行向量。

### 5.2.4 选择分类器

逻辑回归相较于朴素贝叶斯具有多项优势。朴素贝叶斯对特征之间存在过强的条件独立性假设。考虑两个高度相关的特征；更极端地，假设我们将同一个特征 $f_1$ 重复添加了两次。朴素贝叶斯会将这两个 $f_1$ 的副本视为独立特征，将其概率贡献相乘，从而高估了该特征的证据强度。相比之下，逻辑回归对相关特征的鲁棒性要强得多；如果两个特征 $f_1$ 和 $f_2$ 完全相关，回归模型会简单地将权重分别分配给 $w_1$ 和 $w_2$。因此，当存在大量相关特征时，逻辑回归比朴素贝叶斯能给出更准确的概率估计。正因如此，逻辑回归在较大的文档或数据集上通常表现更优，是实践中常见的默认选择。

尽管朴素贝叶斯给出的概率不够准确，但它仍然常常能做出正确的分类决策。此外，朴素贝叶斯在非常小的数据集（Ng and Jordan, 2002）或短文本（Wang and Manning, 2012）上表现可能极佳（有时甚至优于逻辑回归）。此外，朴素贝叶斯实现简单，训练速度极快（无需迭代优化过程）。因此，在某些特定情况下，它仍然是一个合理的选择。


<nav class="pagination justify-content-between">
<a href="../ch5-01">5.1 Sigmoid函数</a>
<a href="../">目录</a>
<a href="../ch5-03">5.3 多项逻辑回归</a>
</nav>

