---
title: "5.2 向量语义：原理"
summary: ""
date: 2025-12-23T18:13:00+08:00
---

向量语义是 NLP 中表示词义的标准方法，有助于我们建模上一节中看到的许多词义特征。
该模型的根源可追溯至20世纪50年代，当时两个重要思想汇聚在一起：一个是上文提到的 Osgood 在 1957 年提出，使用三维空间中的一个点来表示词语内涵的想法；另一个是由 Joos（1950）、Harris（1954）和Firth（1957）等语言学家提出的，通过词语在语言使用中的**分布**（即其邻近词语或语法环境）来定义其意义。
他们的核心思想是：出现在非常相似分布中的两个词（即其邻近词语相似），其意义也相似。

例如，假设你不知道“ongchoi”（一个来自粤语的新借词）的含义，但你在以下语境中看到了它：

(5.1) Ongchoi 加蒜炒着吃很美味。

(5.2) Ongchoi 配米饭非常棒。  

(5.3) ……用咸酱烹制的 ongchoi 叶子……

而你之前在其他语境中见过许多类似的词语：

(5.4) ……菠菜加蒜炒着吃配米饭……

(5.5) ……甜菜的茎和叶都很美味……

(5.6) ……羽衣甘蓝和其他咸味的绿叶蔬菜……

由于 *ongchoi* 和 *spinach*（菠菜）、*chard*（甜菜）、*collard greens*（羽衣甘蓝）一样，都出现在 *rice*（米饭）、*garlic*（蒜）、*delicious*（美味）、*salty*（咸的）等词的周围，这可能暗示 ongchoi 是一种与其他绿叶蔬菜相似的绿叶蔬菜。[^1]
可以通过计算 *ongchoi* 周围上下文中的词语出现次数，用计算的方法实现同样的推理。

[^1]: 它实际上是空心菜（学名：Ipomoea aquatica），与牵牛花同属一族，在英语中有时被称为 water spinach（水菠菜）。

![](/images/speech-and-language-processing/slp-fig-5-1.png)

**图5.1** 对“sweet”附近部分词语的 200 维 word2vec 嵌入进行二维（t-SNE）可视化。图中可见，语义相近的词在空间中彼此靠近。

向量语义的核心思想是，将一个词表示为一个多维语义空间中的点，该空间由词语邻近词的分布情况（我们将在下文介绍具体方法）推导而来。
用于表示词语的向量被称为**嵌入**（embeddings）。
“嵌入”一词源于其数学含义，即从一个空间或结构映射到另一个空间或结构，尽管其含义已有所演变；详见本章末尾。

图 5.1 展示了由 word2vec 算法学习得到的词嵌入可视化结果。显示了把选定词语（“sweet”附近的词）从 200 维空间降维投影到 2 维空间后的位置。
可以看到，“sweet”最近的邻居包括 *honey*（蜂蜜）、*candy*（糖果）、*juice*（果汁）、*chocolate*（巧克力）等语义相关的词。  
语义相近的词在高维空间中彼此靠近，这一思想对语言模型和其他自然语言处理应用具有强大作用。  
例如，第 4 章中的情感分类器依赖训练集和测试集中出现相同的词。  
但如果用嵌入表示词语，分类器只要遇到**意义相近的词**，就能判断情感倾向。
此外，正如我们将看到的，像图 5.1 所示的向量语义模型，可以从文本中自动学习，无需人工标注（即无监督学习）。

本章首先介绍一种简单的教学式嵌入模型：一个词的意义由其上下文中邻近词的出现频次构成的向量来定义。  
我们引入这个模型，是为了帮助理解“向量如何表示词义”这一核心概念。不过，更成熟的变体（比如第 11 章将介绍的 **tf-idf 模型**）才是实际中需要掌握的重要方法。  
这类方法生成的向量通常很长，而且是稀疏的（sparse），即大部分元素为零（因为大多数词根本不会出现在彼此的上下文中）。
随后，我们将介绍 **word2vec 模型族**，它能生成短而**稠密**（dense）的向量，并具备更强的语义表达能力。

我们还会介绍**余弦相似度**（cosine），这是使用嵌入计算两个词、两个句子或两篇文档之间语义相似度的标准方法，也是实际应用中的重要工具。


<nav class="pagination justify-content-between">
<a href="../ch5-01">5.1 词汇语义</a>
<a href="../">目录</a>
<a href="../ch5-03">5.3 基于计数的简单嵌入</a>
</nav>

