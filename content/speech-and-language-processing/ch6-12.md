---
title: "6.12 Evaluating Vector Models"
summary: ""
date: 
---

The most important evaluation metric for vector models is extrinsic evaluation on
tasks, i.e., using vectors in an NLP task and seeing whether this improves perfor-
mance over some other model.

Nonetheless it is useful to have intrinsic evaluations. The most common metric
is to test their performance on **similarity**, computing the correlation between an
algorithm’s word similarity scores and word similarity ratings assigned by humans.
**WordSim-353** (Finkelstein et al., 2002) is a commonly used set of ratings from 0
to 10 for 353 noun pairs; for example ( *plane*, *car*) had an average score of 5.77.
**SimLex-999** (Hill et al., 2015) is a more complex dataset that quantiﬁes similarity
(*cup*, *mug*) rather than relatedness ( *cup*, *coffee*), and includes concrete and abstract
adjective, noun and verb pairs. The **TOEFL dataset** is a set of 80 questions, each
consisting of a target word with 4 additional word choices; the task is to choose
which is the correct synonym, as in the example: *Levied is closest in meaning to: imposed, believed, requested, correlated*
(Landauer and Dumais, 1997). All of these
datasets present words without context.

Slightly more realistic are intrinsic similarity tasks that include context. The
Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the
Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer
evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in
their sentential context, while WiC gives target words in two sentential contexts that
are either in the same or different senses; see Appendix G. The semantic textual
*similarity* task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of
sentence-level similarity algorithms, consisting of a set of pairs of sentences, each
pair with human-labeled similarity scores.

Another task used for evaluation is the analogy task, discussed on page 124,
where the system has to solve problems of the form *a is to b as a\* is to b\**, given *a*, *b*,
and $a^*$ and having to ﬁnd $b^*$ (Turney and Littman, 2005). A number of sets of tuples
have been created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova
et al. 2016), covering morphology (*city:cities::child:children*), lexicographic rela-
tions (*leg:table::spout:teapot*) and encyclopedia relations (*Beijing:China::Dublin:Ireland*),
some drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jur-
gens et al., 2012).

All embedding algorithms suffer from inherent variability. For example because
of randomness in the initialization and the random negative sampling, algorithms
like word2vec may produce different results even from the same dataset, and in-
dividual documents in a collection may strongly impact the resulting embeddings
(Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When em-
beddings are used to study word associations in particular corpora, therefore, it is
best practice to train multiple embeddings with bootstrap sampling over documents
and average the results (Antoniak and Mimno, 2018).


<nav class="pagination justify-content-between">
<a href="../ch6-11">6.11 Bias and Embeddings</a>
<a href="../">目录</a>
<a href="../ch6-13">6.13 Summary</a>
</nav>

