---
title: "2.5 单词和子单词分词"
summary: ""
date: 2025-07-13T22:08:38+08:00
---

上述简单的 Unix 工具适合获取粗略的单词统计，但对于**分词**任务——即将连续文本分割成单词——通常需要更复杂的算法。分词算法大致可以分为两类。在自顶向下分词中，我们定义一套标准并实现相应的规则来完成这种分词。

然而，在大多数自然语言处理应用中，我们通常不会将单词作为输入，而是将其分解为**子单词单元**（subword tokens），这些单元可以是完整的单词、单词的一部分甚至是单个字母。这些衍生自自底向上的分词，使用简单的字母序列统计来构建子单词单元的词汇表，并将输入文本分解为这些子单词。

## 2.5.1 自顶向下（基于规则的）分词

虽然之前的 Unix 命令序列去掉了所有的数字和标点符号，但在大多数自然语言处理应用中，需要保留这些字符。我们经常希望将标点符号单独作为一个标记；逗号对解析器很有用，而句号则有助于标识句子边界。但我们也常常希望保留单词内部的标点符号，例如*m.p.h.*、*Ph.D.*、*AT&T* 和*cap’n*。在价格（$45.55）和日期（01/02/06）中，特殊字符和数字需要保留。我们不希望将这个价格分割成“45”和“55”两个独立的标记。此外还有 URL（`https://www.stanford.edu`）、Twitter 标签（`#nlproc`）或电子邮件地址（`someone@cs.colorado.edu`）。

数字表达式引入了复杂性；逗号除了出现在单词边界外，英语中的数字每隔三位还会出现一个逗号：555,500.50。不同语言有不同的分词方式；例如西班牙语、法语和德语使用逗号表示小数点，而在英语中使用空格（有时也使用句号），例如 555 500,50。

分词工具还可以用来展开由撇号标记的**粘附词**缩写形式，将 `what’re` 转换为 `what are` 和 `we’re` 转换为 `we are`。粘附词是一个不能独立存在的单词部分，只能与其他单词结合出现。这类缩写形式还出现在其他字母文字语言中，包括法语代词（如 `j’ai` 和冠词 `l’homme`）。

根据具体应用的不同，分词算法可能还需要将多词表达式（如 `New York` 或 `rock ’n’ roll`）视为单一标记，这需要某种多词表达式字典的支持。因此，分词与**命名实体识别**紧密相关，后者用于检测姓名、日期和组织机构（第 17 章）。

常用的分词标准之一被称为**宾夕法尼亚大学树库分词标准**，用于语言数据联盟（LDC）发布的已解析语料库（树库）。这一标准会分离出粘附词（如 *doesn’t* 分离为 *does* 和 *n’t*），保持连字符复合词的完整性，并分离所有标点符号（为了节省空间，我们在标记之间显示可见空格 ` `，但实际上通常使用换行符作为输出）：

```text
输入: "The San Francisco-based restaurant," they said,
     "doesn’t charge $10".

输出: " The San Francisco-based restaurant , " they said ,
     " does n’t charge $ 10 " .
```

实际上，由于分词是在任何其他语言处理之前进行的，因此它必须非常快速。对于单词分词，我们通常使用基于正则表达式的确定性算法，这些算法会被编译成高效的有限状态自动机。例如，图 2.12 展示了一个基本的正则表达式，可用于 Python NLTK 库中的 `nltk.regexp_tokenize` 函数，对英文进行分词（Bird 等人，2009；https://www.nltk.org）。

```text
>>> text = 'That U.S.A. poster-print costs $12.40...'
>>> pattern = r'''(?x)   # 设置标志以允许详细注释的正则表达式
... (?:[A-Z]\.)+         # 缩写，例如 U.S.A.
... | \w+(?:-\w+)*       # 包含可选内部连字符的单词
... | \$?\d+(?:\.\d+)?%? # 货币和百分比，例如 $12.40, 82%
... | \.\.\.             # 省略号
... | [][.,;"\'?():_‘-]  # 这些是单独的标记；包括 ], [
... '''
>>> nltk.regexp_tokenize(text, pattern)
['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']
```

**图 2.12** NLTK 自然语言处理工具包（Python 版本）中的正则表达式分词示例（Bird 等人，2009），带有注释以便阅读；(?x) 标志告诉 Python 忽略注释和空白字符。该图来自 Bird 等人（2009）第 3 章。

精心设计的确定性算法可以处理这些歧义，例如撇号在作为属格标记（如“书的封面”中的 `book’s`）、引语（如 `'The other class'`, `she said`）或粘附词（如 `they’re`）时需要不同的分词方式。

对于像中文、日文和泰文这样的语言，单词分词更为复杂，因为这些语言不使用空格来标记潜在的单词边界。以中文为例，词语由字符组成（称为**汉字**）。每个字符通常代表一个意义单位（称为**语素**，morpheme），并且可以单独发音为一个音节。平均而言，一个词语大约由 2.4 个字符组成。但是，在中文中决定什么是词语是比较复杂的。例如，考虑以下句子：

```text
(2.21) 姚明进入总决赛            yáo míng jìn rú zǒng jué sài
       “Yao Ming reaches the finals”
```

正如 Chen 等人（2017b）指出的，这个句子可以被视为 3 个词（“中文树库”分词）：

```text
(2.22) 姚明      进入      总决赛  
       YaoMing  reaches   finals 
```

也可以被视为 5 个词（“北京大学”分词）：

```text
(2.23) 姚  明    进入     总      决赛  
      Yao  Ming reaches overall finals 
```

最后，还可以完全忽略词语，直接将字符作为基本元素，将句子视为一系列 7 个字符：

```text
(2.24) 姚  明    进    入     总      决       赛  
       Yao Ming enter enter overall decision game
```  

事实上，对于大多数中文自然语言处理任务，将字符而不是词语作为输入效果更好，因为字符在大多数应用中处于合理的语义层面，而大多数词语标准则会导致词汇量巨大且包含大量非常罕见的词语（Li 等人，2019b）。

然而，对于日文和泰文来说，字符是一个太小的单位，因此需要**词分割**算法。这些算法在中文的一些特殊情况下也很有用，即需要词边界而不是字符边界时。在这种情况下，我们可以使用下一节介绍的子单词分词算法。

## 2.5.2 字节对编码：一种自底向上的分词算法

处理文本分词的第三种方法是大型语言模型中最常用的方法。与其将标记定义为单词（无论是通过空格还是更复杂的算法），或定义为字符（如中文中的情况），我们可以利用数据自动确定标记应该是什么。这种方法在处理未知词语时特别有用，这是自然语言处理中的一个重要问题。正如我们将在下一章中看到的，NLP 算法通常从一个语料库（**训练**语料库）中学习一些关于语言的事实，然后使用这些事实来处理另一个单独的**测试**语料库及其语言。因此，如果我们的训练语料库包含“low”、“new”和“newer”，但不包含“lower”，那么当“lower”出现在测试语料库中时，系统将不知道如何处理它。

为了应对这个问题，现代分词器会自动推导出一组标记，其中包含比单词小的标记，称为**子词**。子词可以是任意的子字符串，也可以是有意义的单位，例如语素 *-est* 或 *-er*。（语素是指语言中最小的意义单位；例如，单词 *unwashable* 包含语素 *un-*、*wash* 和 *-able*。）在现代分词方案中，大多数标记是单词，但有些标记是频繁出现的语素或其他子词，如 *-er*。每个未见过的词，比如 *lower*，都可以用一系列已知的子词单元表示，例如 *low* 和 *er*，甚至必要时可以用单个字母序列表示。

大多数分词方案包含两部分：一个**标记学习器**（token learner）和一个**标记分段器**（token segmenter）。标记学习器接受原始训练语料库（有时大致按单词分开，例如通过空格），并推导出一个词汇表，即一组标记。标记分段器接受原始测试句子，并将其分割成词汇表中的标记。最常用的两种算法是**字节对编码**（byte-pair encoding）（Sennrich 等人，2016）和**一元语法建模**（unigram language modeling）（Kudo, 2018）。还有一个名为 **SentencePiece** 的库包含了这两种算法的实现（Kudo 和 Richardson，2018a），人们经常简单地使用 **SentencePiece** 指代 **一元语法建模** 分词。

在本节中，我们将介绍这三种方法中最简单的——**字节对编码**（**BPE**）算法（Sennrich 等人，2016）；参见图 2.13。BPE 标记学习器开始时有一个仅由所有单个字符组成的词汇表。然后它检查训练语料库，选择相邻频率最高的两个符号（例如‘A’和‘B’），将新的合并符号‘AB’添加到词汇表中，并在语料库中将所有相邻的‘A’‘B’替换为新的‘AB’。这个过程不断重复计数和合并操作，生成越来越长的字符字符串，直到进行了 k 次合并，生成了 k 个新的标记；k 是算法的一个参数。最终得到的词汇表包括原始字符集加上 k 个新符号。

该算法通常在单词内部运行（不跨越单词边界），因此首先将输入语料库按空格分开，形成一组字符串，每个字符串对应一个单词的字符，再加上一个特殊的结束符，并计算其频率。我们看看它在一个包含 18 个带词频标记的小型输入语料库上的操作（单词 *low* 出现 5 次，*newer* 出现 6 次，依此类推），初始词汇表包含 11 个字母：

```text
corpus             vocabulary
5 l o w __         __, d, e, i, l, n, o, r, s, t, w
2 l o w e s t __
6 n e w e r __
3 w i d e r __
2 n e w __
```

BPE 算法首先统计所有相邻符号对：最频繁的是 `e r`，因为它在 *newer* 中出现了 6 次，在 *wider* 中出现了 3 次，总共出现了 9 次 [^2]。然后我们将这两个符号合并，将 `er` 视为一个符号，并再次统计：

[^2] 注意可能存在平局；我们也可以先合并 *r*，因为它也有 9 次出现。

```text
corpus             vocabulary
5 l o w __         __, d, e, i, l, n, o, r, s, t, w, er
2 l o w e s t __
6 n e w er __
3 w i d er __
2 n e w __
```

现在最频繁的符号对是 `er __`，将它们合并；系统已经学习到应该有一个标记来表示词尾的 `er`，用 `er__` 表示：

```text
corpus             vocabulary
5 l o w __         __, d, e, i, l, n, o, r, s, t, w, er, er
2 l o w e s t __
6 n e w er__
3 w i d er__
2 n e w __
```

接下来，`n e`（总计 8 次）被合并为 `ne`：

```text
corpus            vocabulary
5 l o w __        ___, d, e, i, l, n, o, r, s, t, w, er, er__, ne
2 l o w e s t __
6 ne w er__
3 w i d er__
2 ne w __
```

如果继续下去，接下来的合并操作将是：

- 合并 *(ne, w)*：生成 *new*
- 合并 *(l, o)*：生成 *lo*
- 合并 *(lo, w)*：生成 *low*
- 合并 *(new, er)*：生成 *newer*
- 合并 *(low, )*：生成 *low*

最终词汇表如下：

| merge     | current vocabulary |
| --------- | ----------------- |
| (`ne, w`)  | `__, d, e, i, l, n, o, r, s, t, w, er, er__, ne, new` |
| (`l, o`)  | `__, d, e, i, l, n, o, r, s, t, w, er, er__, ne, new, lo` |
| (`lo, w`) | `__, d, e, i, l, n, o, r, s, t, w, er, er__, ne, new, lo, low` |
| (`new, er__`) | `__, d, e, i, l, n, o, r, s, t, w, er, er__, ne, new, lo, low, newer` |
| (`low, __`) | `__, d, e, i, l, n, o, r, s, t, w, er, er__, ne, new, lo, low, newer , low` |

一旦学会了词汇表，标记分段器就可以用来对测试句子进行分词。标记分段器只是在测试数据上运行我们在训练数据中学到的合并规则。它贪婪地按照学习到的顺序运行这些规则。（因此，测试数据中的频率不起作用，只有训练数据中的频率才起作用）。首先，将每个测试句子的单词分割成字符。然后应用第一个规则：将测试语料库中所有的 `e r` 替换为 `er`，接着应用第二个规则：将测试语料库中所有的 `er _` 替换为 `er_`，依此类推。到最后，如果测试语料库包含字符序列 `n e w e r _`，它将被标记为一个完整的词。但对于一个新的（未知的）单词如 `l o w e r __`，其字符将被合并为两个标记 `low er__`。

```text
function BYTE-PAIR ENCODING (strings C, number of merges k) returns vocab V
V←all unique characters in C # 初始标记集是字符
for i = 1 to k do              # 合并标记 k 次
    t_L, t_R ← Most frequent pair of adjacent tokens in C
    t_NEW ← t_L + t_R             # 将两个标记连接生成新标记
    V←V + t_NEW                 # 更新词汇表
    Replace each occurrence of t_L, t_R in C with t_NEW # 并更新语料库
return V
```

**图 2.13** BPE 算法的标记学习部分，用于将按单个字符或字节分割的语料库转换为词汇表，通过迭代合并标记来学习。改编自 Bostrom 和 Durrett（2020）。

当然，在实际应用中，BPE 在一个非常大的输入语料库上进行数千次合并操作。结果是，大多数单词将以完整的符号表示，而只有非常罕见的单词（和未知单词）才会以它们的部分形式表示。


<nav class="pagination justify-content-between">
<a href="../ch2-03">2.3 语料库</a>
<a href="../">目录</a>
<a href="../ch2-06">2.6 词形归一化、词目归并与词干提取</a>
</nav>

