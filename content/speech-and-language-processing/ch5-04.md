---
title: "5.4 逻辑回归中的学习"
summary: ""
date: 2025-10-06T09:49:00+08:00
---

模型的参数，即权重 $\mathbf{w}$ 和偏置 $b$，是如何学习得到的呢？逻辑回归是监督分类的一个实例，在该任务中，我们知道每个观测样本 $x$ 的正确标签 $y$（0 或 1）。系统通过公式 (5.5) 产生的是 $\hat{y}$，即系统对真实标签 $y$ 的估计值。我们的目标是学习到一组参数（即 $\mathbf{w}$ 和 $b$），使得每个训练样本的预测值 $\hat{y}$ 尽可能接近其真实标签 $y$。

这需要两个在本章引言中已预示过的组成部分。第一个是衡量当前预测标签 $\hat{y}$ 与真实标注标签 $y$ 之间接近程度的度量标准。我们通常不直接衡量相似性，而是讨论其对立面：系统输出与真实输出之间的**距离**，并称此距离为**损失函数**（loss function）或**代价函数**（cost function）。在下一节中，我们将介绍逻辑回归以及神经网络中常用的损失函数——**交叉熵损失**（cross-entropy loss）。

第二个需要的是一个优化算法，用于迭代地更新权重，以最小化该损失函数。解决这一问题的标准算法是**梯度下降**（gradient descent）；我们将在后续章节中介绍其变体——**随机梯度下降**（stochastic gradient descent）算法。

在接下来的两节中，将以更简单的二元逻辑回归为例来描述这些算法，然后在第5.8节中再转向多项逻辑回归。


<nav class="pagination justify-content-between">
<a href="../ch5-03">5.3 多项逻辑回归</a>
<a href="../">目录</a>
<a href="../ch5-05">5.5 交叉熵损失函数</a>
</nav>

