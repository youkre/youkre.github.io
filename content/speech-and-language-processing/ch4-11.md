---
title: "4.11 统计显著性检验"
summary: ""
date: 2025-12-16T16:33:00+08:00
---

在构建系统时，我们常常需要比较两个系统的性能。
如何判断我们刚刚构建的新系统是否优于旧系统？或者是否优于文献中描述的其他系统？这属于统计假设检验（statistical hypothesis testing）的范畴。
本节将介绍用于自然语言处理（NLP）分类器的统计显著性检验方法，主要参考了 Dror 等人（2020）与 Berg-Kirkpatrick 等人（2012）的工作。

假设我们要在某个评价指标 $M$（如 $F_1$ 值或准确率）上比较分类器 $A$ 与 $B$ 的性能。
例如，我们想知道：在某个特定测试集 $x$ 上，我们的逻辑回归情感分类器 $A$（见第5章）是否比朴素贝叶斯情感分类器 $B$ 获得更高的 $F_1$ 分数。
记 $M(A, x)$ 为系统 A 在测试集 $x$ 上的得分，$\delta(x)$ 为 A 与 B 在 $x$ 上的性能差异：

$$
\delta(x) = M(A,x) - M(B,x)
\tag{4.44}
$$

我们希望判断 $\delta(x) > 0$ 是否成立——即逻辑回归分类器在该测试集上的 $F_1$ 是否确实高于朴素贝叶斯分类器。
$\delta(x)$ 被称为**效应量**（effect size）：$\delta$ 越大，说明 A 明显优于 B；$\delta$ 越小，则说明 A 仅略胜一筹。

那么，为何不能直接看 $\delta(x)$ 是否为正？
假设我们发现 A 的 $F_1$ 比 B 高出 0.04，是否就能断定 A 更好？不能！
因为这种优势可能只是偶然出现在当前测试集 $x$ 上。
我们需要更严谨的判断：A 相对于 B 的优势是否具有**可复现性**？也就是说，如果换一个测试集 $x'$，或在其他条件下重复实验，A 是否仍能保持优势？

在统计假设检验的框架下，我们通过形式化两个假设来回答这个问题：

$$
\begin{aligned}
H_0 &: \delta(x) \leq 0 \\
H_1 &: \delta(x) > 0
\end{aligned}
\tag{4.45}
$$

其中，$H_0$ 称为 **零假设**（null hypothesis），它假设 $\delta(x)$ 实际上小于或等于零，即 A 并不优于 B。
我们的目标是判断能否有把握地拒绝零假设，从而支持备择假设 $H_1$（即 A 确实更好）。

为此，我们引入一个随机变量 $X$，其取值范围覆盖所有可能的测试集。
接着我们问：如果零假设 $H_0$ 成立，那么在大量重复实验中，观察到当前 $\delta(x)$ 的概率有多大？
这一概率被形式化为 **p 值**（p-value），即假设零假设 $H_0$ 为真，观测到已知的 $\delta(x)$ 值或更大值的概率：

$$
P(\delta(X) \geq \delta(x) \mid H_0 \text{ 为真})
\tag{4.46}
$$

换句话说，在假设 A **并不优于** B 的前提下，p 值就是我们期望看到 $\delta(x)$ 的概率。
举例来说，若 $\delta(x)$ 很大（比如 A 的 $F_1 = 0.9$，而 B 仅为 0.2），就出乎了我们的预料，因为在 $H_0$ 成立的情况下，这种差距极不可能出现，因此 p 值会很低（即如果 A 实际上并不优于 B，就不可能有这么大的 $\delta(x)$）。
反之，若 $\delta(x)$ 很小，即使 $H_0$ 成立，这种微小差距也可能频繁出现，p 值就会较高。

当 p 值非常小时，说明在零假设下观察到当前结果的概率极低，我们就有理由拒绝零假设。
那么，多小才算“非常小”？
通常采用 0.05 或 0.01 作为阈值。
若设定显著性水平为 0.01，且计算出的 p 值小于 0.01，我们就拒绝 $H_0$，认为 A 确实优于 B。
此时，如果看到的 $\delta(x)$ 低于阀值，就可以决绝这个零假设，我们称该结果（如“A 优于 B”）具有 **统计显著性**（statistically significant）。

那么，如何计算 p 值所需的这一概率？
在 NLP 中，我们通常不使用你可能熟悉的 t 检验或方差分析（ANOVA）等参数检验方法。
因为这些方法对检验统计量的分布（如正态性）有较强假设，而这些假设在 NLP 场景中往往不成立。
因此，NLP 中普遍采用基于重采样（sampling）的非参数检验（non-parametric tests）。其基本思想是：人工构造大量实验场景的变体。
例如，如果我们拥有多个不同的测试集 $x'$，就可以测量每个 $x'$ 上的 $\delta(x')$，从而得到一个经验分布。
然后设定显著性阈值（如 0.01）：若该分布中 99% 以上的 $\delta$ 值都小于我们实际观测到的 $\delta(x)$，即 p 值小于 0.01，我们便可以拒绝零假设，认为观测到的性能差距足够“意外”，从而确认 A 确实是一个比 B 更优的算法。

NLP 中常用的两种非参数检验是：**近似随机化检验**（approximate randomization，Noreen, 1989）和 **自助法检验**（bootstrap test）。
下面我们介绍后者，并聚焦于其配对版本（paired version）——这在 NLP 中最为常见。
所谓 **配对检验**（paired test），是指所比较的两组观测值是对齐的（aligned）：每组中的每个观测都能与另一组中的对应观测配成一对。
当我们比较两个系统在同一测试集上的性能时，这种配对关系自然存在——我们可以将系统 A 在单个样本 $x_i$ 上的表现，与系统 B 在同一个 $x_i$ 上的表现配对比较。

### 4.9.1 配对自助法检验

**自助法检验**（bootstrap test，Efron 和 Tibshirani, 1993）适用于任意评价指标——无论是精确率、召回率、$F_1$ 值，还是机器翻译中常用的 BLEU 指标。
术语 **自助法**（bootstrapping）指的是从原始数据集中有放回地重复抽取大量样本（称为 **自助样本**，bootstrap samples）。
其核心思想是：我们可以从一个已观测到的测试集出发，通过反复重采样，构造出大量“虚拟”测试集。
该方法仅假设原始样本能够代表总体分布。

考虑一个极小的文本分类示例：测试集 $x$ 包含 10 篇文档。
图 4.11 的第一行展示了两个分类器（A 和 B）在此测试集上的结果。
每篇文档属于四种情况之一：A 和 B 都正确；A 和 B 都错误；A 正确而 B 错误；A 错误而 B 正确。  
其中，字母上加删除线（如 $\cancel{B}$）表示该分类器预测错误。
例如，第一篇文档上 A 和 B 均预测正确（AB），第二篇文档上 A 正确但 B 错误（$A\cancel{B}$）。
为简化起见，假设我们使用的评价指标是准确率，则 A 的准确率为 0.70，B 为 0.50，因此性能差 $\delta(x) = 0.20$。

接下来，我们生成大量（例如 $b = 10^5$）虚拟测试集 $x^{(i)}$，每个仍包含 $n = 10$ 篇文档。
图 4.11 展示了其中几个示例。
构造每个虚拟测试集 $x^{(i)}$ 的方法是：从原始测试集 $x$ 的 10 个单元格中有放回地随机抽取 10 次。
例如，在构造第一个虚拟测试集 $x^{(1)}$ 的第一个单元格时，若随机抽中了原始行中的第二个单元格（即 $A\cancel{B}$），就将该值复制到新集合中；接着继续抽取以填充第二个单元格，依此类推，每次均从原始 $x$ 中独立、有放回地采样。

![](/images/speech-and-language-processing/slp-fig-4-11.png)

**图 4.11** 配对自助法检验：从初始真实测试集 $x$ 生成 $b$ 个伪测试集 $x^{(i)}$ 的示例。
每个伪测试集通过有放回地采样 $n=10$ 次生成；每次采样的基本单位是一个单元格，即一篇带有标准标签以及分类器 A 和 B 正误判断的文档。
当然，真实的测试集远不止 10 个样本，且 $b$ 也需足够大。

现在我们拥有了 $b$ 个虚拟测试集，从而得到了一个经验抽样分布。基于此，我们可以统计 A **偶然优于 B** 的频率。
计算这种优势有多种方式，此处我们采用 Berg-Kirkpatrick 等人（2012）提出的版本。
在零假设 $H_0$（即 A 并不优于 B）成立的前提下，我们预期在大量测试集上估计出的 $\delta(X)$ 应接近于零或为负值；若观测到显著更大的正值，则令人意外。
为了量化我们实际观测到的 $\delta(x)$ 有多“意外”，一种直观做法是计算 p 值：统计在多少比例的虚拟测试集中，$\delta(x^{(i)})$ 超过期望值（即 0）的幅度至少达到 $\delta(x)$：

$$
\text{p-value}(x) = \frac{1}{b} \sum_{i=1}^b \mathbf{1} \big( \delta(x^{(i)}) - \delta(x) \geq 0 \big)
$$

（此处 $\mathbf{1}(x)$ 表示指示函数：当 $x$ 为真时取值 1，否则为 0。）

然而，尽管在 $H_0$ 成立时，$\delta(X)$ 在大量测试集上的期望值确实应为 0，但我们通过自助法生成的虚拟测试集并不满足这一条件。
原因在于：这些样本并非来自一个均值为 0 的分布，而是直接从原始测试集 $x$ 中抽取的——而 $x$ 本身恰好偏向 A（$\delta(x) = 0.20$）。
因此，要评估观测到的 $\delta(x)$ 有多意外，我们实际上应计算：在多少比例的虚拟测试集中，$\delta(x^{(i)})$ 超过 $\delta(x)$ 的幅度至少为 $\delta(x)$ 本身，即：

$$
\begin{align*}
\text{p-value}(x) &= \frac{1}{b} \sum_{i=1}^b \mathbf{1} \big( \delta(x^{(i)}) - \delta(x) \geq \delta(x) \big) \\
&= \frac{1}{b} \sum_{i=1}^b \mathbf{1} \big( \delta(x^{(i)}) \geq 2\delta(x) \big) \tag{4.47}
\end{align*}
$$

例如，若我们生成了 10,000 个虚拟测试集，并设定显著性阈值为 0.01，结果仅有 47 个测试集中满足 $\delta(x^{(i)}) \geq 2\delta(x)$，则计算得到的 p 值为 0.0047。由于该值小于 0.01，说明我们观测到的性能差距 $\delta(x)$ 极不可能由偶然因素导致，因此可以拒绝零假设，得出结论：**A 确实优于 B**。

![](/images/speech-and-language-processing/slp-fig-4-12.png)

**图 4.12** Berg-Kirkpatrick 等人（2012）提出的配对自助法算法的一个版本。

完整的自助法检验算法如图 4.12 所示。
其输入包括原始测试集 $x$ 和采样次数 $b$，算法统计在 $b$ 个自助测试集中满足 $\delta(x^{*(i)}) > 2\delta(x)$ 的比例，并将该比例作为**单侧经验 p 值**（one-sided empirical p-value）。


<nav class="pagination justify-content-between">
<a href="../ch4-10">4.10 测试集与交叉验证</a>
<a href="../">目录</a>
<a href="../ch4-12">4.12 分类中的危害防范</a>
</nav>

