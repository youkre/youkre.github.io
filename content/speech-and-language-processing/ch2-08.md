---
title: "2.8 用于词元化的简易 Unix 工具"
summary: ""
date: 2025-12-21T17:55:00+08:00
---

对于英文文本，我们可以通过一条简单的 Unix 命令行完成朴素的单词词元化（tokenization）和词频统计。
正如 Church（1994）所指出的，当我们需要快速获取某个文本语料库的基本信息时，这种方法非常实用。
我们将用到几个 Unix 命令：`tr` 用于系统性地将输入中的特定字符进行替换；`sort` 按字母顺序对输入行进行排序；`uniq` 合并相邻的重复行，并对这些行计数。

例如，我们从莎士比亚全部词语开始，存储在一个文件 `sh.txt` 中。
我们可以使用 `tr` 将所有非字母字符序列替换为换行符，从而实现单词词元化。
这里，`'A-Za-z'` 表示所有英文字母，`-c` 选项表示取反（即匹配非字母字符），因此该命令会将每个非字母字符替换为换行符。
`-s`（“squeeze”，压缩）选项则确保连续的多个非字母字符只生成一个换行符，避免出现空行。

```bash
tr -sc 'A-Za-z' '\n' < sh.txt
```

该命令的输出如下所示：

```text
THE
SONNETS
by
William
Shakespeare
From
fairest
creatures
...
```

现在每行只有一个单词，我们可以将其排序后传给 `uniq -c`，后者会合并相邻的相同行并统计出现次数：

```bash
tr -sc 'A-Za-z' '\n' < sh.txt | sort | uniq -c
```

输出结果类似：

```text
1945 A
72 AARON
19 ABBESS
25 Aaron
6 Abate
1 Abates
...
```

如果我们希望将所有大写字母统一转为小写，可以再加一步转换：

```bash
tr -sc 'A-Za-z' '\n' < sh.txt | tr A-Z a-z | sort | uniq -c
```

输出变为：

```text
14725 a
   97 aaron
    1 abaissiez
   10 abandon
    2 abandoned
    2 abase
    1 abash
   14 abate
...
```

接下来，我们可以再次排序，找出高频词。
`sort` 的 `-n` 选项表示按数值大小排序（而非字母顺序），`-r` 选项表示逆序排列（从高到低）：

```bash
tr -sc 'A-Za-z' '\n' < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r
```

结果显示出莎士比亚作品中频率最高的词汇，与几乎所有语料库一样，都是简短的**功能词**（function words），如冠词、代词、介词等：

```text
27378 the
26084 and
22538 i
19771 to
17481 of
14725 a
13826 you
...
```

这类 Unix 工具在快速构建英文语料库的词频统计时非常便捷。
而对于更复杂的任务，我们通常会转向前面讨论过的更高级的词元化算法。


<nav class="pagination justify-content-between">
<a href="../ch2-07">2.7 正则表达式</a>
<a href="../">目录</a>
<a href="../ch2-09">2.9 最小编辑距离（Minimum Edit Distance）</a>
</nav>

