---
title: "6.2 向量语义"
summary: ""
date: 2025-10-12T08:53:00+08:00
---

向量语义是NLP中表示词义的标准方法，有助于我们建模上一节中看到的许多词义特征。该模型的根源可追溯至20世纪50年代，当时两个重要思想汇聚在一起：一个是上文提到的Osgood在1957年提出的使用三维空间中的一个点来表示词语内涵的想法；另一个是由Joos（1950）、Harris（1954）和Firth（1957）等语言学家提出的，通过词语在语言使用中的**分布**（即其邻近词语或语法环境）来定义其意义。他们的核心思想是：出现在非常相似分布中的两个词（即其邻近词语相似），其意义也相似。

例如，假设你不知道“ongchoi”（ongchoi，一种来自粤语的新借词）的含义，但你在以下语境中看到了它：

(6.1) Ongchoi 加蒜炒着吃很美味。

(6.2) Ongchoi 配米饭非常棒。  

(6.3) ……用咸酱烹制的 ongchoi 叶子……

而你之前在其他语境中见过许多类似的词语：

(6.4) ……菠菜加蒜炒着吃配米饭……

(6.5) ……甜菜的茎和叶都很美味……

(6.6) ……羽衣甘蓝和其他咸味的绿叶蔬菜……

由于 *ongchoi* 和 *spinach*（菠菜）、*chard*（甜菜）、*collard greens*（羽衣甘蓝）一样，都出现在 *rice*（米饭）、*garlic*（蒜）、*delicious*（美味）、*salty*（咸的）等词的周围，这可能暗示 ongchoi 是一种与其他绿叶蔬菜相似的绿叶蔬菜。可以通过计算 *ongchoi* 周围上下文中的词语出现次数，用计算的方法实现同样的推理。

向量语义的核心思想是：将一个词表示为一个多维语义空间中的点，该空间由词语邻近词的分布情况（我们将在下文介绍具体方法）推导而来。用于表示词语的向量被称为**嵌入**（embeddings）（尽管有时该术语更严格地仅指word2vec这类密集向量（见第6.8节），而不包括tf-idf或PPMI等稀疏向量（见第6.3-6.6节））。“嵌入”一词源于其数学含义，即从一个空间或结构到另一个空间或结构的映射，尽管其含义已有所演变；详见本章末尾。

![](/images/speech-and-language-processing/slp-fig-6-1.png)

**图6.1** 一些词语和短语嵌入的二维（t-SNE）投影，显示了语义相近的词语在空间中彼此靠近。原始的60维嵌入是为情感分析训练的。简化自Li等（2015），并添加颜色以助解释。

图6.1展示了为情感分析学习到的嵌入的可视化结果，将选定词语从60维空间投影到二维空间。注意，正向词语、负向词语和中性功能词分别聚集在不同的区域。

向量语义对词语相似性的细粒度建模为NLP应用提供了强大的能力。像第4章或第5章中的情感分类器这样的NLP应用，依赖于训练集和测试集中出现相同的词语。但通过将词语表示为嵌入向量，只要分类器看到一些语义相近的词语，就可以推断出情感倾向。而且，正如我们将看到的，向量语义模型可以完全从文本中自动学习，无需监督。

在本章中，我们将介绍两种最常用的模型。在**tf-idf**模型（一种重要的基线模型）中，词语的意义由其邻近词语出现次数的简单函数定义。我们将看到，这种方法会产生非常长的**稀疏**向量，即大部分元素为零的向量（因为大多数词语在其上下文中根本不会出现）。我们还将介绍**word2vec**模型家族，用于构建短小且**密集**的向量，这些向量具有有用的语义特性。此外，我们还会介绍**余弦相似度**（cosine），这是使用嵌入向量计算两个词、两个句子或两个文档之间**语义相似度**的标准方法，是问答、摘要或自动作文评分等实际应用中的重要工具。


<nav class="pagination justify-content-between">
<a href="../ch6-01">6.1 词汇语义</a>
<a href="../">目录</a>
<a href="../ch6-03">6.3 词语与向量</a>
</nav>

