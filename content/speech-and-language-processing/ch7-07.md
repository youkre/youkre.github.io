---
title: "7.7 语言模型的伦理与安全问题"
summary: ""
date: 2025-12-27T10:25:00+08:00
---

早在大语言模型出现之前，伦理与安全问题就一直是人工智能体设计中的核心考量。
玛丽·雪莱（Mary Shelley，见下图）在其小说《弗兰肯斯坦》中，正是围绕“在未考虑伦理与人文关切的情况下创造人工智能体”这一问题展开叙事的。

大语言模型（LLMs）可能以多种方式带来安全隐患。
例如，LLM 容易生成虚假内容，这一问题被称为“幻觉”（hallucination）。
语言模型的训练目标是生成可预测且连贯的文本，但截至目前我们所介绍的训练算法，并没有任何机制确保生成内容的真实性或正确性。
这对任何依赖事实准确性的应用场景都会造成严重问题！
与此相关的一个表现是，语言模型可能会建议危险行为，例如直接鼓励用户从事危险或非法活动，如自残或伤害他人。
当用户在涉及安全的关键场景中（如寻求医疗建议、处于紧急状况，或表达自残意图时）向语言模型求助，错误的建议可能带来危险，甚至危及生命。
需要指出的是，这类问题并非大语言模型所独有。例如，Bickmore 等人（2018）曾让参与者向三个 LLM 时代之前的商用对话系统（Siri、Alexa、Google Assistant）提出医疗问题，并根据系统回复决定采取何种行动；结果发现，许多被建议的行动一旦付诸实施，将可能导致伤害甚至死亡。
我们将在第 11 章再次讨论幻觉与事实性问题，届时会介绍诸如 “检索增强生成”（retrieval-augmented generation）等缓解方法；此外，在第 9 章中，我们也将探讨通过安全微调（safety tuning）和对齐（alignment）来提升模型安全性。

除了提供错误信息，系统还可能通过言语**攻击用户**，或造成**表征性伤害**（representational harms）（Blodgett 等，2020）。
例如生成带有侮辱性或有害的刻板印象（Cheng 等，2023），以及贬低特定群体的负面态度（Brown 等，2020；Sheng 等，2019）。
无论是言语辱骂还是刻板印象，都可能对用户造成心理伤害。
Gehman 等人（2020）的研究表明，即使输入完全无害的提示，大语言模型仍可能输出仇恨言论并对用户进行言语攻击。
Liu 等人（2020）测试了系统对两组模拟用户输入的响应，这两组输入内容完全相同，仅在提及的性别或种族上有所差异。
他们发现，仅仅将句子中的 “he” 改为 “she”，就可能导致系统回复更具冒犯性和更负面的情绪倾向。
Hofmann 等人（2024）进一步发现，大语言模型甚至会仅仅因为用户使用了特定方言（如非裔美国人英语）而对其产生歧视。
同样，这些问题在大语言模型出现前就已存在。
例如，微软于 2016 年推出的聊天机器人 **Tay**，上线仅 16 小时就被迫下线，因为它开始发布包含种族侮辱、阴谋论和人身攻击的内容。
Tay 的这些偏见和不当行为源于其训练数据，其中包括一些用户故意诱导它重复此类语言（Neff 和 Nagy，2016）。

另一个重要的伦理与安全问题是**隐私**（privacy）。
从计算机诞生之初，隐私就一直是人们关注的焦点。早在 1966 年，魏岑鲍姆（Weizenbaum）设计聊天机器人 ELIZA 作为计算心理治疗实验时，就已触及这一问题。
当时，人们很快对 ELIZA 产生了强烈的情感依赖，并与其进行极为私密的对话——甚至有人在输入信息时要求 Weizenbaum 离开房间。
而当 Weizenbaum 提出可能要保存这些对话记录时，用户立即指出这会侵犯他们的隐私。

如今，用户同样很可能向大语言模型透露非常私人的信息。
事实上，当前 LLM 最常见的用途之一就是提供个人建议与情感支持（Zao-Sanders, 2025）。
而且，系统表现得越像人类，用户就越倾向于披露敏感信息，同时却越不会担忧这种披露可能带来的危害（Ischen 等，2019）。
我们在前文（7.5.2 节）已提到，预训练数据本身往往包含电话号码、地址等私人信息。
这带来了严重风险：大语言模型可能会**泄露**（leak）其训练数据中的信息。
也就是说，攻击者有可能从语言模型中提取出训练数据中的具体内容，例如某人的姓名、电话号码和住址（Henderson 等，2017；Carlini 等，2021）。
如果模型是在极其敏感的私有数据集（如电子健康记录）上训练的，这一问题将更加严峻。

一个相关的安全问题是**情感依赖**（emotional dependence）。Reeves 和 Nass（1996）的研究表明，人们倾向于将人类特征赋予计算机，并以对待真人的方式与之互动——即使他们清楚自己面对的只是一台机器，也会像理解人类话语一样去解读计算机的输出。
因此，LLM 已对用户的认知和情绪状态产生显著影响，甚至导致用户对模型产生情感依赖。
这些现象（情感投入与隐私风险）意味着我们必须认真思考 LLM 对与其交互人群的实际影响。

除了上述对用户造成的直接伤害外，LLM 还可能主动实施其他有害行为，尤其是在基于智能体（agent-based）的范式下，语言模型能够直接与外部世界交互，进一步放大潜在风险。

此外，恶意行为者也可能利用语言模型生成用于**欺诈**、网络钓鱼、宣传、虚假信息传播或其他社会危害活动的文本（Brown 等，2020）。
McGuffie 和 Newhouse（2020）展示了大语言模型如何生成模仿网络极端主义者的文本，从而存在助长极端主义运动、推动激进化和招募新成员的风险。

当然，正如我们在 7.5.2 节所讨论的，许多 LLM 的伦理问题根源在于其预训练语料来自网络爬取的数据，包括数据同意缺失、潜在的版权侵权，以及训练数据中存在的偏见——而语言模型往往会**放大**（amplify）这些偏见，这一点我们在第 5 章讨论词嵌入模型时已经看到。

目前，如何缓解所有这些伦理与安全问题已成为自然语言处理（NLP）领域的重要研究方向。其中关键一步是仔细分析用于预训练大语言模型的数据，以深入理解其中存在的毒性、歧视、隐私和合理使用等问题。
因此，为语言模型提供完整的**数据说明书**（datasheets，见第 20 页）或**模型卡片**（model cards，见第 89 页），详细说明其训练语料的来源与构成，显得尤为重要。
开源模型尤其应明确列出其确切的训练数据。
当前，针对滥用与毒性问题的缓解研究十分活跃，例如开发能有效检测并恰当响应有毒上下文的技术（Wolf 等，2017；Dinan 等，2020；Xu 等，2020）。

此外，**价值敏感设计**（Value Sensitive Design）——即在系统开发早期就前瞻性地考虑可能造成的伤害（Friedman 等，2017；Friedman 和 Hendry，2019）——也至关重要。
Dinan 等人（2021）提出了一系列系统设计的最佳实践建议，例如：无论参与者是用于模型训练，还是与已部署的 LLM 交互，都应获得其知情同意（informed consent）。
由于研究 LLM 的交互特性涉及人类参与者，研究人员还需与所在机构的机构审查委员会（Institutional Review Boards, **IRB**）合作，由 IRB 协助确保实验参与者的安全与权益。


<nav class="pagination justify-content-between">
<a href="../ch7-06">7.6 大语言模型的评估</a>
<a href="../">目录</a>
<a href="../ch8">第 8 章 Transformer</a>
</nav>

