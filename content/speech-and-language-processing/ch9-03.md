---
title: "9.3 使用单个矩阵 X 并行化计算"
summary: ""
date: 2025-11-02T15:09:00+08:00
---

到目前为止，我们对多头注意力以及其余 Transformer 模块的描述，角度都是在单个残差流中，在单个时间步 $i$，计算单个输出。
但正如之前所指出的，为每个词元计算 $\a_i$ 的注意力计算，与其他词元的计算是相互独立的；同样，从输入 $\x_i$ 计算 $\h_i$ 的整个 Transformer 模块中的所有计算也是独立的。
这意味着我们可以轻松地对整个计算过程进行并行化，从而利用高效的矩阵乘法。

要实现并行计算，我们将输入序列中 $N$ 个词元的输入嵌入向量打包（pack）成一个大小为 $[N \times d]$ 的单一矩阵 $\mathbf{X}$。
矩阵 $\mathbf{X}$ 的每一行都是输入序列中一个词元的嵌入向量。
用于大型语言模型的 Transformer 通常具有 $N = 1K, 2K$，甚至多达 32K 个词元（或更多）的输入长度，因此 $\mathbf{X}$ 通常有 1K 到 32K 行，每行的维度为嵌入维度 $d$（即模型维度）。

**注意力的并行化** 我们首先以单个注意力头为例，然后扩展到多头注意力，最后再加入 Transformer 模块中的其他组件。
对于单个注意力头，将 $\mathbf{X}$ 分别乘以形状为 $[d \times d_k]$ 的查询矩阵 $\mathbf{W}^Q$、形状为 $[d \times d_k]$ 的键矩阵 $\mathbf{W}^K$ 和形状为 $[d \times d_v]$ 的值矩阵 $\mathbf{W}^V$，从而生成形状为 $[N \times d_k]$ 的矩阵 $\mathbf{Q}$、$K \in \mathbb{R}^{N \times d_k}$ 和 $V \in \mathbb{R}^{N \times d_v}$，这些矩阵包含了所有词元的查询、键和值向量：

$$
\begin{aligned}
\mathbf{Q} &= \mathbf{XW}^Q; \\
\mathbf{K} &= \mathbf{XW}^K; \\
\mathbf{V} &= \mathbf{XW}^V
\end{aligned}
\tag{9.31}
$$

有了这些矩阵后，可以使用一次矩阵乘法计算 $\mathbf{Q}$ 和 $\mathbf{K}^T$ 的成绩，同得到所有必需的查询-键比较。
其乘积是一个 $N \times N$ 的矩阵，如图 9.8 所示。

**图9.8** 展示了 $N \times N$ 的 $\mathbf{QK}^T$ 矩阵，它通过一次矩阵乘法计算了所有 $\mathbf{q}_i \cdot \mathbf{k}_j$ 的比较。

一旦得到了这个 $\mathbf{QK}^T$ 矩阵，就可以非常高效地对这些分数进行缩放，应用 softmax 函数，然后将结果乘以 $\mathbf{V}$，最终得到一个形状为 $N \times d$ 的矩阵：即输入序列中每个词元的向量嵌入表示。
我们将一个包含 $N$ 个词元的序列、针对一个注意力头的整个自注意力步骤简化为以下计算：

$$
\mathbf{A} = \text{softmax}(\text{mask}(\mathbf{QK}^T / \sqrt{d_k})) \mathbf{V} \tag{9.32}
$$

**屏蔽未来信息** 你可能已经注意到，我们在公式 (9.32) 中引入了一个 `mask`（掩码）函数。
这是因为我们之前描述的自注意力计算存在一个问题：$\mathbf{QK}^T$ 的计算会为每个查询值与每一个键值（*包括那些位于查询之后的键值*）都产生一个分数。
这在语言建模的场景下是不合适的：如果你已经知道了下一个词，那么预测它就太简单了！
为了解决这个问题，矩阵的上三角部分的元素会被置零（实际上是设为 $-\infty$），从而消除模型对序列中后续词元的任何了解。
在实践中，这是通过添加一个掩码矩阵 $\mathbf{M}$ 实现的，其中当 $j > i$ 时（即上三角部分），$M_{ij} = -\infty$，否则 $M_{ij} = 0$。
图 9.9 展示了经过掩码处理后的 $\mathbf{QK}^T$ 矩阵。（我们将在第11章看到，对于需要利用未来信息的任务，该如何利用这些信息。）

图 9.10 以示意图形式展示了单个注意力头所有计算步骤的矩阵并行化过程。

图 9.8 和 图 9.9 也清楚地表明，注意力计算的复杂度与输入长度的平方成正比，因为在每一层，我们都需要计算输入中每一对词元之间的点积。
这使得在非常长的文档（如整本小说）上计算注意力变得非常昂贵。
尽管如此，现代大型语言模型仍然能够处理数千甚至数万个词元的长上下文。

**图9.9** 展示了 $N \times N$ 的 $\mathbf{QK}^T$ 矩阵，其中比较矩阵的上三角部分被置零（设为 $-\infty$，这会使 softmax 的输出变为零）。

**图9.10** 以并行方式展示单个注意力头计算过程的示意图。第一行展示了 $\mathbf{Q}$、$\mathbf{K}$ 和 $\mathbf{V}$ 矩阵的计算。第二行展示了 $\mathbf{QK}^T$ 的计算、掩码操作（softmax 计算和按维度归一化未在图中显示），然后是值向量的加权求和，得到最终的注意力向量。

**多头注意力的并行化** 在多头注意力中，与自注意力类似，输入和输出的维度是模型维度 $d$，查询和键的嵌入维度为 $d_k$，值的嵌入维度为 $d_v$（同样，在最初的 Transformer 论文中，$d_k = d_v = 64$，头数 $h = 8$，$d = 512$）。
因此，对于每个头 $i$，我们都有权重矩阵 $W_{Qi} \in \mathbb{R}^{d \times d_k}$、$W_{Ki} \in \mathbb{R}^{d \times d_k}$ 和 $W_{Vi} \in \mathbb{R}^{d \times d_v}$，这些权重矩阵与被打包进 $\mathbf{X}$ 的输入相乘，生成 $\mathbf{Q} \in \mathbb{R}^{N \times d_k}$、$K \in \mathbb{R}^{N \times d_k}$ 和 $\mathbf{V} \in \mathbb{R}^{N \times d_v}$。
每个 $h$ 个头的输出形状为 $N \times d_v$，因此包含 $h$ 个头的多头层的输出就是 $h$ 个形状为 $N \times d_v$ 的矩阵。
为了在后续处理中使用这些矩阵，它们会被拼接（concatenated）起来，形成一个维度为 $N \times hd_v$ 的单一输出。
最后，我们使用另一个线性投影矩阵 $W^O \in \mathbb{R}^{hd_v \times d}$，将其重塑（reshape）回每个词元的原始输出维度。
将这个拼接后的 $N \times hd_v$ 矩阵乘以 $W^O \in \mathbb{R}^{hd_v \times d}$，就得到了形状为 $[N \times d]$ 的自注意力输出 $\mathbf{A}$。

$$
\begin{aligned}
\mathbf{Q}^i = \mathbf{X}W_{Qi}; \\
\mathbf{K}^i = \mathbf{X}W_{Ki}; \\
\mathbf{V}^i = \mathbf{X}W_{Vi}
\end{aligned}
\tag{9.33}
$$


$$
\begin{aligned}
\text{head}_i = \text{SelfAttention}(\mathbf{Q}^i,\mathbf{K}^i,\mathbf{V}^i) \\
= \text{softmax}\left(\frac{\mathbf{Q}^i \mathbf{K}^{iT}}{\sqrt{d_k}}\right)\mathbf{V}^i
\end{aligned}
\tag{9.34}
$$

$$
\text{MultiHeadAttention}(\mathbf{X}) = (\text{head}_1 \oplus \text{head}_2 \oplus \cdots \oplus \text{head}_h)W^O
\tag{9.35}
$$

**结合并行输入矩阵 X 整合所有部分** 整个 Transformer 层对 $N$ 个输入词元并行计算的函数可以表示为：

$$
\mathbf{O} = \text{LayerNorm}(\mathbf{X} + \text{MultiHeadAttention}(\mathbf{X}))
\tag{9.36}
$$

$$
\mathbf{H} = \text{LayerNorm}(\mathbf{O} + \text{FFN}(\mathbf{O}))
\tag{9.37}
$$

或者，我们可以像之前一样，用一个方程表示模块内每个组件的计算，使用 $\mathbf{T}$（形状为 $[N \times d]$）来代表 Transformer，并用上标区分模块内的每一次计算：

$$
\mathbf{T}^1 = \text{MultiHeadAttention}(\mathbf{X})
\tag{9.38}
$$

$$
\mathbf{T}^2 = \mathbf{X} + \mathbf{T}^1
\tag{9.39}
$$

$$
\mathbf{T}^3 = \text{LayerNorm}(\mathbf{T}^2)
\tag{9.40}
$$

$$
\mathbf{T}^4 = \text{FFN}(\mathbf{T}^3)
\tag{9.41}
$$

$$
\mathbf{T}^5 = \mathbf{T}^4 + \mathbf{T}^3
\tag{9.42}
$$

$$
\mathbf{H} = \text{LayerNorm}(\mathbf{T}^5)
\tag{9.43}
$$

这里，当我们使用类似 $\text{FFN}(\mathbf{T}^3)$ 的记号时，意味着同一个前馈网络（FFN）会并行地应用于窗口内的 $N$ 个嵌入向量中的每一个。
同样，层归一化（LayerNorm）也会并行地对这 $N$ 个词元分别进行。
至关重要的是，Transformer 模块的输入和输出维度是匹配的，以便它们可以被堆叠。
由于每个输入到模块的词元 $\mathbf{x}_i$ 的维度为 $d$，这意味着输入 $\mathbf{X}$ 和输出 $\mathbf{H}$ 的形状都是 $[N \times d]$。


<nav class="pagination justify-content-between">
<a href="../ch9-02">9.2 Transformer 模块</a>
<a href="../">目录</a>
<a href="../ch9-04">9.4 输入：词元和位置的嵌入</a>
</nav>

