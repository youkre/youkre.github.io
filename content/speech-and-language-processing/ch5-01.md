---
title: "5.1 Sigmoid函数"
summary: ""
date: 2025-10-03T08:56:00+08:00
---

二元逻辑回归的目标是训练一个分类器，使其能够对新输入样本的类别做出二元判断。本节我们将介绍**Sigmoid分类器**，它将帮助我们完成这一决策。

考虑一个输入样本 $x$，我们将其表示为一个特征向量 $[x_1, x_2, ..., x_n]$。（下一小节将展示具体的特征示例。）分类器的输出 $y$ 可以是1（表示该样本属于该类别）或0（表示不属于该类别）。我们希望知道该样本属于该类别的概率 $P(y = 1|x)$。例如，这个决策可能是“正面情感”与“负面情感”的区分，特征代表文档中词的出现次数，$P(y = 1|x)$ 表示文档具有正面情感的概率，而 $P(y = 0|x)$ 表示文档具有负面情感的概率。

逻辑回归通过从训练集中学习一个**权重向量**和一个**偏置项（bias term）**来解决此任务。每个权重 $w_i$ 是一个实数，与输入特征 $x_i$ 相关联。权重 $w_i$ 表示该输入特征对分类决策的重要性，其值可以为正（为样本属于正类提供证据），也可以为负（为样本属于负类提供证据）。因此，在情感分析任务中，我们可能会预期单词 *awesome* 具有很高的正值权重，而 *abysmal* 则具有非常低的负值权重。**偏置项**（也称为**截距项**）是另一个实数，会被加到加权后的输入总和中。

在测试阶段——即训练完成后确定了权重之后——分类器首先将每个 $x_i$ 与其对应的权重 $w_i$ 相乘，对所有加权特征求和，并加上偏置项 $b$。得到的单个数值 $z$ 表示支持该类别的加权证据总和：

$$
z = \left( \sum_{i=1}^n w_i x_i \right) + b
$$

(5.2)
{class="text-end"}

在本书其余部分，我们将使用线性代数中的**点积（dot product）**符号来表示此类求和。两个向量 $a$ 和 $b$ 的点积记作 $a \cdot b$，是它们对应元素乘积之和。（注意：我们用粗体 $\mathbf{b}$ 表示向量。）因此，以下表达式与公式 (5.2) 等价：

$$
z = \mathbf{w} \cdot \mathbf{x} + b
$$

(5.3)
{class="text-end"}

但请注意，公式 (5.3) 中没有任何机制强制 $z$ 成为一个合法的概率值，即落在0到1之间。事实上，由于权重是实数，输出甚至可能为负；$z$ 的取值范围是从 $-\infty$ 到 $\infty$。

![](/images/speech-and-language-processing/slp-fig-5-1.png)

::fig:: **图 5.1** Sigmoid函数 $\sigma(z) = \frac{1}{1+e^{-z}}$ 将一个实数值映射到区间 (0,1) 内。它在0附近接近线性，但极端值会被压缩至0或1。

为了生成一个概率值，我们将 $z$ 输入**Sigmoid函数** $\sigma(z)$。Sigmoid函数（因其形状像字母“s”而得名）也被称为**Logistic函数**，逻辑回归也因此得名。Sigmoid函数的数学表达式如下，其图形如图5.1所示：

$$
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + \exp(-z)}
$$

(5.4)
{class="text-end"}

（在本书后续内容中，我们将使用 $\exp(x)$ 表示 $e^x$。）Sigmoid函数具有多个优点：它将一个实数值映射到 (0,1) 区间内，这正是我们对概率值的要求。因为它在0附近近似线性，而在两端趋于平坦，所以能将异常值压缩到接近0或1。此外，它是可微的，这一点将在第5.10节中对模型学习过程起到重要作用。

我们几乎完成了整个过程。如果我们将Sigmoid函数应用于加权特征的总和，就会得到一个介于0和1之间的数值。为了让它成为一个真正的概率分布，我们只需确保两种情况的概率之和为1，即 $P(y=1) + P(y=0) = 1$。我们可以按如下方式实现：

$$
\begin{aligned}
P(y = 1) &= \sigma(\mathbf{w} \cdot \mathbf{x} + b) \\
&= \frac{1}{1 + \exp(-(\mathbf{w} \cdot \mathbf{x} + b))} \\
\\
P(y = 0) &= 1 - \sigma(\mathbf{w} \cdot \mathbf{x} + b) \\
&= 1 - \frac{1}{1 + \exp(-(\mathbf{w} \cdot \mathbf{x} + b))} \\
&= \frac{\exp(-(\mathbf{w} \cdot \mathbf{x} + b))}{1 + \exp(-(\mathbf{w} \cdot \mathbf{x} + b))}
\end{aligned}
$$

(5.5)
{class="text-end"}

Sigmoid函数具有以下性质：

$$
1 - \sigma(x) = \sigma(-x)
$$

(5.6)
{class="text-end"}

因此，我们也可以将 $P(y = 0)$ 表达为 $\sigma(-(\mathbf{w} \cdot \mathbf{x} + b))$。

最后，补充一个术语说明。Sigmoid函数的输入，即公式 (5.3) 中的得分 $z = \mathbf{w} \cdot \mathbf{x} + b$，通常被称为 **logit**。这是因为**logit函数**是Sigmoid函数的逆函数。logit函数定义为**几率比（odds ratio）** $\frac{p}{1-p}$ 的对数：

$$
\mathrm{logit}(p) = \sigma^{-1}(p) = \ln\frac{p}{1 - p}
$$

(5.7)
{class="text-end"}

将 $z$ 称为 **logit**，是在提醒我们：通过使用Sigmoid函数将 $z$（取值范围为 $-\infty$ 到 $\infty$）转换为概率，我们实际上隐含地将 $z$ 解释为不仅仅是任意实数，而是特定的**对数几率（log odds）**。


<nav class="pagination justify-content-between">
<a href="../ch5">第5章 逻辑回归</a>
<a href="../">目录</a>
<a href="../ch5-02">5.2 使用逻辑回归进行分类</a>
</nav>

