---
title: "9.5 语言模型头"
summary: ""
date: 2015-11-04T15:00:38+08:00
---

我们必须介绍的 Transformer 最后一个组件是**语言模型头**（language modeling head）。
这里的“头”（head）指的是，当把预训练好的 Transformer 模型应用于各种任务时，在基础 Transformer 架构之上额外添加的神经网络结构。
而语言模型头，正是我们进行语言建模所需要的那部分电路。

回想一下，从第3章的简单 n-gram 模型，到第7章和第8章的前馈网络和 RNN 语言模型，语言模型本质上都是**词预测器**。
给定一个上下文词序列，它们会为每一个可能的下一个词分配一个概率。
例如，如果前面的上下文是 “*Thanks for all the*”，而我们想知道下一个词是 “*fish*” 的可能性有多大，就需要计算：

```text
P(fish | Thanks for all the)
```

语言模型能够为每一个可能的下一个词赋予这样的条件概率，从而在整个词汇表上生成一个概率分布。
第3章中的 n-gram 语言模型通过统计某个词与前 $n-1$ 个词共同出现的次数来计算其概率，因此其上下文长度为 $n-1$。
而对于 Transformer 语言模型来说，其上下文是整个 Transformer 的上下文窗口大小，这个窗口可以非常大：对于大型模型而言，可以达到 2K、4K，甚至 32K 个词元。

语言模型头的任务是：取最后一个 Transformer 层对**最后一个词元 $N$** 的输出，并用它来预测位置 $N+1$ 处的下一个词。
图 9.14 展示了如何完成这一任务：取最后一层中最后一个词元的输出（一个形状为 $[1 \times d]$ 的 $d$ 维输出嵌入），并生成一个在整个词汇表上的概率分布（我们将从中选择一个词来进行生成）。

**图9.14** 语言模型头：位于 Transformer 顶部的结构，负责将最后一个 Transformer 层对词元 N 的输出嵌入 ($h^L_N$) 映射到词汇表 $V$ 上的一个词的概率分布。

图 9.14 中的第一个模块是一个线性层（linear layer），它的作用是将输出 $h^L_N$——即表示最终模块 $L$ 在位置 $N$ 的输出词元嵌入（因此形状为 $[1 \times d]$）——投影到一个**logit 向量**（logit vector），也称为分数向量（score vector）。
该 logit 向量 $u$ 会对词汇表 $V$ 中的 $|V|$ 个可能的词中的每一个都给出一个分数，因此其维度为 $1 \times |V|$。

这个线性层的权重矩阵可以是独立学习的，但更常见的情况是将其与（嵌入矩阵 $E$ 的）转置矩阵进行**权重绑定**（weight tying）。
回顾一下，在**权重绑定**中，我们使用同一组权重来同时作为模型中两个不同矩阵的参数。
因此，在 Transformer 的输入阶段，嵌入矩阵（形状为 $[|V| \times d]$）用于将词汇表上的独热向量（形状为 $[1 \times |V|]$）映射到一个嵌入向量（形状为 $[1 \times d]$）。
而在语言模型头中，则使用嵌入矩阵的转置 $E^T$（形状为 $[d \times |V|]$）将一个嵌入向量（形状为 $[1 \times d]$）重新映射回词汇表上的一个向量（形状为 $[1 \times |V|]$）。
在学习过程中，矩阵 $E$ 会被优化，以同时胜任这两种映射任务。因此，我们有时称转置矩阵 $E^T$ 为**解嵌入层**（unembedding layer），因为它执行的是这种反向映射。

随后，一个 softmax 层将逻辑值向量 $u$ 转换为在整个词汇表上的概率 $y$。

$$
\mathbf{u} = h^L_N \mathbf{E}^T
\tag{9.44}
$$

$$
\mathbf{y} = \text{softmax}(\mathbf{u})
\tag{9.45}
$$

我们可以利用这些概率来做很多事情，比如帮助评估一段给定文本的概率。
但最重要的用途是**生成文本**，这通过从这些概率 $y$ 中**采样**（sampling）一个词来实现。
我们可以采样概率最高的那个词（“贪婪”解码），也可以使用我们在第10.2节将要介绍的其他采样方法。
无论哪种方式，只要从概率向量 $y$ 中选择了某个条目 $y_k$，就会生成具有索引 $k$ 的那个词。

**图9.15** 一个 Transformer 语言模型（仅解码器），堆叠了多个 Transformer 模块，并将输入词元 $w_i$ 映射到预测的下一个词元 $w_{i+1}$。

图 9.15 展示了针对一个词元 $i$ 的完整堆叠架构。请注意，每个 Transformer 层的输入 $\mathbf{x}_i$ 与前一层的输出 $h^{L-1}_i$ 是相同的。

现在我们已经将所有这些 Transformer 层在页面上展开，可以指出解嵌入层（unembedding layer）的另一个有用特性：它作为一种工具，可以帮助我们解释 Transformer 内部的工作机制，这种方法被称为**logit 透镜**（logit lens）(Nostalgebraist, 2020)。
我们可以取 Transformer 任意一层中的一个向量，然后假装它是最终的嵌入向量，直接将其乘以解嵌入层得到 logit 值，再计算 softmax，就能看到该向量可能代表的词的概率分布。
这为我们窥探模型的内部表征提供了一个有用的窗口。
由于网络在训练时并没有被要求让内部表征以这种方式工作，因此 logit 值透镜并不总是完美有效，但这仍然是一个非常有用的技巧。

在结束本节之前，还有一个术语说明：你有时会看到将 Transformer 用于这种单向因果语言模型的架构被称为**仅解码器模型**（decoder-only model）。
这是因为该模型大致构成了我们在第13章将要学习的、用于机器翻译的 **编码器-解码器模型**（encoder-decoder model）的一半。
(令人困惑的是，Transformer 的原始论文提出的是一个编码器-解码器架构，而后来才定义了标准范式，即仅使用该原始架构中的解码器部分来构建因果语言模型。)


<nav class="pagination justify-content-between">
<a href="../ch9-04">9.4 输入：词元和位置的嵌入</a>
<a href="../">目录</a>
<a href="../ch9-06">9.6 总结</a>
</nav>

