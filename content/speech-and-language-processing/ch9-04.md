---
title: "9.4 输入：词元和位置的嵌入"
summary: ""
date: 2025-11-04T15:24:00+08:00
---

现在我们来讨论输入矩阵 $\mathbf{X}$ 的来源。
给定一个包含 $N$ 个词元的序列（$N$ 是以词元为单位的上下文长度），形状为 $[N \times d]$ 的矩阵 $\mathbf{X}$ 包含了上下文中每个词元的一个**嵌入**（embedding）。
Transformer 通过分别计算两种嵌入来实现这一点：一种是输入词元嵌入，另一种是输入位置嵌入。

词元嵌入（token embedding）在第7章和第8章中已经介绍过，它是一个维度为 $d$ 的向量，将作为输入词元的初始表示。
（当通过 Transformer 的各层在残差流中传递这些向量时，这种嵌入表示会不断变化和丰富，融入上下文信息，并根据我们构建的语言模型类型发挥不同的作用。）
所有初始嵌入都存储在一个嵌入矩阵 $\mathbf{E}$ 中，该矩阵为词汇表 $|V|$ 中的每个词元都设有一行。因此，每个词元对应一个 $d$ 维的行向量，而矩阵 $\mathbf{E}$ 的形状为 $[|V| \times d]$。

给定一个像 *Thanks for all the* 这样的输入词元序列，我们首先将这些词元转换为词汇表索引（这些索引是在我们最初使用 BPE 或 SentencePiece 对输入进行分词时创建的）。
因此，*thanks for all the* 的表示可能为 $\mathbf{w} = [5, 4000, 10532, 2224]$。接下来，使用索引操作从矩阵 $E$ 中选择对应的行（第5行、第4000行、第10532行、第2224行）。

另一种思考如何从嵌入矩阵中选择词元嵌入的方式，是将词元表示为形状为 $[1 \times |V|]$ 的**独热向量**（one-hot vector），即词汇表中每个词对应一个维度。
回想一下，在一个**独热向量**中，除了一个元素为1（其维度对应词元在词汇表中的索引）外，其余所有元素均为0。
例如，如果单词“thanks”在词汇表中的索引是 5，那么 $x_5 = 1$，而对于所有 $i \neq 5$，有 $x_i = 0$，如下所示：

```text
[0 0 0 0 1 0 0 ... 0 0 0 0]
 1 2 3 4 5 6 7 ...  ... |V|
```

将嵌入矩阵 $E$ 与一个仅有一个非零元素 $x_i = 1$ 的独热向量相乘，就相当于直接选出了对应词 $i$ 的行向量，从而得到了词 $i$ 的嵌入向量，如图 9.11 所示。

**图9.11** 通过将嵌入矩阵 $\mathbf{E}$ 与一个在索引5处为1的独热向量相乘，来选择词 $V_5$ 的嵌入向量。

我们可以将这个想法扩展到整个词元序列：将序列表示为一个由 $N$ 个独热向量组成的矩阵，每个向量对应 Transformer 上下文窗口中的一个位置，如图 9.12 所示。

**图9.12** 通过将一个与词元ID序列 $W$ 对应的独热矩阵与嵌入矩阵 $E$ 相乘，来选择整个输入序列的嵌入矩阵。

这些词元嵌入本身是与位置无关的。为了表示序列中每个词元的位置，需要将这些词元嵌入与针对输入序列中每个位置的**位置嵌入**（positional embeddings）结合起来。

那么，这些位置嵌入从何而来呢？最简单的方法称为**绝对位置**（absolute position），即从随机初始化的嵌入开始，为每个可能的输入位置（直到某个最大长度）分配一个嵌入。
例如，就像我们为单词 *fish* 有一个嵌入一样，我们也会为位置 3 分配一个嵌入。
与词嵌入类似，这些位置嵌入也会在训练过程中与其他参数一同被学习。
我们可以将它们存储在一个形状为 $[N \times d]$ 的矩阵 $E_{pos}$ 中。

为了生成一个能够捕捉位置信息的输入嵌入，只需将每个输入词元的词嵌入与其对应的位置嵌入相加。
单个的词元嵌入和位置嵌入都是 $[1 \times d]$ 大小，因此它们的和也是一个 $[1 \times d]$ 的向量。
这个新的嵌入向量将作为后续处理的输入。
图 9.13 展示了这一思路。

**图9.13** 一种建模位置的简单方法：将绝对位置的嵌入加到词元嵌入上，生成一个相同维度的新嵌入。

输入的最终表示，即矩阵 $\mathbf{X}$，是一个 $[N \times d]$ 的矩阵，其中每一行 $i$ 表示输入序列中第 $i$ 个词元的表示，其计算方式为：将 $E[id(i)]$（即在位置 $i$ 出现的词元ID的嵌入）与 $P[i]$（位置 $i$ 的位置嵌入）相加。

这种简单的绝对位置嵌入方法存在一个潜在问题：在输入的初始位置上，会有大量的训练样本，而接近最大长度限制的位置上的样本则相对较少。
这些靠后的位置嵌入可能训练不足，在测试时泛化能力较差。
为了解决这个问题，一种替代绝对位置嵌入的方法是选择一个静态函数，该函数将整数输入映射为实值向量，并以某种方式捕捉位置之间的内在关系。
也就是说，它能捕捉到这样一个事实：输入中的位置4与位置5的关系比与位置17的关系更近。
在最初的 Transformer 论文中，使用了不同频率的正弦和余弦函数的组合。
甚至存在更复杂的位置嵌入方法，例如表示**相对位置**（relative position）而非绝对位置的方法，这些方法通常在每一层的注意力机制内部实现，而不是仅在初始输入时添加一次。


<nav class="pagination justify-content-between">
<a href="../ch9-03">9.3 使用单个矩阵 X 并行化计算</a>
<a href="../">目录</a>
<a href="../ch9-05">9.5 语言模型头</a>
</nav>

