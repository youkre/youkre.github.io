---
title: "6.13 Summary"
summary: ""
date: 
---

- In vector semantics, a word is modeled as a vector—a point in high-dimensional
space, also called an **embedding**. In this chapter we focus on static **embeddings**,
where each word is mapped to a ﬁxed embedding.
- Vector semantic models fall into two classes: **sparse** and **dense**. In sparse
models each dimension corresponds to a word in the vocabulary V and cells
are functions of **co-occurrence counts**. The **term-document** matrix has a
row for each word (**term**) in the vocabulary and a column for each document.
The **word-context** or **term-term** matrix has a row for each (target) word in
the vocabulary and a column for each context term in the vocabulary. Two
sparse weightings are common: the **tf-idf** weighting which weights each cell
by its **term frequency** and **inverse document frequency**, and **PPMI** (point-
wise positive mutual information), which is most common for word-context
matrices.
- Dense vector models have dimensionality 50–1000. **Word2vec** algorithms
like **skip-gram** are a popular way to compute dense embeddings. Skip-gram
trains a logistic regression classiﬁer to compute the probability that two words
are ‘likely to occur nearby in text’. This probability is computed from the dot
product between the embeddings for the two words.
- Skip-gram uses stochastic gradient descent to train the classiﬁer, by learning
embeddings that have a high dot product with embeddings of words that occur
nearby and a low dot product with noise words.
- Other important embedding algorithms include **GloVe**, a method based on
ratios of word co-occurrence probabilities.
- Whether using sparse or dense vectors, word and document similarities are
computed by some function of the dot product between vectors. The cosine
of two vectors—a normalized **dot product**—is the most popular such metric.

## Bibliographical and Historical Notes

The idea of vector semantics arose out of research in the 1950s in three distinct
ﬁelds: linguistics, psychology, and computer science, each of which contributed a
fundamental aspect of the model.

The idea that meaning is related to the distribution of words in context was
widespread in linguistic theory of the 1950s, among distributionalists like Zellig
Harris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos
(1950) put it,

> the linguist’s “meaning” of a morpheme. . . is by deﬁnition the set of conditional probabilities of its occurrence in context with all other morphemes.

The idea that the meaning of a word might be modeled as a point in a multi-
dimensional semantic space came from psychologists like Charles E. Osgood, who
had been studying how people responded to the meaning of words by assigning val-
ues along scales like *happy/sad* or *hard/soft*. Osgood et al. (1957) proposed that the
meaning of a word in general could be modeled as a point in a multidimensional
Euclidean space, and that the similarity of meaning between two words could be
modeled as the distance between these points in the space.

A ﬁnal intellectual source in the 1950s and early 1960s was the ﬁeld then called
**mechanical indexing**, now known as **information retrieval**. In what became known
as the **vector space model** for information retrieval (Salton 1971, Sparck Jones
1986), researchers demonstrated new ways to deﬁne the meaning of words in terms
of vectors (Switzer, 1965), and reﬁned methods for word similarity based on mea-
sures of statistical association between words like mutual information (Giuliano, 1965)
and idf (Sparck Jones, 1972), and showed that the meaning of documents
could be represented in the same vector spaces used for words. Around the same
time, (Cordier, 1965) showed that factor analysis of word association probabilities
could be used to form dense vector representations of words.

Some of the philosophical underpinning of the distributional way of thinking
came from the late writings of the philosopher Wittgenstein, who was skeptical of
the possibility of building a completely formal theory of meaning deﬁnitions for
each word. Wittgenstein suggested instead that “the meaning of a word is its use in
the language” (Wittgenstein, 1953, PI 43). That is, instead of using some logical lan-
guage to deﬁne each word, or drawing on denotations or truth values, Wittgenstein’s
idea is that we should deﬁne a word by how it is used by people in speaking and un-
derstanding in their day-to-day interactions, thus preﬁguring the movement toward
embodied and experiential models in linguistics and NLP (Glenberg and Robertson
2000, Lake and Murphy 2021, Bisk et al. 2020, Bender and Koller 2020).

More distantly related is the idea of deﬁning words by a vector of discrete fea-
tures, which has roots at least as far back as Descartes and Leibniz (Wierzbicka 1992,
Wierzbicka 1996). By the middle of the 20th century, beginning with the work of
Hjelmslev (Hjelmslev, 1969) (originally 1943) and ﬂeshed out in early models of
generative grammar (Katz and Fodor, 1963), the idea arose of representing mean-
ing with **semantic features**, symbols that represent some sort of primitive meaning.
For example words like hen, rooster, or chick, have something in common (they all
describe chickens) and something different (their age and sex), representable as:

```text
hen     +female, +chicken, +adult
rooster -female, +chicken, +adult
chick   +chicken, -adult
```

The dimensions used by vector models of meaning to deﬁne words, however, are
only abstractly related to this idea of a small ﬁxed number of hand-built dimensions.
Nonetheless, there has been some attempt to show that certain dimensions of em-
bedding models do contribute some speciﬁc compositional aspect of meaning like
these early semantic features.

The use of dense vectors to model word meaning, and indeed the term **embedding**,
grew out of the **latent semantic indexing** (LSI) model (Deerwester et al., 1988)
recast as **LSA** (**latent semantic analysis**) (Deerwester et al., 1990). In LSA
**singular value decomposition** — **SVD** — is applied to a term-document matrix (each
cell weighted by log frequency and normalized by entropy), and then the ﬁrst 300
dimensions are used as the LSA embedding. Singular Value Decomposition (SVD)
is a method for ﬁnding the most important dimensions of a data set, those dimen-
sions along which the data varies the most. LSA was then quickly widely applied:
as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking
(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Ju-
rafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000,
Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Juraf-
sky, 2001a), and essay grading (Rehder et al., 1998). Related models were simul-
taneously developed and applied to word sense disambiguation by Sch¨utze (1992b).
LSA also led to the earliest use of embeddings to represent words in a probabilis-
tic classiﬁer, in the logistic regression document router of Sch ¨utze et al. (1995).
The idea of SVD on the term-term matrix (rather than the term-document matrix)
as a model of meaning for NLP was proposed soon after LSA by Sch ¨utze (1992b).
Sch¨utze applied the low-rank (97-dimensional) embeddings produced by SVD to the
task of word sense disambiguation, analyzed the resulting semantic space, and also
suggested possible techniques like dropping high-order dimensions. See Sch ¨utze
(1997).

A number of alternative matrix models followed on from the early SVD work,
including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent
Dirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factoriza-
tion (NMF) (Lee and Seung, 1999).

The LSA community seems to have ﬁrst used the word “embedding” in Landauer
et al. (1997), in a variant of its mathematical meaning as a mapping from one space
or mathematical structure to another. In LSA, the word embedding seems to have
described the mapping from the space of sparse count vectors to the latent space of
SVD dense vectors. Although the word thus originally meant the mapping from one
space to another, it has metonymically shifted to mean the resulting dense vector in
the latent space, and it is in this sense that we currently use the word.

By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that
neural language models could also be used to develop embeddings as part of the task
of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and
Collobert et al. (2011) then demonstrated that embeddings could be used to represent
word meanings for a number of NLP tasks. Turian et al. (2010) compared the value
of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011)
showed that recurrent neural nets could be used as language models. The idea of
simplifying the hidden layer of these neural net language models to create the skip-
gram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The
negative sampling training algorithm was proposed in Mikolov et al. (2013b). There
are numerous surveys of static embeddings and their parameterizations (Bullinaria
and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark
2014, Levy et al. 2015).

See Manning et al. (2008) and Chapter 14 for a deeper understanding of the role
of vectors in information retrieval, including how to compare queries with docu-
ments, more details on tf-idf, and issues of scaling to very large datasets. See Kim
(2019) for a clear and comprehensive tutorial on word2vec. Cruse (2004) is a useful
introductory linguistic text on lexical semantics.

## Exercises


<nav class="pagination justify-content-between">
<a href="../ch6-12">6.12 Evaluating Vector Models</a>
<a href="../">目录</a>
<a href="../ch9">第九章 Transformer</a>
</nav>

