---
title: "6.13 小结"
summary: ""
date: 
---

- 在向量语义学中，一个词被建模为一个向量——即高维空间中的一个点，也称为**嵌入**（embedding）。本章聚焦于**静态嵌入**（static embeddings），其中每个词被映射到一个固定的嵌入表示。
- 向量语义模型可分为两类：**稀疏**（sparse）和**稠密**（dense）。在稀疏模型中，每一维对应词汇表 V 中的一个词，单元格的值是**共现频次**的某种函数。**词–文档矩阵**（term-document matrix）为词汇表中的每个词（术语）设一行，为每个文档设一列。**词–上下文矩阵**（word-context 或 term-term matrix）为词汇表中的每个（目标）词设一行，为每个上下文词设一列。两种常用的稀疏加权方法是：**tf-idf** 加权，其单元格值由**词频**（term frequency）和**逆文档频率**（inverse document frequency）共同决定；**PPMI**（点互信息的正值部分，positive pointwise mutual information），常用于词–上下文矩阵。
- 稠密向量模型的维度通常为 50–1000。**Word2vec** 算法（如 **skip-gram**）是计算稠密嵌入的流行方法。Skip-gram 训练一个逻辑回归分类器，用于估计两个词“在文本中彼此邻近出现”的概率，该概率通过两个词嵌入向量的**点积**（dot product）计算得出。
- Skip-gram 使用随机梯度下降训练该分类器：学习得到的嵌入应与邻近词的嵌入具有高点积，而与噪声词（负样本）的嵌入具有低点积。
- 其他重要的嵌入算法包括 **GloVe**，它基于词共现概率的比值构建嵌入。
- 无论是使用稀疏还是稠密向量，词与文档之间的相似度都通过向量点积的某种函数来计算。其中最常用的度量是两个向量的**余弦相似度**（cosine similarity）——即归一化后的**点积**。

## 参考文献与历史注记

向量语义的思想起源于 20 世纪 50 年代三个不同领域的研究：语言学、心理学和计算机科学，每个领域都为该模型贡献了关键思想。

20 世纪 50 年代的语言学理论中，分布主义思想广为流传。以 Zellig Harris、Martin Joos 和 J. R. Firth 为代表的分布主义者，以及 Thomas Sebeok 等符号学家，都认为词义与其在上下文中的分布密切相关。正如 Joos（1950）所言：

> 语言学家对一个语素的“意义”……按定义就是它与所有其他语素在上下文中共同出现的条件概率集合。

将词义建模为多维语义空间中一个点的想法，则来自心理学家 Charles E. Osgood 等人的研究。他们通过让人们在诸如 *happy/sad*（快乐/悲伤）或 *hard/soft*（坚硬/柔软）等量表上对词义打分，来研究人们对词义的感知。Osgood 等人（1957）提出，一个词的意义总体上可被建模为欧几里得多维空间中的一个点，而两个词在意义上的相似性则可用它们在该空间中的距离来刻画。

第三个思想来源是 20 世纪 50 年代末至 60 年代初被称为**机械索引**（mechanical indexing）的领域，即如今的**信息检索**（information retrieval）。在后来被称为**向量空间模型**（vector space model）的信息检索框架中（Salton, 1971；Sparck Jones, 1986），研究者提出了用向量定义词义的新方法（Switzer, 1965），并改进了基于统计关联度量（如互信息（Giuliano, 1965）和 IDF（Sparck Jones, 1972））的词相似度计算方法，同时证明文档的意义也可以用与词相同的向量空间来表示。几乎同时，Cordier（1965）展示了如何通过对词语联想概率进行因子分析，生成词的稠密向量表示。

这种分布主义思维方式的部分哲学基础来自哲学家维特根斯坦（Wittgenstein）晚年的著作。他对为每个词建立完全形式化的意义定义持怀疑态度。维特根斯坦提出：“一个词的意义就是它在语言中的用法”（Wittgenstein, 1953, 《哲学研究》第 43 节）。也就是说，我们不应试图用某种逻辑语言来定义词义，也不应依赖指称或真值条件，而应通过人们在日常言语交流中如何使用和理解词语来界定其意义。这一观点预示了后来语言学和自然语言处理中朝向**具身认知**（embodied）与**经验模型**（experiential models）的发展方向（Glenberg & Robertson, 2000；Lake & Murphy, 2021；Bisk 等, 2020；Bender & Koller, 2020）。

更远一些的思想渊源是用一组离散特征向量来定义词义，这一理念至少可追溯至笛卡尔（Descartes）和莱布尼茨（Leibniz）（Wierzbicka, 1992, 1996）。到 20 世纪中叶，从 Hjelmslev（1969，原版发表于 1943 年）的工作开始，并在早期生成语法模型中得到进一步发展（Katz 和 Fodor, 1963），学界提出了用**语义特征**（semantic features）来表示意义的想法——这些特征是代表某种基本语义单元的符号。例如，“hen”（母鸡）、“rooster”（公鸡）和“chick”（小鸡）这几个词既有共性（都指鸡），也有差异（年龄和性别不同），可以用如下方式表示：

```text
hen     +female, +chicken, +adult  
rooster -female, +chicken, +adult  
chick   +chicken, -adult
```

然而，现代向量语义模型用于定义词义的维度，仅在抽象层面上与这种由人工构建、数量固定且有限的语义特征维度相关。尽管如此，已有研究尝试证明，嵌入模型中的某些维度确实能捕捉到类似早期语义特征那样的具体组合性语义成分。

使用稠密向量建模词义、乃至“**嵌入**”（embedding）这一术语本身，均源于**潜在语义索引**（Latent Semantic Indexing, LSI）模型（Deerwester 等, 1988），随后被重新表述为**潜在语义分析**（Latent Semantic Analysis, LSA）（Deerwester 等, 1990）。在 LSA 中，对词–文档矩阵（每个单元格按对数频次加权，并通过熵进行归一化）应用**奇异值分解**（Singular Value Decomposition, SVD），然后取前 300 个维度作为 LSA 嵌入。SVD 是一种找出数据集中最重要维度的方法——即数据变化最大的那些方向。LSA 随后被迅速广泛应用于多个领域：作为认知模型（Landauer 和 Dumais, 1997）、拼写检查（Jones 和 Martin, 1997）、语言建模（Bellegarda, 1997；Coccaro 和 Jurafsky, 1998；Bellegarda, 2000）、形态归纳（Schone 和 Jurafsky, 2000, 2001b）、多词表达式识别（MWEs）（Schone 和 Jurafsky, 2001a）以及作文自动评分（Rehder 等, 1998）。与此同时，Schütze（1992b）也独立开发了相关模型，并将其应用于词义消歧任务。LSA 还首次将嵌入用于概率分类器中，例如 Schütze 等人（1995）提出的基于逻辑回归的文档路由系统。不久之后，Schütze（1992b）又提出对**词–词矩阵**（而非词–文档矩阵）进行 SVD，以此作为自然语言处理中的语义模型。他将 SVD 生成的低秩（97 维）嵌入用于词义消歧任务，分析了所得语义空间的结构，并提出了诸如丢弃高阶维度等可能的技术改进（参见 Schütze, 1997）。

在早期 SVD 工作的基础上，后续又发展出多种替代性矩阵模型，包括**概率潜在语义索引**（Probabilistic Latent Semantic Indexing, PLSI）（Hofmann, 1999）、**潜在狄利克雷分配**（Latent Dirichlet Allocation, LDA）（Blei 等, 2003）以及**非负矩阵分解**（Non-negative Matrix Factorization, NMF）（Lee 和 Seung, 1999）。

据考证，“嵌入”（embedding）一词最早由 Landauer 等人（1997）在 LSA 社群中使用，其含义源自数学中的“嵌入”概念，即从一个空间或数学结构到另一个的映射。在 LSA 中，“词嵌入”最初指的是从稀疏计数向量空间到 SVD 所得稠密潜在向量空间的映射过程。尽管该词最初强调的是“映射”本身，但后来发生了转喻性演变，如今通常指代映射后得到的潜在空间中的稠密向量——我们当前正是在这个意义上使用“嵌入”一词。

进入下一个十年，Bengio 等人（2003, 2006）证明，神经语言模型也可在词预测任务中同时学习出词嵌入。随后，Collobert 和 Weston（2007, 2008）以及 Collobert 等人（2011）进一步展示了嵌入可用于多种 NLP 任务中的词义表示。Turian 等人（2010）比较了不同类型嵌入在不同 NLP 任务中的效果。Mikolov 等人（2011）证明循环神经网络可用作语言模型。而 Mikolov 等人（2013a）则提出简化这类神经网络语言模型的隐藏层，从而创造出 **skip-gram**（以及 CBOW）算法。负采样（negative sampling）训练方法则由 Mikolov 等人（2013b）提出。关于静态嵌入及其参数设置，已有大量综述文献（Bullinaria 和 Levy, 2007, 2012；Lapesa 和 Evert, 2014；Kiela 和 Clark, 2014；Levy 等, 2015）。

若想更深入理解向量在信息检索中的作用（包括如何比较查询与文档、tf-idf 的更多细节，以及大规模数据集下的扩展问题），请参阅 Manning 等人（2008）及本书第 14 章。Kim（2019）提供了一篇清晰全面的 word2vec 教程。Cruse（2004）则是一本关于词汇语义学的优秀入门语言学教材。


<nav class="pagination justify-content-between">
<a href="../ch6-12">6.12 向量模型的评估</a>
<a href="../">目录</a>
<a href="../ch9">第9章 Transformer</a>
</nav>

