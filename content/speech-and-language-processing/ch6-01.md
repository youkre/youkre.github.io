---
title: "6.1 Lexical Semantics"
summary: ""
date: 
---

Let’s begin by introducing some basic principles of word meaning. How should
we represent the meaning of a word? In the n-gram models of Chapter 3, and in
classical NLP applications, our only representation of a word is as a string of letters,
or an index in a vocabulary list. This representation is not that different from a
tradition in philosophy, perhaps you’ve seen it in introductory logic classes, in which
the meaning of words is represented by just spelling the word with small capital
letters; representing the meaning of “dog” as DOG , and “cat” as CAT, or by using an
apostrophe (DOG ’).

Representing the meaning of a word by capitalizing it is a pretty unsatisfactory
model. You might have seen a version of a joke due originally to semanticist Barbara
Partee (Carlson, 1977):

> Q: What’s the meaning of life?
> A: LIFE ’

Surely we can do better than this! After all, we’ll want a model of word meaning
to do all sorts of things for us. It should tell us that some words have similar mean-
ings (*cat* is similar to *dog*), others are antonyms (*cold* is the opposite of *hot*), some
have positive connotations (*happy*) while others have negative connotations (*sad*). It
should represent the fact that the meanings of *buy*, *sell*, and *pay* offer differing per-
spectives on the same underlying purchasing event. (If I buy something from you,
you’ve probably sold it to me, and I likely paid you.) More generally, a model of
word meaning should allow us to draw inferences to address meaning-related tasks
like question-answering or dialogue.

In this section we summarize some of these desiderata, drawing on results in the
linguistic study of word meaning, which is called **lexical semantics**; we’ll return to
and expand on this list in Appendix G and Chapter 21.

**Lemmas and Senses** Let’s start by looking at how one word (we’ll choosemouse)
might be deﬁned in a dictionary (simpliﬁed from the online dictionary WordNet):

```text
mouse (N)
1. any of numerous small rodents...
2. a hand-operated device that controls a cursor...
```

Here the form mouse is the **lemma**, also called the **citation form**. The form
*mouse* would also be the lemma for the word *mice*; dictionaries don’t have separate
deﬁnitions for inﬂected forms like *mice*. Similarly *sing* is the lemma for *sing*, *sang*,
*sung*. In many languages the inﬁnitive form is used as the lemma for the verb, so
Spanish *dormir* “to sleep” is the lemma *forduermes* “you sleep”. The speciﬁc forms
*sung* or *carpets* or *sing* or *duermes* are called **wordforms**.

As the example above shows, each lemma can have multiple meanings; the
lemma *mouse* can refer to the rodent or the cursor control device. We call each
of these aspects of the meaning of *mouse* a **word sense**. The fact that lemmas can
be **polysemous** (have multiple senses) can make interpretation difﬁcult (is someone
who types “mouse info” into a search engine looking for a pet or a tool?). Chap-
ter 11 and Appendix G will discuss the problem of polysemy, and introduce
**word sense disambiguation**, the task of determining which sense of a word is being used
in a particular context.

**Synonymy** One important component of word meaning is the relationship be-
tween word senses. For example when one word has a sense whose meaning is
identical to a sense of another word, or nearly identical, we say the two senses of
those two words are **synonyms**. Synonyms include such pairs as

```text
couch/sofa vomit/throw up ﬁlbert/hazelnut car/automobile
```

A more formal deﬁnition of synonymy (between words rather than senses) is that
two words are synonymous if they are substitutable for one another in any sentence
without changing the *truth conditions* of the sentence, the situations in which the
sentence would be true.

While substitutions between some pairs of words like *car / automobile* or 
*water* / $H_2O$ are truth preserving, the words are still not identical in meaning. Indeed,
probably no two words are absolutely identical in meaning. One of the fundamental
tenets of semantics, called the **principle of contrast**(Girard 1718, Br´eal 1897, Clark
1987), states that a difference in linguistic form is always associated with some dif-
ference in meaning. For example, the word $H_2O$ is used in scientiﬁc contexts and
would be inappropriate in a hiking guide -- *water* would be more appropriate -- and
this genre difference is part of the meaning of the word. In practice, the word syn-
onym is therefore used to describe a relationship of approximate or rough synonymy.

**Word Similarity** While words don’t have many synonyms, most words do have
lots of *similar* words. Cat is not a synonym of *dog*, but *cats* and dogs are certainly
similar words. In moving from synonymy to similarity, it will be useful to shift from
talking about relations between word senses (like synonymy) to relations between
words (like similarity). Dealing with words avoids having to commit to a particular
representation of word senses, which will turn out to simplify our task.

The notion of word **similarity** is very useful in larger semantic tasks. Knowing
how similar two words are can help in computing how similar the meaning of two
phrases or sentences are, a very important component of tasks like question answer-
ing, paraphrasing, and summarization. One way of getting values for word similarity
is to ask humans to judge how similar one word is to another. A number of datasets
have resulted from such experiments. For example the SimLex-999 dataset (Hill
et al., 2015) gives values on a scale from 0 to 10, like the examples below, which
range from near-synonyms (*vanish*, *disappear*) to pairs that scarcely seem to have
anything in common (*hole*, *agreement*):

| --- | --- | --- |
| vanish | disappear | 9.8 |
| belief | impression | 5.95 |
| muscle | bone | 3.65 |
| modest | ﬂexible | 0.98 |
| hole | agreement | 0.3 |

**Word Relatedness** The meaning of two words can be related in ways other than
similarity. One such class of connections is called word **relatedness** (Budanitsky
and Hirst, 2006), also traditionally called word **association** in psychology.

Consider the meanings of the words coffee and cup. Coffee is not similar to cup;
they share practically no features (coffee is a plant or a beverage, while a cup is a
manufactured object with a particular shape). But coffee and cup are clearly related;
they are associated by co-participating in an everyday event (the event of drinking
coffee out of a cup). Similarly *scalpel* and *surgeon* are not similar but are related
eventively (a surgeon tends to make use of a scalpel).

One common kind of relatedness between words is if they belong to the same
**semantic ﬁeld**. A semantic ﬁeld is a set of words which cover a particular semantic
domain and bear structured relations with each other. For example, words might be
related by being in the semantic ﬁeld of hospitals ( *surgeon*, *scalpel*, *nurse*,
*anesthetic*, *hospital*), restaurants (*waiter*, *menu*, *plate*, *food*, *chef*), or houses (*door*, *roof*,
*kitchen*, *family*, *bed*). Semantic ﬁelds are also related to **topic models**, like
**Latent Dirichlet Allocation, LDA**, which apply unsupervised learning on large sets of texts
to induce sets of associated words from text. Semantic ﬁelds and topic models are
very useful tools for discovering topical structure in documents.

In Appendix G we’ll introduce more relations between senses like **hypernymy**
or **IS-A**, **antonymy** (opposites) and **meronymy** (part-whole relations).

**Semantic Frames and Roles** Closely related to semantic ﬁelds is the idea of a
**semantic frame**. A semantic frame is a set of words that denote perspectives orsemantic frame
participants in a particular type of event. A commercial transaction, for example,
is a kind of event in which one entity trades money to another entity in return for
some good or service, after which the good changes hands or perhaps the service is
performed. This event can be encoded lexically by using verbs like *buy* (the event
from the perspective of the buyer), *sell* (from the perspective of the seller), *pay*
(focusing on the monetary aspect), or nouns like *buyer*. Frames have semantic roles
(like *buyer*, *seller*, *goods*, *money*), and words in a sentence can take on these roles.

Knowing that *buy* and *sell* have this relation makes it possible for a system to
know that a sentence like *Sam bought the book from Ling* could be paraphrased as
*Ling sold the book to Sam*, and that Sam has the role of the *buyer* in the frame and
Ling the *seller*. Being able to recognize such paraphrases is important for question
answering, and can help in shifting perspective for machine translation.

**Connotation** Finally, words have affective meanings or **connotations**. The word
*connotation* has different meanings in different ﬁelds, but here we use it to mean the
aspects of a word’s meaning that are related to a writer or reader’s emotions, senti-
ment, opinions, or evaluations. For example some words have positive connotations
(*wonderful*) while others have negative connotations (*dreary*). Even words whose
meanings are similar in other ways can vary in connotation; consider the difference
in connotations between *fake*, *knockoff*, *forgery*, on the one hand, and *copy*, *replica*,
*reproduction* on the other, or *innocent* (positive connotation) and *naive* (negative
connotation). Some words describe positive evaluation (great, love) and others neg-
ative evaluation (*terrible*, *hate*). Positive or negative evaluation language is called
**sentiment**, as we saw in Chapter 4, and word sentiment plays a role in importantsentiment
tasks like sentiment analysis, stance detection, and applications of NLP to the lan-
guage of politics and consumer reviews.

Early work on affective meaning (Osgood et al., 1957) found that words varied
along three important dimensions of affective meaning:

> **valence**: the pleasantness of the stimulus
> **arousal**: the intensity of emotion provoked by the stimulus
> **dominance**: the degree of control exerted by the stimulus

Thus words like *happy* or *satisﬁed* are high on valence, while *unhappy* or
*annoyed* are low on valence. *Excited* is high on arousal, while *calm* is low on arousal.
*Controlling* is high on dominance, while *awed* or *inﬂuenced* are low on dominance.
Each word is thus represented by three numbers, corresponding to its value on each
of the three dimensions:

| | Valence | Arousal | Dominance |
| courageous | 8.05 | 5.5 | 7.38 |
| music | 7.67 | 5.57 | 6.5 |
| heartbreak | 2.45 | 5.65 | 3.58 |
| cub | 6.71 | 3.95 | 4.24 |

Osgood et al. (1957) noticed that in using these 3 numbers to represent the
meaning of a word, the model was representing each word as a point in a three-
dimensional space, a vector whose three dimensions corresponded to the word’s
rating on the three scales. This revolutionary idea that word meaning could be rep-
resented as a point in space (e.g., that part of the meaning of *heartbreak* can be
represented as the point [2.45,5.65,3.58]) was the ﬁrst expression of the vector se-
mantics models that we introduce next.


<nav class="pagination justify-content-between">
<a href="../ch6">CHAPTER 6 Vector Semantics and Embeddings</a>
<a href="../">目录</a>
<span></span>
</nav>

