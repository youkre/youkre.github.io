---
title: "6.6 点互信息（PMI）"
summary: ""
date: 2025-10-14T09:15:00+08:00
---

一种替代tf-idf的加权函数是PPMI（正点互信息），它用于词-词矩阵，即向量的维度对应词语而非文档的情况。PPMI基于这样一种核心理念：衡量两个词之间关联性的最佳方式，是看它们在语料库中的共现频率比随机预期的共现频率高出多少。

**点互信息**（Pointwise Mutual Information, PMI）（Fano, 1961）[^1] 是自然语言处理中最重要概念之一。它衡量的是两个事件 $x$ 和 $y$ 的共现频率，与假设它们相互独立时的预期频率相比如何：

[^1] PMI基于两个随机变量 $X$ 和 $Y$ 之间的互信息（mutual information），定义为：

    $$
    I(X,Y ) = \sum_x \sum_y P(x,y)\log_2\frac{P(x,y)}{P(x)P(y)}
    \tag{6.15}
    $$

    Fano 使用“互信息”（mutual information）指代我们现在称为“点互信息”（pointwise mutual information）的概念，而用“互信息的期望”（expectation of the mutual information）指代我们现在称为“互信息”（mutual information）的概念，这在术语上有点混淆。

$$
I(x,y) = \log_2\frac{P(x,y)}{P(x)P(y)}
\tag{6.16}
$$

目标词 $w$ 与上下文词 $c$ 之间的点互信息（Church and Hanks 1989, Church and Hanks 1990）定义如下：

$$
PMI(w,c) = \log_2\frac{P(w,c)}{P(w)P(c)}
\tag{6.17}
$$

分子告诉我们两个词一起出现的频率（假设使用最大似然估计MLE来计算概率）。分母告诉我们，在假设两个词独立出现的情况下，**预期**它们共现的频率；回忆一下，两个独立事件同时发生的概率就是这两个事件概率的乘积。因此，这个比值给出了两个词的共现频率比随机预期高出多少的估计。当我们需要找出强关联的词语时，PMI是一个有用的工具。

PMI值的范围从负无穷到正无穷。但负的PMI值（意味着两词的共现频率比随机预期**更低**）往往不可靠，除非我们的语料库非常庞大。要判断两个各自概率均为 $10^{-6}$ 的词是否比随机情况更少共现，我们需要确信它们共同出现的概率显著低于 $10^{-12}$，而这种精度需要极大的语料库。此外，尚不清楚人类判断是否甚至能够评估这种“不相关性”的分数。因此，更常见的做法是使用**正点互信息**（Positive PMI，简称 **PPMI**），它将所有负的PMI值替换为零（Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994）[^2]：

[^2] 正点互信息也巧妙地解决了零计数的问题，用0替代 $log(0)$ 产生的无穷大。

$$
PPMI(w,c) = \max\left(\log_2\frac{P(w,c)}{P(w)P(c)}, 0\right)
\tag{6.18}
$$

更正式地讲，假设我们有一个共现矩阵 $F$，包含 $W$ 行（词语）和 $C$ 列（上下文），其中 $f_{ij}$ 表示词 $w_i$ 与上下文 $c_j$ 共现的次数。这可以转换为一个PPMI矩阵，其中 $PPMI_{ij}$ 给出词 $w_i$ 与上下文 $c_j$ 的PPMI值（也可表示为 $PPMI(w_i, c_j)$ 或 $PPMI(w = i, c = j)$），如下所示：

$$
\begin{aligned}
p_{ij} &= \frac{f_{ij}}{\sum_{i=1}^W\sum_{j=1}^C f_{ij}} \\
p_{i*} &= \frac{\sum_{j=1}^C f_{ij}}{\sum_{i=1}^W\sum_{j=1}^C f_{ij}} \\
p_{*j} &= \frac{\sum_{i=1}^W f_{ij}}{\sum_{i=1}^W\sum_{j=1}^C f_{ij}}
\end{aligned}
\tag{6.19}
$$

$$
PPMI_{ij} = \max\left(\log_2\frac{p_{ij}}{p_{i*}p_{*j}}, 0\right)
\tag{6.20}
$$

我们看一些PPMI的计算示例。我们将使用图6.10，它重复了图6.6并添加了所有计数的边际值，并假设为了计算方便，这些是唯一相关的词/上下文。

| | computer | data | result | pie | sugar | count(w) |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **cherry** | 2 | 8 | 9 | 442 | 25 | 486 |
| **strawberry** | 0 | 0 | 1 | 60 | 19 | 80 |
| **digital** | 1670 | 1683 | 85 | 5 | 4 | 3447 |
| **information** | 3325 | 3982 | 378 | 5 | 13 | 7703 |
| **count(context)** | 4997 | 5673 | 473 | 512 | 61 | 11716 |

**图6.10** 维基百科语料库中四个词在五个上下文中的共现计数，以及边际值，为计算目的假设没有其他词/上下文重要。

例如，我们可以计算 $PPMI(\text{information}, \text{data})$，假定图6.6包含了所有相关的词上下文/维度，计算如下：

$$
\begin{aligned}
P(w=\text{information}, c=\text{data}) &= \frac{3982}{11716} = 0.3399 \\
P(w=\text{information}) &= \frac{7703}{11716} = 0.6575 \\
P(c=\text{data}) &= \frac{5673}{11716} = 0.4842 \\
PPMI(\text{information},\text{data}) &= \log_2(0.3399/(0.6575 \times 0.4842)) = 0.0944
\end{aligned}
$$

图6.11展示了根据图6.10的计数计算出的联合概率，图6.12展示了PPMI值。不出所料，“cherry”（樱桃）和“strawberry”（草莓）与“pie”（派）和“sugar”（糖）高度相关，而“data”（数据）与“information”（信息）有轻微关联。

| | computer | data | result | pie | sugar | p(w) |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **cherry** | 0.0002 | 0.0007 | 0.0008 | 0.0377 | 0.0021 | 0.0415 |
| **strawberry** | 0.0000 | 0.0000 | 0.0001 | 0.0051 | 0.0016 | 0.0068 |
| **digital** | 0.1425 | 0.1436 | 0.0073 | 0.0004 | 0.0003 | 0.2942 |
| **information** | 0.2838 | 0.3399 | 0.0323 | 0.0004 | 0.0011 | 0.6575 |
| **p(context)** | 0.4265 | 0.4842 | 0.0404 | 0.0437 | 0.0052 | |

**图6.11** 将图6.6中的计数替换为联合概率，右侧一列为行边际，底部一行为列边际。

| | computer | data | result | pie | sugar |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **cherry** | 0 | 0 | 0 | 4.38 | 3.30 |
| **strawberry** | 0 | 0 | 0 | 4.10 | 5.51 |
| **digital** | 0.18 | 0.01 | 0 | 0 | 0 |
| **information** | 0.02 | 0.09 | 0.28 | 0 | 0 |

**图6.12** PPMI矩阵，显示词与上下文词之间的关联性，由图6.11的计数计算得出。注意，大多数PPMI值为0的情况是原本PMI为负值的；例如 $PMI(\text{cherry},\text{computer}) = -6.7$，意味着“cherry”和“computer”在维基百科上的共现频率比随机预期要低，而PPMI将负值替换为零。

PMI的问题在于它偏向于低频事件；非常罕见的词往往具有很高的PMI值。一种减少这种对低频事件偏倚的方法是稍微改变 $P(c)$ 的计算方式，使用一个不同的函数 $P_\alpha(c)$，将上下文词的概率提升到 $\alpha$ 次幂：

$$
PPMI_\alpha (w,c) = \max\left(\log_2\frac{P(w,c)}{P(w)P_\alpha(c)}, 0\right)
\tag{6.21}
$$

$$
P_\alpha(c) = \frac{\text{count}(c)^\alpha}{\sum_c \text{count}(c)^\alpha}
\tag{6.22}
$$

Levy 等人（2015）发现，设置 $\alpha = 0.75$，能够在多种任务上提升词嵌入的性能（借鉴了下文公式6.32中描述的skip-gram所使用的类似加权方法）。这种方法有效，因为将计数提升到 $\alpha = 0.75$ 次幂会增加对罕见上下文分配的概率，从而降低它们的PMI值（当 $c$ 罕见时，$P_\alpha(c) > P(c)$）。

另一种可能的解决方案是拉普拉斯平滑（Laplace smoothing）：在计算PMI之前，给每个计数加上一个小的常数 $k$（常用值为0.1-3），从而缩小（折扣）所有非零值。$k$ 越大，非零计数的折扣越多。


<nav class="pagination justify-content-between">
<a href="../ch6-05">6.5 TF-IDF：向量中词语的加权</a>
<a href="../">目录</a>
<a href="../ch6-07">6.7 tf-idf 或 PPMI 向量模型的应用</a>
</nav>

