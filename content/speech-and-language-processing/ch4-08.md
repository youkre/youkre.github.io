---
title: "4.8 多项逻辑回归中的学习"
summary: ""
date: 2025-12-23T08:49:00.000Z
---

多项逻辑回归的损失函数将二元逻辑回归的损失函数从 2 个类别推广到了 $K$ 个类别。
回顾一下，二元逻辑回归的交叉熵损失函数（重复自公式 5.23）为：

$$
L_{CE}(\hat{y}, y) = -\log p(y|x) = -[y \log \hat{y} + (1 - y)\log(1 - \hat{y})]
\tag{4.36}
$$

多项逻辑回归的损失函数将公式 4.36 中的两项（一项在 $y = 1$ 时非零，另一项在 $y = 0$ 时非零）推广为 $K$ 项。
如上所述，对于多项回归，将 $y$ 和 $\hat{y}$ 都表示为向量。
真实标签 $y$ 是一个包含 $K$ 个元素的向量，每个元素对应一个类别，当正确类别为 $c$ 时，$y_c = 1$，而 $y$ 的其余所有元素均为 0。
分类器将生成一个包含 $K$ 个元素的估计向量 $\hat{y}$，其中每个元素 $\hat{y}_k$ 表示估计的概率 $p(y_k = 1|x)$。

把二元逻辑回归推广，可以得到单个样本 $x$ 的损失函数，它是 $K$ 个输出类别的对数之和，每个项由指示函数 $y_k$ 加权（公式 4.37）。
这实际上就等于正确类别 $c$ 的负对数概率（公式 4.38）：

$$
\begin{align*}
L_{CE}(\hat{y}, y) &= -\sum_{k=1}^K y_k \log \hat{y}_k \tag{4.37} \\
&= -\log \hat{y}_c \quad \text{（其中 } c \text{ 是正确类别）} \tag{4.38} \\
&= -\log \hat{p}(y_c = 1|x) \quad \text{（其中 } c \text{ 是正确类别）} \\
&= -\log \frac{\exp(\mathbf{w}_c \cdot \mathbf{x} + b_c)}{\sum_{j=1}^K \exp(\mathbf{w}_j \cdot \mathbf{x} + b_j)} \quad \text{（} c \text{ 是正确类别）}  \tag{4.39}
\end{align*}
$$

我们是如何从公式 4.37 得到公式 5.38 的呢？
因为只有一个类别（记为 $c$）是正确的，向量 $\mathbf{y}$ 仅在 $k = c$ 时取值为 1，即 $y_c = 1$，而对于所有 $j \neq c$，有 $y_j = 0$。
这意味着公式 (5.45) 求和中的所有项都为 0，除了对应正确类别 $c$ 的那一项。
因此，交叉熵损失就是对应正确类别的输出概率的对数的负值，因此也将公式 4.38 称为**负对数似然损失**（negative log likelihood loss）。

当然，对于梯度下降，我们不需要损失本身，而是需要它的梯度。
单个样本的梯度与公式 4.30 中看到的二元逻辑回归梯度 $(\hat{y} - y)x$ 非常相似。
考虑梯度的一部分，即对单个权重的偏导数。
对于每个类别 $k$，输入 $x$ 的第 $i$ 个元素的权重为 $w_{k,i}$。损失函数对 $w_{k,i}$ 的偏导数是多少？
这个导数实际上是类别 $k$ 的真实值（为 1 或 0）与分类器输出的类别 $k$ 的概率之间的差值，再乘以对应于类别 $k$ 的权重向量中第 $i$ 个元素的输入值 $x_i$：

$$
\begin{align*}
\frac{\partial L_{CE}}{\partial w_{k,i}} &= - (y_k - \hat{y}_k) x_i \\
&= - (y_k - p(y_k = 1|x)) x_i \\
&= - \left(y_k - \frac{\exp(\mathbf{w}_k \cdot \mathbf{x} + b_k)}{\sum_{j=1}^K \exp(\mathbf{w}_j \cdot \mathbf{x} + b_j)}\right) x_i \tag{4.40}
\end{align*}
$$

第 6 章介绍神经网络时，将再次讨论 softmax 回归的梯度，介绍该梯度的推导过程（见公式 6.35–公式 6.43）。


<nav class="pagination justify-content-between">
<a href="../ch4-07">4.7 梯度下降</a>
<a href="../">目录</a>
<a href="../ch4-09">4.9 评估指标：精确率、召回率与 F 值</a>
</nav>

