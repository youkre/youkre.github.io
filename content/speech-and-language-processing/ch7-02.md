---
title: "7.2 文本的条件生成：基本原理"
summary: ""
date: 2025-12-26T20:38:00+08:00
---

语言模型背后的一个基本原理是，几乎所有我们想用语言完成的任务，都可以建模为文本的**条件生成**（conditional generation）。
（这里特指**解码器**语言模型，即本章和下一章讨论的对象。）

所谓条件生成，是指在给定一段输入文本的条件下生成新文本。
具体来说，我们向大语言模型（LLM）提供一段输入文本，称为**提示**（prompt），然后让 LLM 以该提示及其后续已生成的词元为条件，逐个生成新的词元。  
生成过程首先计算在已有上下文 $w_{<i}$ 下，下一个词元 $w_i$ 的概率 $P(w_i \mid w_{<i})$，再从该概率分布中采样出一个词元。

后续章节会详述所有技术细节，但本节的目标只是建立直观理解。
仅仅通过计算下一个词的概率，LLM 如何完成各种不同的语言任务？

假设我们要做情感分析这类分类任务。
我们可以将其视为条件生成问题，给语言模型如下提示：

> The sentiment of the sentence “I like Jackie Chan” is:

然后比较接下来出现 “positive” 和 “negative” 这两个词元的条件概率，看哪个更高。
如图 7.4 所示，我们对比以下两个概率：

```text
P(“positive” | “The sentiment of the sentence ‘I like Jackie Chan’ is:”)
P(“negative” | “The sentiment of the sentence ‘I like Jackie Chan’ is:”)
```

![](/images/speech-and-language-processing/slp-fig-7-4.png)

**图 7.4** 计算在给定前缀后，`positive` 与 `negative` 两个词元出现的概率。

如果 `positive` 的概率更高，我们就判定该句情感为正面；反之，若  `negative`概率更高，则判定为负面。

同样的思路也适用于问答任务：系统接收一个问题，需给出文本形式的答案。
我们可以将问答任务转化为词元预测问题：向语言模型输入一个问题，并附上一个表示答案即将开始的标记（如 `A:`），例如：

> Q: Who wrote the book “The Origin of Species”? A:

接着，我们让语言模型基于该前缀计算下一个词元的概率分布：

```text
P(w | Q: Who wrote the book “The Origin of Species”? A: )
```

并观察哪些词元 $w$ 具有高概率。
如图 7.5 所示，我们可能会发现 `Charles` 的概率非常高。  
如果我们选择 `Charles` 并将其加入前缀，再计算新前缀下的下一个词元概率：

```text
P(w | Q: Who wrote the book “The Origin of Species”? A: Charles)
```

此时，`Darwin` 很可能成为概率最高的词元，于是我们选择它作为答案的下一部分。

![](/images/speech-and-language-processing/slp-fig-7-5.png)

**图 7.5** 通过计算问题前缀之后各词元的概率来回答问题；在此例中，正确词元 `Charles` 拥有最高概率。


<nav class="pagination justify-content-between">
<a href="../ch7-01">7.1 语言模型的三种架构</a>
<a href="../">目录</a>
<a href="../ch7-03">7.3 提示</a>
</nav>

