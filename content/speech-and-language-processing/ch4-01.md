---
title: "4.1 机器学习与分类"
summary: ""
date: 2025-12-22T15:01:00+08:00
---

分类的目标是接收一个单一输入（我们将每个输入称为一个 **样本**（observation）），从中提取一些有用的 **特征**（features）或属性，并据此将该样本 **分类**（classify）到一组离散类别中的某一个。
我们用 $x$ 表示输入，输出则来自一个固定的类别集合 $Y = \{y_1, y_2, \dots, y_M\}$。
我们的目标是返回一个预测类别 $\hat{y} \in Y$。
符号 $\hat{y}$（称为 **hat** 或 **尖音符**）用于表示估计值或预测值。
有时你也会看到输出类别集合被记作 $C$ 而非 $Y$。

以情感分析为例，输入 $x$ 可能是一篇评论或其他文本，而输出集合 $Y$ 可能是：

$$
\{\texttt{positive}, \texttt{negative}\}
$$

或

$$
\{0, 1\}
$$

对于语言识别（language identification）任务，输入可能是一段待判定语种的文本，输出集合 $Y$ 则是所有可能的语言，例如：

$$
Y = \{\texttt{Abkhaz}, \texttt{Ainu}, \texttt{Albanian}, \texttt{Amharic}, ..., \texttt{Zulu}, \texttt{Zuñi}\}
$$

实现分类的方法有很多。
一种方法是使用人工编写的规则。
例如，我们可以制定如下规则：

> 如果单词 “love” 出现在 x 中，且其前未出现单词 “don’t”，则分类为 positive。

人工规则可以作为现代自然语言处理系统的一部分组件。例如，在情感分析中，人工整理的正面词与负面词列表就是此类规则的应用，我们将在下文进一步讨论。
然而，规则往往较为脆弱，随着场景或数据随时间变化，规则可能失效；而且在许多任务中，不同特征之间存在复杂的交互关系（比如上述规则中 “don’t” 对 “love” 的否定作用），人类很难设计出在各种情况下都有效的规则。

另一种我们稍后将介绍的方法是：通过提示（prompting）来要求大语言模型给某些文本加上标签。
提示工程可能非常强大，但也存在明显缺陷：大语言模型常常会产生“幻觉”（hallucinate），可能无法解释为何选择某个特定类别。

正因如此，目前最主流的分类方法是采用 **监督式机器学习**（supervised machine learning）。
监督式机器学习是一种范式：除了输入和输出类别集合外，我们还需要一个 **带标注的训练集**（labeled training set）和一个 **学习算法**（learning algorithm）。
我们在第3章曾提到过训练集，当时将其作为计算 n-gram 统计量的基础。
但在监督式机器学习中，训练集是**带标签的**，即其中包含一组输入样本，每个样本都关联着正确的输出（即“监督信号”）。
一般来说，我们可以将文本分类任务中的训练集表示为 $m$ 个输入/输出对的集合，其中每个输入 $x$ 是一段文本，每个都由人工标注了对应的正确类别（即真实标签）：

$$
\text{训练集}: \{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),..., (x^{(m)},y^{(m)})\}
\tag{4.1}
$$

我们将使用带括号的上标来指代训练集中的各个样本（或实例）。
因此，在情感分类任务中，训练集可能由一系列句子或其他文本组成，每个都带有其正确的情感标签。

我们的目标是从这个训练集中学习一个分类器，使其能够将**新的**输入 $x$ 映射到其正确的类别 $y \in Y$。
它通过从这些训练句子中学习识别有用的特征（例如“awesome”或“awful”这样的词）来实现这一目标。
**概率分类器**（Probabilistic classifiers）是机器学习分类器的一个子类：它们不仅给出预测结果（即该样本属于哪个类别），还会输出该观测属于各个类别的概率。
这种完整的类别概率分布对下游决策非常有用；在系统融合时，避免过早做出离散决策往往能带来更好的整体性能。

实现这一监督式机器学习任务的算法有很多（如朴素贝叶斯、支持向量机、神经网络、微调后的大语言模型等），但逻辑回归具备前文所述的多重优势，因此我们将重点介绍它！
任何机器学习分类器通常包含以下四个组成部分：

1. 输入**的特征表示**（feature representation）  
   对每个输入样本 $x^{(i)}$，我们将其表示为一个特征向量 $[x_1, x_2, \dots, x_n]$。
   我们通常用 $x^{(j)}_i$ 表示第 $j$ 个输入的第 $i$ 个特征，有时简写为 $x_i$；此外也会见到其他记法，如 $f_i$、$f_i(x)$，或在多分类场景下的 $f_i(c, x)$。

2. 分类函数（classification function）  
   该函数通过计算条件概率 $P(y|x)$ 来输出预测类别 $\hat{y}$。我们将引入用于分类的 **sigmoid 函数** 和 **softmax 函数**。

3. **目标函数**（objective function）  
   这是我们希望在学习过程中优化的函数，通常涉及最小化一个与训练样本误差相对应的损失函数。
   我们将介绍 **交叉熵损失函数**（cross-entropy loss function）。

4. **优化算法**（optimization algorithm）  
   用于优化目标函数的算法。我们将介绍 **随机梯度下降**（stochastic gradient descent, SGD）。

从最高层次来看，逻辑回归——实际上所有监督式机器学习分类器——都包含两个阶段：

**训练阶段**（training）：  
使用随机梯度下降和交叉熵损失函数来训练系统（对逻辑回归而言，即学习下文将介绍的权重参数 $w$ 和偏置 $b$）。

**测试阶段**（test）：  
给定一个测试样本 $x$，我们计算其属于每个类别 $y_i$ 的概率 $P(y_i|x)$，并返回概率更高的标签（例如 $y = 1$ 或 $y = 0$）。

逻辑回归可用于将样本分为两类（如“正面情感”与“负面情感”），也可扩展至多类分类。
由于二分类情形的数学形式更简单，我们将在接下来几节中首先介绍这一特例，从 **sigmoid 函数** 开始；随后在第 4.4 节转向处理多于两类的情形，即 **多项逻辑回归**（multinomial logistic regression），并引入 **softmax 函数**。


<nav class="pagination justify-content-between">
<a href="../ch4">第4章 罗辑回归与文本分类</a>
<a href="../">目录</a>
<a href="../ch4-02">4.2 Sigmoid 函数</a>
</nav>

