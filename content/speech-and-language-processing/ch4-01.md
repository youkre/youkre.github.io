---
title: "4.1 朴素贝叶斯分类器"
summary: ""
date: 2025-09-20T08:29:00+08:00
---

在本节中，我们将介绍**多项式朴素贝叶斯分类器（multinomial naive Bayes classifier）**，之所以这样命名，是因为它是一种贝叶斯分类器，并对特征之间的相互作用做出了一个简化的（即“朴素的”）假设。

该分类器的核心思想如图 4.1 所示。我们将一篇文本文档表示为**词袋（bag of words）**，即一个无序的词汇集合，忽略词语在文档中的位置，只保留它们在文档中出现的频率。在图中的示例中，我们并不记录“我爱这部电影”或“我会推荐它”等短语中词语的顺序，而是简单地记录：单词 *I* 在整个摘录中出现了 5 次，*it* 出现了 6 次，*love*、*recommend* 和 *movie* 各出现 1 次，等等。

<figure>

![](/images/speech-and-language-processing/slp-fig-4-1.png)

<figcaption>

**图 4.1** 多项式朴素贝叶斯分类器应用于电影评论的直观示意图。忽略了词语的位置（即词袋假设），仅利用每个词的出现频率。

</figcaption>
</figure>

朴素贝叶斯是一种**概率型分类器**，这意味着对于一篇文档 $d$，它会在所有类别 $c \in C$ 中返回一个后验概率最大的类别 $\hat{c}$。在公式 (4.1) 中，我们使用上帽符号 $\hat{}$ 表示“我们对正确类别的估计”，并使用 **argmax** 表示选择使某个函数（此处为概率 $P(c|d)$）达到最大值的那个参数（此处为类别 $c$）。

$$
\hat{c} = \underset{c \in C}{\mathrm{argmax}}\, P(c|d) \tag{4.1}
$$

这种**贝叶斯推断（Bayesian inference）**的思想自贝叶斯（Bayes, 1763）的工作以来就已为人所知，并由 Mosteller 和 Wallace（1964）首次应用于文本分类。贝叶斯分类的直观思想是使用贝叶斯法则将公式 (4.1) 转换为其他一些具有有用性质的概率。贝叶斯法则如公式 (4.2) 所示，它提供了一种将任意条件概率 $P(x|y)$ 分解为另外三个概率的方法：

$$
P(x|y) = \frac{P(y|x)P(x)}{P(y)} \tag{4.2}
$$

然后我们可以将公式 (4.2) 代入公式 (4.1)，得到公式 (4.3)：

$$
\hat{c} = \underset{c \in C}{\mathrm{argmax}}\, P(c|d) = \underset{c \in C}{\mathrm{argmax}}\, \frac{P(d|c)P(c)}{P(d)} \tag{4.3}
$$

我们可以方便地简化公式 (4.3)，即去掉分母 $P(d)$。这是可行的，因为我们对每一个可能的类别都要计算 $\frac{P(d|c)P(c)}{P(d)}$。但 $P(d)$ 对于所有类别来说是相同的；我们始终是在为同一个文档 $d$ 寻找最可能的类别，而该文档的概率 $P(d)$ 是固定的。因此，我们可以选择使以下更简单公式最大化的类别：

$$
\hat{c} = \underset{c \in C}{\mathrm{argmax}}\, P(c|d) = \underset{c \in C}{\mathrm{argmax}}\, P(d|c)P(c) \tag{4.4}
$$

我们称朴素贝叶斯为**生成式（generative）**模型，因为我们可以将公式 (4.4) 解读为一种关于文档如何生成的隐含假设：首先从先验概率 $P(c)$ 中抽取一个类别，然后通过从条件概率 $P(d|c)$ 中抽样来生成文档中的词语。（事实上，我们可以想象通过遵循这一过程来生成人工文档，或至少生成它们的词频统计。）我们将在第 5 章进一步讨论生成式模型的这种直观理解。

回到分类问题：通过选择使两个概率乘积最大的类别，来计算给定某篇文档 $d$ 时最可能的类别 $\hat{c}$：即类别的**先验概率** $P(c)$ 和文档的**似然度** $P(d|c)$：

$$
\hat{c} = \underset{c \in C}{\mathrm{argmax}}\, \overbrace{P(d|c)}^{\text{似然度}} \overbrace{P(c)}^{\text{先验}} \tag{4.5}
$$

不失一般性，我们可以将文档 $d$ 表示为一组特征 $f_1, f_2, ..., f_n$：

$$
\hat{c} = \underset{c \in C}{\mathrm{argmax}}\, \overbrace{P(f_1,f_2,....,f_n|c)}^{\text{似然度}} \overbrace{P(c)}^{\text{先验}} \tag{4.6}
$$

不幸的是，公式 (4.6) 仍然难以直接计算：如果没有一些简化假设，估计每一种可能特征组合（例如，每一种可能的词及其位置组合）的概率将需要大量的参数和难以实现的大规模训练集。因此，朴素贝叶斯分类器引入了两个简化假设。

第一个是上文直观讨论过的**词袋（bag-of-words）假设**：我们假设词语的位置无关紧要，“love”这个词出现在文档的第一个、第二十个还是最后一个位置，对分类的影响都是一样的。因此，我们假设特征 $f_1, f_2, ..., f_n$ 只编码词语的身份（即词本身），而不编码其位置信息。

第二个通常被称为**朴素贝叶斯假设**：即**条件独立性假设**，假设在给定类别 $c$ 的条件下，各个特征的概率 $P(f_i|c)$ 是相互独立的，因此可以“朴素地”相乘，如下所示：

$$
P(f_1,f_2,....,f_n|c) = P(f_1|c) \cdot P(f_2|c) \cdot ... \cdot P(f_n|c) \tag{4.7}
$$

因此，朴素贝叶斯分类器所选择类别的最终公式为：

$$
c_{NB} = \underset{c \in C}{\mathrm{argmax}}\, P(c) \prod_{f \in F} P(f|c) \tag{4.8}
$$

为了将朴素贝叶斯分类器应用于文本，我们将如上所述，使用文档中的每个词作为特征，并通过一个索引遍历文档中的每一个词位置来考虑文档中的所有词：

$$
\text{positions} \leftarrow \text{测试文档中的所有词位置}
$$

$$
c_{NB} = \underset{c \in C}{\mathrm{argmax}}\, P(c) \prod_{i \in \text{positions}} P(w_i|c) \tag{4.9}
$$

与语言模型的计算类似，朴素贝叶斯的计算通常在**对数空间（log space）**中进行，以避免数值下溢（underflow）并提高计算速度。因此，公式 (4.9) 通常被表示为 [^1]：

$$
c_{NB} = \underset{c \in C}{\mathrm{argmax}}\, \log P(c) + \sum_{i \in \text{positions}} \log P(w_i|c) \tag{4.10}
$$

通过在对数空间中考虑特征，公式 (4.10) 将预测类别计算为输入特征的线性函数。使用输入的线性组合来做出分类决策的分类器——例如朴素贝叶斯和逻辑回归——被称为**线性分类器（linear classifiers）**。

[^1]: 在本书的实际使用中，若未特别指明对数的底数，我们将使用 log 表示自然对数（ln）。


<nav class="pagination justify-content-between">
<a href="../ch4">第4章 朴素贝叶斯、文本分类与情感分析</a>
<a href="../">目录</a>
<a href="../ch4-02">4.2 训练朴素贝叶斯分类器</a>
</nav>

