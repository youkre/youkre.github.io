---
title: "5.6 梯度下降"
summary: ""
date: 2025-10-07T08:56:00+08:00
---

使用梯度下降的目标是找到最优的权重：即最小化为模型定义的损失函数。在下面的公式 (5.25) 中，我们将明确表示交叉熵损失函数 $L_{CE}$ 是由权重参数化的。在机器学习中，我们通常将待学习的参数统称为 $\theta$；在逻辑回归中，$\theta = \{\mathbf{w}, b\}$。因此，目标是找到一组权重，使得损失函数在所有训练样本上的平均值最小：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmin}} \frac{1}{m} \sum_{i=1}^m L_{CE}( f(x^{(i)};\theta), y^{(i)})
\tag{5.25}
$$

该如何找到这个（或任何）损失函数的最小值呢？梯度下降是一种通过确定函数在参数空间 $\theta$ 中哪个方向上升最陡峭，然后朝相反方向移动，从而找到函数最小值的方法。其直观理解是：如果你在峡谷中徒步，想要最快地走到谷底的河流处，可能会环顾四周，找到地面坡度最陡的方向，然后朝着那个方向下坡行走。

对于逻辑回归而言，这个损失函数恰好是**凸函数**（convex）。凸函数最多只有一个最小值，不存在会陷入的局部最小值，因此无论从哪个点开始，梯度下降都能保证找到全局最小值。（相比之下，多层神经网络的损失函数是非凸的，梯度下降在训练神经网络时可能会陷入局部最小值，而永远无法找到全局最优解。）

尽管该算法（以及梯度的概念）是为向量方向设计的，我们先考虑一个更简单的可视化场景：假设我们系统的参数只是一个标量 $w$，如图5.4所示。

假设随机初始化 $\mathbf{w}$ 为某个值 $w_1$，并假设损失函数 $L$ 恰好具有图5.4所示的形状。此时，我们需要算法告诉我们，在下一次迭代中，是应该向左移动（使 $w_2$ 小于 $w_1$）还是向右移动（使 $w_2$ 大于 $w_1$），才能到达最小值。

![](/images/speech-and-language-processing/slp-fig-5-4.png)

**图 5.4** 通过沿函数斜率的反方向移动 $\mathbf{w}$，迭代地找到该损失函数最小值的第一步。由于斜率为负，需要将 $\mathbf{w}$ 向正方向（右侧）移动。此处上标用于表示学习步骤，因此 $w_1$ 表示 $\mathbf{w}$ 的初始值（即0），$w_2$ 表示第二步的值，依此类推。

梯度下降算法通过计算当前点处损失函数的**梯度**（gradient），并朝其相反方向移动来解决这个问题。一个多变量函数的梯度是一个向量，指向函数值增长最快的方向。梯度是斜率在多变量情况下的推广，因此对于像图5.4中这样的单变量函数，我们可以非正式地将梯度视为斜率。图5.4中的虚线显示了在点 $\mathbf{w} = w_1$ 处这个假设损失函数的斜率。可以看到，这条虚线的斜率为负。因此，为了找到最小值，梯度下降告诉我们应朝相反方向移动：即把 $\mathbf{w}$ 向正方向移动。

在梯度下降中，移动的幅度大小由斜率 $\frac{d}{dw} L(f(x;w),y)$ 的值乘以一个**学习率**（learning rate）$\eta$ 决定。较高的（更快的）学习率意味着我们在每一步中对 $\mathbf{w}$ 的调整更大。对参数的调整量等于学习率乘以梯度（在我们的单变量例子中即斜率）：

$$
w^{t+1} = w^{t} - \eta \frac{d}{dw} L( f(x;w), y)
\tag{5.26}
$$

现在，我们将这种直觉从单个标量变量 $\mathbf{w}$ 扩展到多个变量的情况。因为我们不仅仅需要决定向左或向右移动，而是需要知道在由 $N$ 个参数构成的 $N$ 维空间（即参数向量 $\theta$ 的空间）中，应该朝哪个方向移动。**梯度**正是这样一个向量；它表达了在每个维度上最陡峭坡度的方向分量。如果只考虑两个权重维度（例如一个权重 $w$ 和一个偏置 $b$），梯度可能是一个包含两个正交分量的向量，每个分量分别告诉我们地面在 $\mathbf{w}$ 维度和 $b$ 维度上的坡度。图5.5展示了在红点处取到的一个二维梯度向量的可视化。

在实际的逻辑回归中，参数向量 $\mathbf{w}$ 远不止1或2维，因为输入特征向量 $\mathbf{x}$ 可能很长，我们需要为每个 $x_i$ 设置一个权重 $w_i$。对于 $\mathbf{w}$（以及偏置 $b$）中的每个维度/变量 $w_i$，梯度都会有一个分量，告诉我们相对于该变量的斜率。在每个维度 $w_i$ 上，用损失函数对 $w_i$ 的偏导数 $\frac{\partial}{\partial w_i}$ 来表示斜率。本质上我们是在问：“该变量 $w_i$ 的微小变化会对总损失函数 $L$ 产生多大影响？”

因此，形式上，一个多变量函数 $f$ 的梯度是一个向量，其中每个分量表示 $f$ 对其中一个变量的偏导数。我们将使用倒置的希腊字母 delta 符号 $∇$ 来表示梯度，并将 $\hat{y}$ 表示为 $f(x;\theta)$ 以更清楚地体现其对 $\theta$ 的依赖：

$$
\nabla L( f(x;\theta), y) =
\begin{bmatrix}
\frac{\partial}{\partial w_1}L(f(x;\theta), y) \\
\frac{\partial}{\partial w_2}L(f(x;\theta), y) \\
\vdots \\
\frac{\partial}{\partial w_n}L(f(x;\theta), y) \\
\frac{\partial}{\partial b}L(f(x;\theta), y)
\end{bmatrix}
\tag{5.27}
$$

因此，基于梯度更新 $\theta$ 的最终方程为：

$$
\theta^{t+1} = \theta^t - \eta \nabla L(f(x;\theta), y)
\tag{5.28}
$$

![](/images/speech-and-language-processing/slp-fig-5-5.png)

**图 5.5** 二维空间中（$\mathbf{w}$ 和 $b$）红点处梯度向量的可视化，图中红色箭头指向了我们寻找最小值的方向：即梯度的反方向（注意：梯度指向函数值增加的方向，而非减少的方向）。


<nav class="pagination justify-content-between">
<a href="../ch5-05">5.5 交叉熵损失函数</a>
<a href="../">目录</a>
<span></span>
</nav>

