---
title: "5.7 正则化"
summary: ""
date: 2025-10-09T10:19:00+08:00
---

> Numquam ponenda est pluralitas sine necessitate  
> “如无必要，勿增实体”  
> ——奥卡姆的威廉

学习那些使模型完美匹配训练数据的权重存在一个问题。如果某个特征恰好只出现在一个类别中，并因此能完美预测结果，那么它将被赋予一个非常高的权重。特征的权重会试图完美拟合训练集的细节，实际上这种拟合过于完美，以至于模型会学习到那些只是偶然与类别相关的噪声因素。这个问题被称为**过拟合**（overfitting）。一个好的模型应该能够从训练数据**泛化**（generalize）到未见过的测试集上，而一个过拟合的模型泛化能力会很差。

为了避免过拟合，我们在公式 (5.25) 的损失函数中添加一个新的**正则化项**（regularization term）$R(\theta)$，从而得到针对 $m$ 个样本批次的以下损失函数（此处稍作改写，从最小化损失变为最大化对数概率，并移除了不影响 argmax 结果的 $\frac{1}{m}$ 项）：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \sum_{i=1}^m \log P(y^{(i)}|x^{(i)}) - \alpha R(\theta)
\tag{5.36}
$$

新的正则化项 $R(\theta)$ 用于惩罚过大的权重。因此，一个虽然完美匹配训练数据但使用了许多高权重值的参数设置，其受到的惩罚会大于另一个虽然对数据的拟合稍差、但使用了更小权重的参数设置。计算正则化项 $R(\theta)$ 有两种常用方法。

**L2 正则化**（L2 regularization）是权重值的二次函数，之所以这样命名，是因为它使用了权重值的 L2 范数（的平方）。L2 范数 $||\theta||^2$ 等同于向量 $\theta$ 到原点的欧几里得距离。如果 $\theta$ 包含 $n$ 个权重，则：

$$
R(\theta) = ||\theta||_2^2 = \sum_{j=1}^n \theta_j^2
\tag{5.37}
$$

L2 正则化的损失函数变为：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \left[ \sum_{i=1}^m \log P(y^{(i)}|x^{(i)}) \right] - \alpha \sum_{j=1}^n \theta_j^2
\tag{5.38}
$$

**L1 正则化**（L1 regularization）是权重值的线性函数，得名于 L1 范数 $||\theta||_1$，即权重绝对值之和，也称为**曼哈顿距离**（Manhattan distance）（曼哈顿距离是指在像纽约这样具有网格状街道的城市中，两点之间需要步行的实际距离）：

$$
R(\theta) = ||\theta||_1 = \sum_{j=1}^n |\theta_j|
\tag{5.39}
$$

L1 正则化的损失函数变为：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \left[ \sum_{i=1}^m \log P(y^{(i)}|x^{(i)}) \right] - \alpha \sum_{j=1}^n |\theta_j|
\tag{5.40}
$$

这类正则化方法源于统计学，其中 L1 正则化被称为**套索回归**（lasso regression）（Tibshirani, 1996），而 L2 正则化被称为**岭回归**（ridge regression），两者在语言处理中都很常用。由于 L2 正则化具有简单的导数（$\theta^2$ 的导数就是 $2\theta$），因此更容易优化；而 L1 正则化则更复杂（$|\theta|$ 的导数在零点不连续）。然而，L2 正则化倾向于产生包含许多小权重的权重向量，而 L1 正则化则倾向于产生稀疏解，即少数较大的权重和大量为零的权重。因此，L1 正则化会产生更稀疏的权重向量，也就是有效特征的数量更少。

L1 和 L2 正则化都可以从贝叶斯角度解释为对权重分布先验的约束。L1 正则化可以看作是对权重施加了拉普拉斯先验（Laplace prior）。L2 正则化则对应于假设权重服从均值为 $µ = 0$ 的高斯分布（Gaussian distribution）。在高斯（或正态）分布中，一个值离均值越远，其概率越低（由方差 $\sigma$ 缩放）。通过对权重使用高斯先验，我们实际上是在表达权重更倾向于为 0 的偏好。权重 $\theta_j$ 的高斯分布形式为：

$$
\frac{1}{\sqrt{2\pi\sigma^2_j}} \exp\left(-\frac{(\theta_j - \mu_j)^2}{2\sigma^2_j}\right)
\tag{5.41}
$$

如果我们为每个权重乘上一个关于该权重的高斯先验，那么我们实际上是在最大化以下约束：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \prod_{i=1}^m P(y^{(i)}|x^{(i)}) \times \prod_{j=1}^n \frac{1}{\sqrt{2\pi\sigma_j^2}} \exp \left(-\frac{(\theta_j - \mu_j)^2}{2\sigma_j^2}\right)
\tag{5.42}
$$

在对数空间中，假设均值 $µ = 0$ 且 $2\sigma^2 = 1$，上式等价于：

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \sum_{i=1}^m \log P(y^{(i)}|x^{(i)}) - \alpha \sum_{j=1}^n \theta_j^2
\tag{5.43}
$$

这与公式 (5.38) 的形式相同。


<nav class="pagination justify-content-between">
<a href="../ch5-06">5.6 梯度下降</a>
<a href="../">目录</a>
<a href="../ch5-08">5.8 多项逻辑回归中的学习</a>
</nav>

