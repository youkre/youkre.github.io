---
title: "5.11 小结"
summary: ""
date: 2025-10-10T09:08:00.000Z
---

本章介绍了**逻辑回归**模型在**分类**中的应用。

- 逻辑回归是一种监督式机器学习分类器，它从输入中提取实值特征，每个特征乘以一个权重后求和，并将结果通过**sigmoid**函数传递以生成概率。使用一个阈值来做出决策。
- 逻辑回归可以用于两类（例如，正面和负面情感）或多类（**多项逻辑回归**，例如用于n元文本分类、词性标注等）的情况。
- 多项逻辑回归使用**softmax**函数来计算概率。
- 权重（向量 $\mathbf{w}$ 和偏置 $b$）通过损失函数（如**交叉熵损失**）从标记的训练集中学习，该损失函数需要被最小化。
- 最小化这个损失函数是一个**凸优化**问题，迭代算法如梯度下降被用来找到最优权重。
- **正则化**被用来避免过拟合。
- 由于其能够透明地研究单个特征的重要性，逻辑回归也是最有用的分析工具之一。

## 文献与历史注记

逻辑回归是在统计学领域发展起来的，在20世纪60年代已被用于二进制数据的分析，尤其在医学中广泛应用（Cox, 1969）。从20世纪70年代末开始，它成为语言学中研究语言变异的形式基础之一（Sankoff and Labov, 1979）。

然而，直到20世纪90年代，逻辑回归才在自然语言处理中变得普遍，当时它似乎同时从两个方向出现。第一个来源是信息检索和语音处理这两个相邻领域，它们都曾使用回归方法，并且都为NLP贡献了许多其他统计技术。实际上，早期将逻辑回归用于文档路由的例子之一是最早使用（LSI）嵌入作为词表示的NLP应用之一（Schütze et al., 1995）。

与此同时，在20世纪90年代初，IBM Research在名为**最大熵**建模或**maxent**（Berger et al., 1996）的名称下开发并应用于NLP，这似乎是独立于统计文献的发展。在这个名称下，它被应用于语言模型（Rosenfeld, 1996）、词性标注（Ratnaparkhi, 1996）、解析（Ratnaparkhi, 1997）、共指消解（Kehler, 1997b）和文本分类（Nigam et al., 1999）。

关于分类的更多信息可以在机器学习教材中找到（Hastie et al., 2001; Witten and Frank, 2005; Bishop, 2006; Murphy, 2012）。


<nav class="pagination justify-content-between">
<a href="../ch5-10">5.10 进阶：梯度公式的推导</a>
<a href="../">目录</a>
<a href="../ch6">CHAPTER 6 Vector Semantics and Embeddings</a>
</nav>

