---
title: "5.3 多项逻辑回归"
summary: ""
date: 2025-10-05T09:00:00+08:00
---

有时我们需要处理超过两个类别的分类问题。例如，可能需要进行三类情感分类（正面、负面或中性）。或者，可能会分配第17章将要介绍的一些标签，比如一个词的词性（从10个、30个甚至50个不同的词性中选择），或一个短语的命名实体类型（从“人名”、“地点”、“组织”等标签中选择）。

在这种情况下，使用**多项逻辑回归**（multinomial logistic regression），也被称为**Softmax回归**（在早期的NLP文献中，有时会看到它被称为**最大熵分类器**，maxent classifier）。在多项逻辑回归中，我们希望将每个观测样本标记为 $K$ 个类别中的某一个 $k$，并规定只有一个类别是正确的（有时称为**硬分类**；一个观测样本不能同时属于多个类别）。我们采用以下表示方式：每个输入 $\mathbf{x}$ 的输出 $\mathbf{y}$ 将是一个长度为 $K$ 的向量。如果类别 $c$ 是正确的类别，我们将 $y_c$ 设为1，并将 $\mathbf{y}$ 的所有其他元素设为0，即 $y_c = 1$ 且 $y_j = 0 \quad \forall j \neq c$。像这样的向量 $\mathbf{y}$（只有一个值为1，其余为0）被称为**独热向量**（one-hot vector）。分类器的任务是生成一个估计向量 $\hat{\mathbf{y}}$。对于每个类别 $k$，值 $\hat{y}_k$ 将是分类器对概率 $p(y_k = 1|\mathbf{x})$ 的估计。

### 5.3.1 Softmax函数

多项逻辑分类器使用Sigmoid函数的一个推广版本，称为**Softmax函数**，来计算 $p(y_k = 1|\mathbf{x})$。Softmax函数接收一个包含 $K$ 个任意值的向量 $\mathbf{z} = [z_1, z_2, ..., z_K]$，并将其映射为一个概率分布，其中每个值都在 $[0,1]$ 范围内，且所有值的总和为1。与Sigmoid函数一样，它也是一种指数函数。

对于一个维度为 $K$ 的向量 $\mathbf{z}$，Softmax函数定义如下：

$$
\mathrm{softmax}(z_i) = \frac{\exp(z_i)}{\sum_{j=1}^K \exp(z_j)} \quad 1 \leq i \leq K
$$

(5.16)
{class="text-end"}

因此，输入向量 $\mathbf{z} = [z_1, z_2, ..., z_K]$ 的Softmax结果本身也是一个向量：

$$
\mathrm{softmax}(\mathbf{z}) = \left[\frac{\exp(z_1)}{\sum_{i=1}^K \exp(z_i)}, \frac{\exp(z_2)}{\sum_{i=1}^K \exp(z_i)}, \cdots, \frac{\exp(z_K)}{\sum_{i=1}^K \exp(z_i)}\right]
$$

(5.17)
{class="text-end"}

分母 $\sum_{i=1}^K \exp(z_i)$ 用于将所有值归一化为概率。例如，给定向量：

$$
\mathbf{z} = [0.6, 1.1, -1.5, 1.2, 3.2, -1.1]
$$

其对应的（四舍五入后的）Softmax($\mathbf{z}$) 结果为：

$$
[0.05, 0.09, 0.01, 0.10, 0.74, 0.01]
$$

与Sigmoid函数类似，Softmax函数也具有将极端值压缩至0或1的特性。因此，如果其中一个输入值远大于其他输入值，Softmax会倾向于将其概率推高至接近1，同时抑制较小输入值的概率。

最后，请注意，与Sigmoid函数的情况类似，我们将作为Softmax函数输入的得分向量 $\mathbf{z}$ 称为 **logits**（参见公式 (5.7)）。

### 5.3.2 在逻辑回归中应用 Softmax

当我们将 Softmax 应用于逻辑回归时，其输入（与 Sigmoid 函数类似）将是权重向量 $\mathbf{w}$ 与输入向量 $\mathbf{x}$ 的点积（再加上一个偏置项）。但现在，我们需要为 $K$ 个类别中的每一个都设置独立的权重向量 $\mathbf{w}_k$ 和偏置项 $b_k$。因此，每个输出类别 $\hat{y}_k$ 的概率可以计算如下：

$$
p(y_k = 1|\mathbf{x}) = \frac{\exp(\mathbf{w}_k \cdot \mathbf{x} + b_k)}{\sum_{j=1}^K \exp(\mathbf{w}_j \cdot \mathbf{x} + b_j)}
$$

(5.18)
{class="text-end"}

公式 (5.18) 的形式似乎暗示我们需要分别计算每个输出。然而，为了更高效地利用现代向量化硬件进行计算，我们通常会以另一种方式构建方程。为此，我们将 $K$ 个权重向量表示为一个权重矩阵 $\mathbf{W}$，并将偏置项表示为一个偏置向量 $\mathbf{b}$。$\mathbf{W}$ 的每一行 $k$ 对应于权重向量 $\mathbf{w}_k$。因此，$\mathbf{W}$ 的形状为 $[K \times f]$，其中 $K$ 是输出类别的数量，$f$ 是输入特征的数量。偏置向量 $\mathbf{b}$ 则包含 $K$ 个输出类别各自的偏置值。以这种方式表示权重后，我们可以用一个简洁的公式一次性计算出 $\hat{\mathbf{y}}$，即所有 $K$ 个类别的输出概率向量：

$$
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{Wx} + \mathbf{b})
$$

(5.19)
{class="text-end"}

如果你仔细推导一下矩阵运算过程，就会发现第一个输出类别 $\hat{y}_1$ 的估计得分（在应用 Softmax 之前）确实等于 $\mathbf{w}_1 \cdot \mathbf{x} + b_1$。

图 5.3 直观地展示了在二元逻辑回归和多项逻辑回归中，权重向量与权重矩阵在计算输出类别概率时所起的不同作用。

### 5.3.3 多项逻辑回归中的特征

多项逻辑回归中的特征与二元逻辑回归中的特征作用类似，唯一的区别是如上所述，我们需要为 $K$ 个类别中的每一个都设置独立的权重向量和偏置项。回顾第81页提到的二元分类中的感叹号特征 $x_5$：

$$
x_5 =
\begin{cases}
1 & \text{if “!” } \in \text{doc} \\
0 & \text{otherwise}
\end{cases}
$$

在二元分类中，某个特征上的正权重 $w_5$ 会促使分类器倾向于将样本判为 $y = 1$（正面情感），而负权重则会使其倾向于 $y = 0$（负面情感），其绝对值大小表示该特征的重要性。而在多项逻辑回归中，由于每个类别都有独立的权重，因此一个特征可能对某个类别是支持证据，而对另一个类别则是反对证据。

例如，在三类情感分类任务中，我们需要将每篇文档分配到三个类别之一：+（正面）、−（负面）或 0（中性）。此时，与感叹号相关的特征可能对“中性”类别具有负权重，而对“正面”或“负面”类别具有正权重：

| 特征定义 | $w_{5,+}$ | $w_{5,-}$ | $w_{5,0}$ |
| :--- | :---: | :---: | :---: | :---: |
| $f_5(x)$ | $\begin{cases} 1 & \text{if “!” } \in \text{doc} \\ 0 & \text{otherwise} \end{cases}$ | 3.5 | 3.1 | -5.3 |

由于这些特征权重同时依赖于输入文本和输出类别，我们有时会明确表示这种依赖关系，将特征本身写成 $f(x, y)$：即输入和类别的函数。使用这种记法，上面的 $f_5(x)$ 可以表示为三个特征：$f_5(x,+)$、$f_5(x,−)$ 和 $f_5(x,0)$，每个特征对应一个单一的权重。我们将在第17章描述条件随机场（CRF）时使用这种记法。

![](/images/speech-and-language-processing/slp-fig-5-3.png)

**图 5.3** 二元逻辑回归与多项逻辑回归的对比。二元逻辑回归使用单个权重向量 $\mathbf{w}$，输出为标量 $\hat{y}$。在多项逻辑回归中，有 $K$ 个分别对应 $K$ 个类别的独立权重向量，它们被整合进一个权重矩阵 $\mathbf{W}$ 中，输出为向量 $\hat{\mathbf{y}}$。为清晰起见，图中省略了偏置项。


<nav class="pagination justify-content-between">
<a href="../ch5-02">5.2 使用逻辑回归进行分类</a>
<a href="../">目录</a>
<a href="../ch5-04">5.4 逻辑回归中的学习</a>
</nav>

