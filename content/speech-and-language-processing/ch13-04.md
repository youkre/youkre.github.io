---
title: "13.4 堆叠式与双向 RNN 架构"
summary: ""
date: 2025-12-31T18:51:00+08:00
---

循环神经网络具有很强的灵活性。
通过将展开后的计算图的前馈特性与向量作为通用输入/输出相结合，复杂的网络可以被视作模块，并以富有创意的方式进行组合。
本节将介绍在自然语言处理中使用 RNN 时两种更为常见的网络架构。

### 13.4.1 堆叠式 RNN（Stacked RNNs）

到目前为止，我们所举的例子中，RNN 的输入都是词或字符嵌入（即向量）组成的序列，输出则是可用于预测词、标签或序列类别标签的向量。
然而，我们完全可以将一个 RNN 输出的整个序列，作为另一个 RNN 的输入序列。
**堆叠式 RNN**（Stacked RNNs）正是由多个 RNN 层组成，其中某一层的输出作为下一层的输入，如图 13.10 所示。

![](/images/speech-and-language-processing/slp-fig-13-10.png)

**图 13.10** 堆叠式循环网络。较低层的输出作为更高层的输入，最后一层网络的输出即为最终输出。

堆叠式 RNN 通常优于单层网络。
其成功的一个原因似乎是：网络在不同层上学习到了不同抽象层次的表示。
正如人类视觉系统早期阶段检测边缘，再利用这些边缘识别更大的区域和形状一样，堆叠网络的底层可以学习到一些基础表示，这些表示可作为更高层的有效抽象——而这些抽象可能很难在单一 RNN 中直接学到。
堆叠 RNN 的最佳层数因具体任务和训练数据而异。
然而，随着堆叠层数的增加，训练成本会迅速上升。

### 13.4.2 双向 RNN（Bidirectional RNNs）

标准的 RNN 在时刻 $t$ 进行预测时，仅利用左侧（即之前）的上下文信息。
但在许多应用场景中，我们可以访问整个输入序列；此时，我们希望也能利用当前时刻 $t$ 右侧（即后续）的上下文信息。
一种实现方式是运行两个独立的 RNN：一个从左到右处理序列，另一个从右到左处理序列，然后将它们的表示拼接起来。

在前文讨论的从左到右的 RNN 中，时刻 $t$ 的隐藏状态代表了网络到该时刻为止对序列所掌握的全部信息。
该状态是输入 $\mathbf{x}_1, \dots, \mathbf{x}_t$ 的函数，表示当前时刻左侧的上下文：

$$
\mathbf{h}^f_t = \text{RNN}_{\text{forward}}(\mathbf{x}_1, \dots, \mathbf{x}_t)
\tag{13.16}
$$

这里的符号 $\mathbf{h}^f_t$ 仅表示时刻 $t$ 常规的前向隐藏状态，即网络从序列开头到当前时刻所“学到”的全部信息。

为了利用当前输入右侧的上下文，我们可以在一个**反转**的输入序列上训练另一个 RNN。
在这种设置下，时刻 $t$ 的隐藏状态就包含了当前输入**右侧**的信息：

$$
\mathbf{h}^b_t = \text{RNN}_{\text{backward}}(\mathbf{x}_t, \dots, \mathbf{x}_n)
\tag{13.17}
$$

这里，$\mathbf{h}^b_t$ 表示从时刻 $t$ 到序列末尾所提取到的全部信息。

**双向 RNN**（Bidirectional RNN，Schuster 和 Paliwal，1997）将两个独立的 RNN 结合在一起：一个按正常顺序（从开始到结束）处理输入，另一个按逆序（从结束到开始）处理输入。
然后，我们将这两个网络在每个时间步计算出的表示拼接成一个向量，从而同时捕获该时刻输入的左侧和右侧上下文。
我们使用分号 “;” 或等价符号 $\oplus$ 表示向量拼接：

$$
\begin{aligned}
\mathbf{h}_t &= [\mathbf{h}^f_t; \mathbf{h}^b_t] \\
&= \mathbf{h}^f_t \oplus \mathbf{h}^b_t
\end{aligned}
\tag{13.18}
$$

图 13.11 展示了这样一个双向网络，它将前向和后向传播的输出进行拼接。
除了拼接之外，也可以通过逐元素相加或相乘等简单方式融合前向与后向上下文。
这样，每个时间步的输出就能同时包含当前输入左侧和右侧的信息。
在序列标注任务中，这些拼接后的输出可作为局部标签决策的基础。

![](/images/speech-and-language-processing/slp-fig-13-11.png)

**图 13.11** 双向 RNN。
分别训练前向和后向两个模型，并在每个时间点将其输出拼接，以表示该时刻的双向状态。

双向 RNN 在序列分类任务中也被证明非常有效。
回顾图 13.8，在序列分类中，我们通常将 RNN 的最终隐藏状态作为后续前馈分类器的输入。
但这种方法存在一个问题：最终隐藏状态自然更偏向于反映句子末尾的信息，而对开头的信息保留较少。
双向 RNN 为这一问题提供了一个简洁的解决方案：如图 13.12 所示，我们只需将前向 RNN 的最终隐藏状态与后向 RNN 的最终隐藏状态（例如通过拼接）结合起来，并将该组合表示作为后续处理的输入。

![](/images/speech-and-language-processing/slp-fig-13-12.png)

**图 13.12** 用于序列分类的双向 RNN。
前向传播和后向传播得到的最终隐藏单元合并起来表示整个序列。
合并的表示构成后续分类起额输入。


<nav class="pagination justify-content-between">
<a href="../ch13-03">13.3 RNN 在其他 NLP 任务中的应用</a>
<a href="../">目录</a>
<a href="../ch13-05">13.5 长短期记忆网络（LSTM）</a>
</nav>

