---
title: "9.1 注意力机制"
summary: ""
date: 2025-11-01T21:05:00+08:00
---

回顾第6章的内容，在 **word2vec** 和其他静态词嵌入方法中，一个词的语义表示始终是同一个向量，与上下文无关：例如，单词 *chicken* 总是由同一个固定的向量表示。
因此，代词 *it* 的静态向量可能只能编码“这是一个用于动物或无生命事物的代词”这一信息。
但在实际语境中，它的含义要丰富得多。请考虑以下两个句子中的 *it*：

(9.1) **The chicken** didn’t cross the road because **it** was too tired.

(9.2) **The chicken** didn’t cross the road because **it** was too wide.

在句子 (9.1) 中，`it` 指的是 *chicken*（即读者知道是鸡太累了），而在句子 (9.2) 中，`it` 指的是 *road*（即读者知道路太宽了）[^2]。
也就是说，如果我们想要理解整个句子的含义，就必须让 `it` 在第一个句子中与 *chicken* 关联，在第二个句子中与 *road* 关联，这种关联是依赖于上下文的。

[^2] 我们称在第一个例子中 `it` 与 *chicken* 共指（corefer），在第二个例子中与 *road* 共指；我们将在第23章再次讨论这一现象。

此外，设想我们像一个因果语言模型那样从左到右阅读，处理到单词 `it` 为止：

(9.3) **The chicken** didn’t cross the road because **it**

此时，我们尚不清楚 `it` 最终会指向哪一个实体！因此，在这个时刻，对 `it` 的表示可能同时包含 *chicken* 和 *road* 的某些特征，因为模型正在尝试预测接下来会发生什么。

这种词语之间存在丰富语言关系、且这些相关词可能相距甚远的现象，在语言中极为普遍。再看两个例子：

(9.4) The **keys** to the cabinet **are** on the table.

(9.5) I walked along the **pond**, and noticed one of the trees along the **bank**.

在 (9.4) 中，短语 *The keys* 是句子的主语，在英语（以及许多语言）中，它必须与动词 *are* 在语法数上保持一致；本例中两者均为复数。
在英语中，我们不能对像 *keys* 这样的复数主语使用单数动词 *is*（我们将在第18章更详细地讨论一致性问题）。
在 (9.5) 中，我们知道 *bank* 指的是池塘或河流的岸边，而不是金融机构，这得益于上下文，尤其是像 *pond* 这样的词。（我们将在第11章更深入地讨论词义问题。）

所有这些例子的核心在于：那些帮助我们理解词语在上下文中含义的“上下文词”，可能在句子或段落中距离很远。
Transformer 能够通过整合这些关键上下文词的语义信息，构建出词语的上下文相关表示，即**上下文嵌入**（contextual embeddings）。
在 Transformer 中，我们逐层地构建输入词元语义的、越来越丰富的上下文化表示。
在每一层，我们都将上一层中关于词元 $i$ 的信息与其邻近词元的信息结合起来，为每个位置上的每个词生成一个上下文化后的表示。

**注意力**（Attention）正是 Transformer 中的机制，它通过为上下文中的其他词元（来自第 $k−1$ 层的表示）分配权重并进行组合，从而构建第 $k$ 层词元的表示。

**图9.2** 展示了在第 $k+1$ 层计算代词 *it* 的表示时所涉及的自注意力权重分布 $\alpha$。在计算 *it* 的表示时，模型对第 $l$ 层的各个词赋予不同的注意力，颜色越深表示自注意力值越高。请注意，模型对对应 *chicken* 和 *road* 两个词元的列赋予了很高的注意力，这是一个合理的结果，因为在出现 *it* 的位置，它都有可能与 *chicken* 或 *road* 共指，因此我们希望 *it* 的表示能够借鉴这两个先前词元的表示。该图改编自 Uszkoreit (2017)。

图9.2 展示了一个来自 Transformer 的简化示意图（Uszkoreit, 2017）。
该图描述了当前词元为 *it* 时，在 Transformer 的第 $k+1$ 层为其计算上下文表示的情景，此时需要参考此前所有词元在第 $k$ 层的表示。
图中用颜色深浅来表示对各个上下文词的注意力分布：*chicken* 和 *road* 两个词元都具有较高的注意力权重，这意味着在计算 *it* 的表示时，我们将主要借鉴 *chicken* 和 *road* 的表示。
这有助于最终构建出 *it* 的表示，因为 *it* 最终将与 *chicken* 或 *road* 之一形成共指关系。

接下来，我们将讨论这种注意力分布是如何表示和计算的。

### 9.1.1 注意力机制的正式描述

如前所述，注意力计算是一种方法，用于通过有选择地关注并整合前一层中先前词元的信息，来计算 Transformer 某一层上特定词元的向量表示。
注意力机制接收一个输入词元的在位置 $i$ 所对应的表示 $mathbf{x}_i$，以及一个包含此前输入$\mathbf{x}_1..x_{i-1}$的上下文窗口，生成一个输出$\mathbf{a}_i$。

在因果式、从左到右的语言模型中，上下文指的是所有位于当前词之前的词。
也就是说，在处理 $\mathbf{x}_i$ 时，模型可以访问 $\mathbf{x}_i$ 本身以及上下文窗口内所有先前词元的表示（上下文窗口通常包含数千个词元），但无法访问位置 $i$ 之后的任何词元。（相比之下，在第11章中，我们将推广注意力机制，使其也能“向前看”未来的词。）

图9.3 展示了整个因果式自注意力层中的信息流动过程，其中相同的注意力计算在每个词元位置 $i$ 并行进行。
因此，一个自注意力层将输入序列 ($x_1,...,x_n$) 映射为等长的输出序列 ($a_1,...,a_n$)。

**图9.3** 因果式自注意力中的信息流。在处理每个输入 $\mathbf{x}_i$ 时，模型会关注到从序列起始直到并包括 $\mathbf{x}_i$ 的所有输入。

**注意力的简化版本** 注意力机制的核心本质上只是一个对上下文向量的加权求和，其复杂性主要体现在权重的计算方式以及求和的对象上。
为了便于理解，我们首先介绍一个简化的注意力原理：位置 $i$ 的注意力输出 $\mathbf{a}_i$，就是所有 $x_j$ （$j \leq i$）表示的加权和 的；我们用 $\alpha_{ij}$ 表示 $x_j$ 对 $a_i$ 的贡献程度：

$$
\text{简化版本: } \mathbf{a}_i = \sum_{j\leq i} \alpha_{ij} \times \mathbf{x}_j
\tag{9.6}
$$

每个 $\alpha_{ij}$ 是一个标量，用于在对输入求和计算 $a_i$ 时，对输入 $x_j$ 的值进行加权。
那么，我们该如何计算这个 $\alpha$ 权重呢？
在注意力机制中，我们会根据每个先前的嵌入向量与当前词元 $i$ 的**相似度**来为其分配权重。
因此，注意力的输出是先前词元嵌入向量的加权和，权重由它们与当前词元嵌入向量的相似度决定。
我们通过**点积**（dot product）来计算相似度得分，它将两个向量映射为一个从 $-\infty$ 到 $\infty$ 的标量值。
得分越高，表明被比较的两个向量越相似。我们将使用 softmax 函数对这些得分进行归一化，从而得到权重向量 $\alpha_{ij}, j \leq i$。

$$
\text{简化版本: } \text{score}(x_i, x_j) = x_i \cdot x_j \tag{9.7}
$$

$$
\alpha_{ij} &= \text{softmax}(\text{score}(x_i, x_j)) \quad \forall j \leq i \tag{9.8}
$$

因此，在图9.3中，为了计算 $a_3$，需要计算三个得分：$x_3 \cdot x_1$、$x_3 \cdot = x_2$ 和 $\x_3 \cdot x_3$，然后使用 softmax 进行归一化，将得到的概率作为权重，表示它们各自对当前位置 $i$ 的相对相关性。
当然，由于 $x_i$ 与自身非常相似，会产生很高的点积，因此 softmax 权重很可能会在 $\mathbf{x}_i$ 上达到最高。
但其他上下文词也可能与 $i$ 相似，softmax 也会给这些词分配一定的权重。然后，我们使用这些权重作为公式 (9.6) 中的 $\alpha$ 值，计算出加权和，即为我们的 $a_3$。

公式 (9.6) 至 (9.8) 中的简化版注意力展示了基于注意力的 $\a_i$ 计算方法：将 $\x_i$ 与先前的向量进行比较，将这些得分归一化为一个概率分布，并用该分布来对先前向量的求和进行加权。
不过现在我们可以去掉这些简化假设了。

**使用查询（Query）、键（Key）和值（Value）矩阵的单个注意力头** 在了解了注意力的基本原理后，我们引入真正的**注意力头**（attention head），这是 Transformer 中实际使用的注意力版本。（“头”（head）这个词在 Transformer 中常用来指代特定的结构化层。）
注意力头允许我们清晰地区分每个输入嵌入在注意力过程中所扮演的三个不同角色：

* 作为*当前元素*与前面的输入进行比较。我们将此角色称为**查询**（query）。
* 作为*先前的输入*，它被拿来与当前元素进行比较，以确定一个相似度权重。我们将此角色称为**键**（key）。
* 最后，作为先前元素的**值**（value），它会被赋予权重并累加起来，以计算当前元素的输出。

为了区分这三个不同角色，Transformer 引入了权重矩阵 $\mathbf{W}^Q$、$\mathbf{W}^K$ 和 $\mathbf{W}^V$。
这些权重矩阵会将每个输入向量 $x_i$ 分别投影到其作为“键”、“查询”或“值”的表示空间：

$$
\begin{aligned}
q_i = x_i \mathbf{W}^Q; \\
k_i = x_i \mathbf{W}^K; \\
v_i = x_i \mathbf{W}^V
\end{aligned}
\tag{9.9}
$$

有了这些投影之后，在计算当前元素 $x_i$ 与某个先前元素 $x_j$ 的相似度时，我们将使用当前元素的**查询**向量 $q_i$ 与先前元素的**键**向量 $k_j$ 之间的点积。
此外，点积的结果可能是一个任意大的（正或负）数值，而对过大的值进行指数运算（如在 softmax 中）可能导致数值不稳定，并在训练过程中造成梯度消失或爆炸。
为了避免这种情况，我们会通过一个与嵌入向量大小相关的因子对点积进行缩放，即除以查询和键向量维度 $d_k$ 的平方根。
因此，我们将简化的公式 (9.7) 替换为公式 (9.11)。
随后的 softmax 计算得到 $\alpha_{ij}$ 的过程保持不变，但 $\a_i$ 的输出计算现在是基于对**值向量** $v$ 的加权求和（见公式 9.13）。

以下是计算单个自注意力输出向量 $\a_i$（由单个输入向量 $x_i$ 得出）的完整公式集。该版本的注意力机制通过将先前元素的“值”进行加权求和来计算 $a_i$，每个“值”的权重由其“键”与当前元素的“查询”之间的相似度决定：

$$
\begin{aligned}
q_i = x_i\mathbf{W}^Q; \\
k_j = x_j\mathbf{W}^K; \\
v_j = x_j\mathbf{W}^V
\end{aligned}
\tag{9.10}
$$

$$
\text{score}(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}
\tag{9.11}
$$

$$
\alpha_{ij} = \text{softmax}(\text{score}(x_i, x_j)) \quad \forall j \leq i
\tag{9.12}
$$

$$
a_i &= \sum_{j\leq i} \alpha_{ij} v_j
\tag{9.13}
$$

图9.4 展示了在一个序列中计算第三个输出 $a_3$ 值的过程。

**图9.4** 使用因果式（从左到右）自注意力计算序列中第三个元素 $a_3$ 的值。

我们来讨论一下维度（shapes）。
注意力的输入 $x_i$ 和输出 $a_i$ 具有相同的维度 $1 \times d$（我们通常称 $d$ 为**模型维度**，正如将在9.2节中讨论的那样，每个 Transformer 模块的输出 $h_i$，以及模块内部的中间向量，也都具有相同的 $1 \times d$ 维度）。

我们为“键”和“查询”向量设定一个维度 $d_k$。“查询”向量和“键”向量的维度均为 $1 \times d_k$，因此可以计算它们的点积 $q_i \cdot k_j$。
我们为“值”向量设定一个独立的维度 $d_v$。
变换矩阵 $\mathbf{W}^Q$ 的形状为 $[d \times d_k]$，$\mathbf{W}^K$ 为 $[d \times d_k]$，$\mathbf{W}^V$ 为 $[d \times d_v]$。在最初的 Transformer 研究中（Vaswani 等人，2017），$d$ 为 512，$d_k$ 和 $d_v$ 均为 64。

**多头注意力**（Multi-head Attention） 公式 (9.11)-(9.13) 描述的是一个**注意力头**。
但实际上，Transformer 使用的是多个注意力头。
其理由是，每个头可能出于不同的目的关注上下文：不同的头可能专门用于表示上下文元素与当前词元之间的不同语言学关系，或者用于在上下文中寻找特定类型的模式。

因此，在**多头注意力**中，有 $h$ 个独立的注意力头并行地存在于模型的同一深度层中，每个头都有自己的一组参数，使其能够针对输入之间关系的不同方面建模。
因此，自注意力层中的每个头 $i$ 都拥有自己独立的“键”、“查询”和“值”矩阵：$W_{Ki}$、$W_{Qi}$ 和 $W_{Vi}$。
这些矩阵用于将输入投影到每个头各自的“键”、“值”和“查询”嵌入空间。

当使用多个头时，模型维度 $d$ 仍用于输入和输出，“键”和“查询”嵌入的维度为 $d_k$，“值”嵌入的维度为 $d_v$（在最初的 Transformer 论文中，$d_k = d_v = 64$，头数 $h = 8$，模型维度 $d = 512$）。
因此，对于每个头 $i$，我们有形状为 $[d \times d_k]$ 的 $W_{Qi}$ 权重层，形状为 $[d \times d_k]$ 的 $W_{Ki}$，以及形状为 $[d \times d_v]$ 的 $W_{Vi}$。

以下是包含多个头的注意力机制的公式；图9.5 展示了其工作原理。

$$
\begin{aligned}
q^c_i &= x_i\mathbf{W}^{Qc}; \\
k^c_j &= x_j\mathbf{W}^{Kc}; \\
v^c_j &= x_j \mathbf{W}^{Vc}; \\
\forall c 1 \leq c \leq h
\end{aligned}
\tag{9.14}
$$

$$
\text{score}^c(x_i,x_j) = \frac{q^c_i \cdot k^c_j}{\sqrt{d_k}}
\tag{9.15}
$$

$$
\alpha^c_{ij} = \text{softmax}(\text{score}^c(x_i,x_j)) \quad \forall j \leq i
\tag{9.16}
$$

$$
\text{head}^c_i = \sum_{j\leq i}\alpha^c_{ij}v^c_j
\tag{9.17}
$$

$$
a_i = (\text{head}^1 \oplus \text{head}^2 \oplus \cdots \oplus \text{head}^h)\mathbf{W}^O
\tag{9.18}
$$

$$
\text{MultiHeadAttention}(x_i,[x_1,\cdots, x_N]) = a_i
\tag{9.19}
$$

每个 $h$ 个头的输出形状为 $1 \times d_v$，因此具有 $h$ 个头的多头注意力层的输出由 $h$ 个形状为 $1 \times d_v$ 的向量组成。这些向量被**拼接**（concatenated）起来，形成一个维度为 $1 \times hd_v$ 的单一输出。然后，我们再使用另一个线性投影矩阵 $\mathbf{W}^O \in \mathbb{R}^{hd_v \times d}$ 将其重塑，最终在每个输入位置 $i$ 产生一个与输入维度相同的多头注意力向量 $a_i$（即 $[1 \times d]$）。

**图9.5** 针对输入 $x_i$ 的多头注意力计算过程，产生输出 $a_i$。一个多头注意力层包含 $h$ 个头，每个头都有自己的“键”、“查询”和“值”权重矩阵。来自各个头的输出被拼接起来，然后通过一个投影矩阵降维至 $d$，从而产生一个与输入大小相同的输出。


<nav class="pagination justify-content-between">
<a href="../ch9">第9章 Transformer</a>
<a href="../">目录</a>
<a href="../ch9-02">9.2 Transformer 模块</a>
</nav>

