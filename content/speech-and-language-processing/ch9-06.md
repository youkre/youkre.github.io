---
title: "9.6 总结"
summary: ""
date: 2025-11-04T15:45:00+08:00
---

本章介绍了用于语言建模任务的 Transformer 及其各个组成部分。
我们将在下一章继续探讨语言建模的相关问题，包括训练和采样等。

以下是本章涵盖的主要内容总结：

- **Transformer** 是一种基于**多头注意力**（multi-head attention）的非循环神经网络，而多头注意力是一种**自注意力**（self-attention）机制。多头注意力计算通过将先前词元的向量（根据它们与当前词处理的相关性进行加权）累加进来，将输入向量 $\mathbf{x}_i$ 映射为输出向量 $\mathbf{a}_i$。
- 一个**Transformer 模块**（transformer block）包含一个**残差流**（residual stream），其中来自前一层的输入会直接传递到下一层，并与不同组件的输出相加。这些组件包括一个**多头注意力层**，后接一个**前馈层**，每个组件之前都带有**层归一化**（layer normalization）。通过堆叠多个 Transformer 模块，可以构建出更深、更强大的网络。
- Transformer 的输入是通过将一个嵌入（由**嵌入矩阵**计算得出）与一个表示词元在上下文窗口中序列位置的**位置编码**（positional encoding）相加得到的。
- 语言模型可以通过堆叠 Transformer 模块来构建，在顶部配有一个**语言模型头**（language model head）。该头部将一个**解嵌入**（unembedding）矩阵应用于顶层的输出 $H$，以生成**逻辑值**（logits），然后通过 softmax 函数生成词的概率分布。
- 基于 Transformer 的语言模型拥有非常宽的上下文窗口（对于大型模型，可宽达 32768 个词元），这使得它们能够利用海量的上下文信息来预测下一个词。

## 文献与历史注记

Transformer (Vaswani et al., 2017) 的发展借鉴了两条先前的研究路线：**自注意力**（self-attention）和**记忆网络**（memory networks）。

编码器-解码器注意力（encoder-decoder attention）的思想——即使用对输入词编码的软权重来指导生成式解码器（见第13章）——最初由 Graves (2013) 在手写生成的背景下提出，并由 Bahdanau 等人 (2015) 应用于机器翻译（MT）。
这一思想后来被扩展到自注意力机制：不再需要区分独立的编码和解码序列，而是将注意力视为一种对词元进行加权的方式，用以汇集从较低层传递到较高层的信息（Ling et al., 2015; Cheng et al., 2016; Liu et al., 2016）。

Transformer 的其他一些方面，包括**键**（key）、**查询**（query）和**值**（value）的术语，则来源于**记忆网络**（memory networks）。
记忆网络是一种为神经网络添加外部读写存储器的机制，其核心是使用查询的嵌入去匹配关联记忆中代表内容的键（Sukhbaatar et al., 2015; Weston et al., 2015; Graves et al., 2014）。


<nav class="pagination justify-content-between">
<a href="../ch9-05">9.5 语言模型头</a>
<a href="../">目录</a>
<span></span>
</nav>

