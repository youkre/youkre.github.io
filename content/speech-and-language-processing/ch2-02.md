---
title: "2.2 词"
summary: ""
date: 2025-10-14T21:00:00+08:00
---

在讨论如何处理词语之前，需要明确：什么才算作一个“词”？

我们从一个特定的**语料库**（corpus，复数形式为 corpora）开始分析。语料库是指可供计算机读取的文本或语音集合。例如，布朗语料库（Brown corpus）是1963–1964年间由布朗大学整理的一个百万词级英文书面语料库，包含来自500篇不同体裁（如新闻、小说、非虚构作品、学术文章等）的样本（Kučera 和 Francis，1967）。请看下面这句出自布朗语料库的句子：

```text
He stepped out into the hall, was delighted to encounter a water brother.
```

如果不将标点符号视为词，则该句包含13个词；若计入标点，则为15个词。是否将句号（“.”）、逗号（“,”）等符号当作独立的词，取决于具体任务。标点对于识别结构边界（如逗号、句号、冒号）以及表达某些语义特征（如问号、感叹号、引号）至关重要。在某些任务中，例如词性标注、句法分析或语音合成，有时会将标点符号当作独立的词来处理。

另一个例子是20世纪90年代初收集的美国英语电话对话语料库——Switchboard语料库。该语料库包含2430段陌生人之间的对话，平均每段约6分钟，总计约240小时的语音，相当于约300万个词（Godfrey 等，1992）。这类口语语料库在定义“词”时引入了更多复杂性。来看其中一句**话语**（utterance），即口语中与句子相对应的语言单位：

```text
I do uh main - mainly business data processing
```

这句话包含了两种类型的**不流畅现象**（disfluencies）。被中断的“main-”被称为**片段**（fragment）；像“uh”和“um”这样的词则被称为**填充词**或**填充停顿**（fillers or filled pauses）。是否应将这些也视为“词”？答案仍然取决于应用场景。如果构建的是语音转写系统，可能最终希望去除这些不流畅成分。

然而，在某些情况下也会保留这些不流畅现象。例如，“uh”或“um”在语音识别中其实有助于预测下一个词，因为它们可能暗示说话人正在重新组织句子或思路。因此，在语音识别任务中，这些填充词通常被当作常规词处理。此外，由于不同人使用不同的填充词，它们也可作为说话人身份识别的线索。事实上，Clark 和 Fox Tree（2002）的研究表明，“uh”和“um”具有不同的语用含义。你认为它们的区别是什么？

在思考“什么是词”这一问题时，或许最重要的是要区分两种关于“词”的表述方式，这种区分将在本书后续内容中持续发挥作用。**词元**（word types）指语料库中不同词形的数量；如果词汇集合为 V，则词元总数即为词汇量 $|V|$。**词例**（word instances）则是指文本中实际出现的所有词的总数 $N$，即“运行词数”。[^1] 如果忽略标点后，下面这句布朗语料库中的句子包含14个词元和16个词例：

[^1] 在早期传统中，乃至偶尔在现今文献中，人们可能用“词符”（word tokens）指代词例。但现在我们倾向于将“词符”（token）一词保留用于表示子词分词算法（subword tokenization）的输出结果。

```text
They picnicked by the pool, then lay back on the grass and looked at the stars.
```

我们仍需做出许多判断！例如，大写形式的词（如 *They*）和小写形式的词（如 *they*）是否应视为同一词元？答案依然是：视任务而定。在语音识别等任务中，我们更关注词序列本身而非格式，因此可能将 *They* 和 *they* 视为同一词元；但在命名实体识别等任务中，大写是一个有用的特征，有助于判断某个词是否为人名或地名，因此应予以保留。有时我们会维护两个版本的自然语言处理模型：一个保留大小写信息，另一个则不区分大小写。

| 语料库 | 词元数 = \|V\| | 词例数 = N |
|:------:|:-------:|:--: |
| 莎士比亚作品 | 3.1万 | 88.4万 |
| 布朗语料库 | 3.8万 | 100万 |
| Switchboard电话对话 | 2万 | 240万 |
| 美国当代英语语料库（COCA） | 200万 | 4.4亿 |
| Google n-grams | 1300万 | 1万亿 |

**图 2.11** 一些英语语料库的大致词形词元数与词例数。其中最大的Google n-grams语料库包含1300万个词元，但此统计仅包含出现次数不少于40次的词形，因此实际总数会更大。

英语中到底有多少个词？当我们谈论一种语言的词汇总量时，通常指的是词元数量。图2.11展示了基于若干英语语料库统计出的词元数和词例数的大致情况。语料库越大，发现的词元数量就越多。词元数 $|V|$ 与词例数 $N$ 之间的这种关系被称为**赫尔丹定律**（Herdan’s Law，Herdan，1960）或**希普斯定律**（Heaps’ Law，Heaps，1978），以分别在语言学和信息检索领域发现该规律的学者命名。其数学表达式如公式(2.18)所示，其中 $k$ 和 $\beta$ 为正常数，且 $0 < \beta < 1$：

$$
|V| = kN^\beta
\tag{2.18}
$$

$\beta$ 的取值依赖于语料库的规模和文本类型，但对于图2.11中较大的语料库而言，$\beta$ 通常介于0.67至0.75之间。粗略而言，这意味着一段文本的词汇量增长速度显著快于其长度（以词数计）的平方根。

我们还可以进一步细化区分。考虑像“cats”和“cat”这样的屈折形式。我们称这两个词为不同的**词形**（wordforms），但具有相同的**词目**（lemma）。**词目**是指一组具有相同词干、相同主要词性以及相同词义的词汇形式的集合。**词形**则是指词的完整屈折或派生形式。因此，“cat”和“cats”这两个词形共享同一个词目，可表示为“cat”。

对于阿拉伯语等形态结构复杂的语言，我们通常需要进行**词目归并**（lemmatization）。但在大多数英语任务中，使用词形已足够。本书中所讨论的“词”几乎都指词形（尽管我们将在2.6节中介绍词目归并与相关的词干提取（stemming）的基本算法）。即使在英语中，当需要衡量词典中的词条数量时，也会涉及词目的概念。词典中的**词条**或**黑体形式**（boldface forms）是对词目数量的一种粗略估计（上界），因为某些词目可能对应多个黑体形式。1989年版的《牛津英语词典》共收录了61.5万个词条。

最后需要指出的是，在实际应用中，许多自然语言处理任务（例如神经语言建模）并不直接以“词”作为内部表示的基本单位。相反，我们会将输入字符串进行**分词**（tokenize），生成**词符**（tokens），这些词符可以是完整的词，也可以只是词的一部分。我们将在2.5.2节介绍**BPE**（字节对编码）算法时再次深入探讨这一分词问题。


<nav class="pagination justify-content-between">
<a href="../ch2">第2章 正则表达式、词元化与编辑距离</a>
<a href="../">目录</a>
<a href="../ch2-03">2.3 语料库</a>
</nav>

