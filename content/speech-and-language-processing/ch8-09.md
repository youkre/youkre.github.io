---
title: "8.9 Transformer 的可解释性"
summary: ""
date: 2025-12-28T12:52:00+08:00
---

基于 Transformer 的语言模型为何能在各种语言任务上表现如此出色？
**可解释性**（interpretability）这一子领域——有时也称为**机制性可解释性**（mechanistic interpretability）——致力于从机制层面理解 Transformer 内部究竟发生了什么。
在接下来的两个小节中，我们将讨论 Transformer 可解释性研究中两个被深入探索的方向。

### 8.9.1 上下文学习与归纳头（In-Context Learning and Induction Heads）

为了让模型完成我们期望的任务，提示（prompting）与预训练（pretraining）在本质上是两种截然不同的方式。
预训练通过梯度下降更新模型参数，依据某个损失函数进行学习。
而带示例的提示（prompting with demonstrations）却能在不更新任何参数的情况下，教会模型执行新任务。
模型在处理提示的过程中，从这些示例中“学到”了关于任务的某种规律。

即使没有显式示例，提示过程本身也可被视为一种学习形式。
例如，随着模型在提示中读取的位置越靠后，它对后续词元的预测往往就越准确。
上下文中的信息正在提升模型的预测能力。

Brown 等人（2020）在介绍 GPT-3 时首次提出术语**上下文学习**（in-context learning），用以描述语言模型通过提示进行的这种学些。
上下文学习意味着语言模型在推理阶段仅通过前向传播（不进行任何梯度更新），就能学会执行新任务、更好地预测词元或总体上降低其损失。

那么，上下文学习是如何实现的？
尽管尚无定论，但已有若干引人注目的假说。
其中一种核心观点基于**归纳头**（induction heads）的概念（Elhage et al., 2021；Olsson et al., 2022）。
归纳头（induction heads）是一种**计算回路**（circuit）的名称，即网络中实现特定功能的一种抽象组件。
它是在 Transformer 的注意力计算中发现的一种结构，最初通过研究仅含 1–2 个注意力头的微型语言模型而被识别出来。

归纳头的功能是预测重复出现的序列模式。
例如，当输入序列为 `AB...A` 时，它会预测下一个词应为 `B`，从而实现一种**模式补全**（pattern completion）规则 $AB\ldots A \rightarrow B$。
它通过注意力计算中的一个 **前缀匹配**组件（prefix matching component）来实现这一点：当处理当前词元 `A` 时，该组件会在上下文中向后搜索，以找到 `A` 的先前出现位置。
一旦找到之前的 `A`，归纳头就使用**复制**机制（copying mechanism）“复制”紧随其后的词 `B`，通过提升 `B` 的出现概率来完成预测。
图 8.19 展示了一个实例。

![](/images/speech-and-language-processing/slp-fig-8-19.png)

**图 8.19** 一个归纳头在看到 `vintage` 时，利用**前缀**匹配机制找到之前出现的 `vintage`，再通过**复制**机制预测 `cars` 将再次出现。图源自 Crosbie & Shutova, 2022。

Olsson 等人（2022）进一步提出这种模式补全规则的一种泛化的、模糊的版本，即实现类似 $A^*B^*\ldots A \rightarrow B$，其中 $A^* \approx A$、$B^* \approx B$（这里的 $\approx$ 表示它们在某种意义上具有语义相似性），这一规则可能是上下文学习背后的原因。
支持该假说的证据来自 Crosbie 与 Shutova（2022）的研究：他们发现，移除（ablate）归纳头会导致上下文学习性能显著下降。
**“消融”**（ablation）原为医学术语，意为“切除某物”。
在 NLP 可解释性研究中，它被用作检验因果关系的工具：如果移除某个假设的关键组件，预期效果应随之消失。
具体而言，Crosbie 与 Shutova 首先识别出在随机输入序列上表现出归纳头行为的注意力头，然后通过将输出矩阵 $\mathbf{W}_O$ 中对应项置零，从而“关闭”这些头的输出。
结果表明，经过消融的模型在上下文学习任务上表现大幅退化——尤其在从提示中的示例进行学习时，性能明显变差。

### 8.9.2 Logit 透镜

另一种实用的可解释性工具是 **Logit Lens**（Nostalgebraist, 2020），它提供了一种可视化 Transformer 内部层表示内容的方法。

其核心思想是：任取 Transformer 中任意一层的任意向量，假装它是最终输出前的嵌入向量，然后将其乘以**解嵌入层**，得到 logit 向量，并通过 softmax 计算出对应的词表概率分布。
这样，我们就能大致看出该内部向量“试图表达”哪些词。
这为我们理解模型内部表征提供了一个有用的窗口。
当然，由于网络并未被显式训练成让中间层向量具备这种语义可读性，Logit Lens 并非总是有效。但即便如此，它仍是一个有用的技巧，能帮助我们可视化 Transformer 各层的内部状态。


<nav class="pagination justify-content-between">
<a href="../ch8-08">8.8 应对规模挑战</a>
<a href="../">目录</a>
<a href="../ch9">第 9 章 掩码语言模型</a>
</nav>

