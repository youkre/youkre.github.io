---
title: "4.2 训练朴素贝叶斯分类器"
date: "2025-09-21T09:13:05+08:00"
---

我们如何学习概率 $P(c)$ 和 $P(f_i|c)$ 呢？首先考虑**最大似然估计（maximum likelihood estimate）**。我们将直接使用数据中的频率。对于类别先验概率 $P(c)$，我们计算训练集中属于每个类别 $c$ 的文档所占的百分比。令 $N_c$ 表示训练数据中类别为 $c$ 的文档数量，$N_{doc}$ 表示文档总数。那么：

$$
\hat{P}(c) = \frac{N_c}{N_{doc}} \tag{4.11}
$$

为了学习概率 $P(f_i|c)$，我们假设特征就是某个词在文档词袋中出现与否，因此需要计算 $P(w_i|c)$，即词 $w_i$ 在所有属于主题 $c$ 的文档的所有词中出现的频率。首先将所有类别为 $c$ 的文档连接成一个巨大的“类别 $c$”文本，然后使用词 $w_i$ 在这个合并文本中的频率来估计其概率：

$$
\hat{P}(w_i|c) = \frac{\mathrm{count}(w_i,c)}{\sum_{w \in V} \mathrm{count}(w,c)} \tag{4.12}
$$

这里，词汇表 $V$ 包含了所有类别中所有词类型的并集，而不仅仅是某一类别 $c$ 中的词。

然而，最大似然估计存在一个问题。假设我们试图估计词“fantastic”在 *positive*（正面）类别下的概率，但训练集中没有任何文档同时包含“fantastic”这个词且被标记为 *positive*。也许“fantastic”这个词恰好出现在 *negative*（负面）类别中（带有讽刺意味？）。在这种情况下，该特征的概率将为零：

$$
\hat{P}(\text{“fantastic”}|positive) = \frac{\mathrm{count}(\text{“fantastic”},positive)}{\sum_{w \in V} \mathrm{count}(w,positive)} = 0 \tag{4.13}
$$

但由于朴素贝叶斯会“朴素地”将所有特征的似然度相乘，任何一个类别的似然项中出现零概率，都会导致该类别的总概率为零，无论其他证据多么有力！

最简单的解决方案是第3章介绍的**加一平滑（add-one smoothing）**，也称**拉普拉斯平滑（Laplace smoothing）**。虽然在语言建模中，拉普拉斯平滑通常会被更复杂的平滑算法取代，但在朴素贝叶斯文本分类中，它仍然被广泛使用：

$$
\hat{P}(w_i|c) = \frac{\mathrm{count}(w_i,c)+ 1}{\sum_{w \in V} (\mathrm{count}(w,c) + 1)} = \frac{\mathrm{count}(w_i,c)+ 1}{(\sum_{w \in V} \mathrm{count}(w,c)) +|V|} \tag{4.14}
$$

再次强调，词汇表 $V$ 必须包含所有类别中所有词类型的并集，这一点至关重要，而不仅仅是一个类别 $c$ 中的词（请尝试说服自己为什么必须如此；参见本章末尾的练习题）。

那么，对于那些在测试数据中出现，但由于在任何训练文档的任何类别中都未出现过而完全不在我们词汇表中的词，我们该如何处理呢？对于这类**未知词（unknown words）**，解决方案是忽略它们——将它们从测试文档中移除，完全不计算它们的概率。

最后，一些系统选择完全忽略另一类词：**停用词（stop words）**，即像 *the* 和 *a* 这样非常常见的词。这可以通过按训练集中的词频对词汇表进行排序，并将频率最高的前10到100个词定义为停用词来实现，或者使用网上可获取的众多预定义停用词列表之一。然后，训练和测试文档中所有这些停用词的实例都会被简单地移除，就好像它们从未出现过一样。然而，在大多数文本分类应用中，使用停用词列表并不能提高性能，因此更常见的做法是利用整个词汇表，而不使用停用词列表。

图 4.2 展示了最终的算法。

<figure>

![](/images/speech-and-language-processing/slp-fig-4-2.png)

<figcaption>

**图 4.2** 朴素贝叶斯算法，使用加1平滑（add-1 smoothing）。若要改用加α平滑（add-α smoothing），只需在训练阶段将似然概率的计数中的 +1 改为 +α 即可。

</figcaption>
</figure>
