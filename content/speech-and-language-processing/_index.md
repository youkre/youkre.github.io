---
title: "言语与语言处理"
---

原文：[Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)

## 目录

- [第2章 词与词元](ch2)
  - [2.1 词](ch2-01)
  - [2.2 语素：词的组成部分](ch2-02)
  - [2.3 Unicode](ch2-03)
  - [2.4 子词切分：字节对编码](ch2-04)
  - [2.5 基于规则的词元化](ch2-05)
  - [2.6 语料库](ch2-06)
  - [2.7 正则表达式](ch2-07)
  - [2.8 用于词元化的简易 Unix 工具](ch2-08)
  - [2.9 最小编辑距离（Minimum Edit Distance）](ch2-09)
- [第3章 N元语言模型](ch3)
  - [3.1 N元模型](ch3-01)
  - [3.2 评估语言模型：训练集与测试集](ch3-02)
  - [3.3 评估语言模型：困惑度（Perplexity）](ch3-03)
  - [3.4 从语言模型中采样句子](ch3-04)
  - [3.5 对训练集的泛化与过拟合](ch3-05)
  - [3.6 平滑、插值与回退](ch3-06)
  - [3.7 进阶：困惑度与熵的关系](ch3-07)
- [第4章 罗辑回归与文本分类](ch4)
  - [4.1 机器学习与分类](ch4-01)
  - [4.2 Sigmoid 函数](ch4-02)
  - [4.3 使用逻辑回归进行分类](ch4-03)
  - [4.4 多项逻辑回归](ch4-04)
  - [4.5 逻辑回归中的学习](ch4-05)
  - [4.6 交叉熵损失函数](ch4-06)
  - [4.7 梯度下降](ch4-07)
  - [4.8 多项逻辑回归中的学习](ch4-08)
  - [4.9 评估指标：精确率、召回率与 F 值](ch4-09)
  - [4.10 测试集与交叉验证](ch4-10)
  - [4.11 统计显著性检验](ch4-11)
  - [4.12 分类中的危害防范](ch4-12)
  - [4.13 模型的解释](ch4-13)
  - [4.14 进阶：正则化](ch4-14)
  - [4.15 进阶：梯度公式的推导](ch4-15)
- [第5章 嵌入](ch5)
  - [5.1 词汇语义](ch5-01)
  - [5.2 向量语义：原理](ch5-02)
  - [5.3 基于计数的简单嵌入](ch5-03)
  - [5.4 用余弦度量相似性](ch5-04)
  - [5.5 Word2vec](ch5-05)
  - [5.6 嵌入向量的可视化](ch5-06)
  - [5.7 嵌入向量的语义特性](ch5-07)
  - [5.8 偏见与嵌入](ch5-08)
  - [5.9 向量模型的评估](ch5-09)
- [第 6 章 神经网络](ch6)
  - [6.1 单元](ch6-01)
  - [6.2 XOR 问题](ch6-02)
  - [6.3 前馈神经网络](ch6-03)
  - [7.4 前馈网络在自然语言处理中的应用：分类任务](ch6-04)
  - [6.5 作为神经网络分类器输入的嵌入表示](ch6-05)
  - [6.5 神经网络的训练](ch6-06)
- [第 7 章 大语言模型](ch7)
  - [7.1 语言模型的三种架构](ch7-01)
  - [7.2 文本的条件生成：基本原理](ch7-02)
  - [7.3 提示](ch7-03)
  - [7.4 生成与采样](ch7-04)
  - [7.5 大语言模型的训练](ch7-05)
  - [7.6 大语言模型的评估](ch7-06)
  - [7.7 语言模型的伦理与安全问题](ch7-07)
- [第 8 章 Transformer](ch8)
  - [8.1 注意力机制](ch8-01)
  - [8.2 Transformer 块](ch8-02)
  - [8.3 使用单个矩阵 X 并行化计算](ch8-03)
  - [8.4 输入：词元嵌入与位置嵌入](ch8-04)
  - [8.5 语言建模头部](ch8-05)
  - [8.6 关于采样方法的更多讨论](ch8-06)
  - [8.7 训练](ch8-07)
  - [8.8 应对规模挑战](ch8-08)
  - [8.9 Transformer 的可解释性](ch8-09)
- [第 9 章 掩码语言模型](ch9)
  - [9.1 双向 Transformer 编码器](ch9-01)
  - [11.2 双向编码器的训练](ch9-02)
  - [9.3 上下文嵌入](ch9-03)
  - [9.4 用于分类任务的微调](ch9-04)
  - [9.5 用于序列标注的微调：命名实体识别](ch9-05)
- [第11章 信息检索与检索增强生成](ch11)
  - [11.1 信息检索](ch11-01)
  - [11.2 基于稠密向量的信息检索](ch11-02)
  - [11.3 使用 RAG 回答问题](ch11-03)
  - [11.4 问答数据集](ch11-04)
  - [11.5 问答系统的评估](ch11-05)
- [第 12 章 机器翻译](ch12)
  - [12.1 语言差异与类型学](ch12-01)
  - [12.2 基于编码器-解码器的机器翻译](ch12-02)
  - [12.3 编码器-解码器模型的细节](ch12-03)
  - [12.4 机器翻译中的解码：束搜索](ch12-04)
  - [12.5 低资源场景下的翻译](ch12-05)
  - [12.6 机器翻译评估](ch12-06)
  - [12.7 偏见与伦理问题](ch12-07)
- [第 13 章 循环神经网络与长短期记忆网络](ch13)
  - [13.1 循环神经网络](ch13-01)
  - [13.2 RNN 作为语言模型](ch13-02)
  - [13.3 RNN 在其他 NLP 任务中的应用](ch13-03)
  - [13.4 堆叠式与双向 RNN 架构](ch13-04)
  - [13.5 长短期记忆网络（LSTM）](ch13-05)
  - [13.6 小结：常见的 RNN 自然语言处理架构](ch13-06)
  - [13.7 基于 RNN 的编码器-解码器模型](ch13-07)
  - [13.8 注意力机制（Attention）](ch13-08)
