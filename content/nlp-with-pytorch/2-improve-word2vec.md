---
title: "æ”¹è¿›Word2Vecï¼šä»â€œæš´åŠ›è®¡ç®—â€åˆ°â€œèªæ˜å­¦ä¹ â€"
summary: "Word2Vec åŸå§‹æ¨¡å‹è®¡ç®—å¤ªæ…¢ï¼Ÿæœ¬æ–‡å¸¦ä½ å‡çº§ï¼ç”¨ nn.Embedding æ›¿ä»£ one-hotï¼Œé«˜æ•ˆæå–è¯å‘é‡ã€‚å¼•å…¥è´Ÿé‡‡æ ·ï¼ŒåŒ–â€œå¤§æµ·æé’ˆâ€ä¸ºâ€œçœŸå‡åˆ¤æ–­â€ï¼Œå¤§å¹…åŠ é€Ÿè®­ç»ƒã€‚ä»£ç å®æˆ˜ï¼Œæ•™ä½ æ‰“é€ èªæ˜é«˜æ•ˆçš„è¯å‘é‡æ¨¡å‹ã€‚"
date: 2025-10-07T15:19:00+08:00
---

> ä¸Šä¸€ç¯‡ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªâ€œå¡«ç©ºæ¸¸æˆâ€æ•™ä¼šäº†æ¨¡å‹é¢„æµ‹å•è¯â€”â€”è¿™å°±æ˜¯ç»å…¸çš„ **CBOW æ¨¡å‹**ã€‚  
> ä½†æˆ‘ä»¬çš„æ¨¡å‹æœ‰ä¸ªâ€œå°æ¯›ç—…â€ï¼šå®ƒå¤ªâ€œç¬¨â€äº†ï¼Œæ¯æ¬¡éƒ½è¦æŠŠæ•´ä¸ªè¯è¡¨è¿‡ä¸€éï¼Œåƒä¸ªå­¦ç”Ÿæ¯æ¬¡è€ƒè¯•éƒ½è¦æŠŠå­—å…¸ä»å¤´åˆ°å°¾èƒŒä¸€éã€‚  
> ä»Šå¤©ï¼Œæˆ‘ä»¬å°±æ¥ç»™å®ƒâ€œå‡çº§è£…å¤‡â€ï¼Œè®©å®ƒå˜å¾—æ›´èªæ˜ã€æ›´é«˜æ•ˆï¼

## é—®é¢˜æ¥äº†ï¼šæ¨¡å‹å¤ªâ€œç´¯â€äº†

è¿˜è®°å¾—ä¸Šä¸€ç¯‡çš„æ¨¡å‹æ˜¯æ€ä¹ˆå·¥ä½œçš„å—ï¼Ÿ

1. è¾“å…¥ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼ˆæ¯”å¦‚ `["you", "goodbye"]`ï¼‰ã€‚
2. æ¨¡å‹è¦è®¡ç®—è¿™ä¸ªè¯ç»„åˆå’Œ**è¯è¡¨ä¸­æ¯ä¸€ä¸ªè¯**çš„åŒ¹é…ç¨‹åº¦ã€‚
3. æœ€åè¾“å‡ºä¸€ä¸ªé•¿é•¿çš„æ¦‚ç‡åˆ—è¡¨ï¼Œå‘Šè¯‰æˆ‘ä»¬ä¸‹ä¸€ä¸ªè¯æœ€å¯èƒ½æ˜¯â€œsayâ€ã€â€œhelloâ€è¿˜æ˜¯â€œcatâ€ã€‚

è¿™å¬èµ·æ¥æ²¡é—®é¢˜ï¼Œä½†å¦‚æœè¯è¡¨æœ‰ **10ä¸‡ä¸ªè¯** å‘¢ï¼Ÿæ¨¡å‹æ¯æ¬¡éƒ½è¦åšä¸€æ¬¡ 300Ã—100,000 çš„çŸ©é˜µä¹˜æ³•ï¼ˆå‡è®¾å‘é‡æ˜¯300ç»´ï¼‰ï¼Œè¿™å°±åƒè®©ä¸€ä¸ªå­¦ç”Ÿæ¯æ¬¡åªçŒœä¸€ä¸ªå­—ï¼Œå´è¦å†™å®Œ10ä¸‡é“é€‰æ‹©é¢˜ï¼

æ›´ç³Ÿç³•çš„æ˜¯ï¼Œè®­ç»ƒæ—¶åå‘ä¼ æ’­è¦æ›´æ–° **3000ä¸‡ä¸ªå‚æ•°**ï¼ˆ300 Ã— 100,000ï¼‰ï¼Œè¿™ç®€ç›´æ˜¯â€œç¾éš¾çº§â€çš„è®¡ç®—å¼€é”€ã€‚

> **ä¸€å¥è¯æ€»ç»“é—®é¢˜**ï¼šæˆ‘ä»¬è®©æ¨¡å‹åšäº†ä¸€é“â€œ10ä¸‡é€‰1â€çš„è¶…éš¾é€‰æ‹©é¢˜ï¼Œå®ƒç´¯å®äº†ï¼

## å‡çº§ç¬¬ä¸€æ­¥ï¼šå‘Šåˆ«One-Hotï¼Œæ‹¥æŠ± `nn.Embedding`

è¿˜è®°å¾—æˆ‘ä»¬æ˜¯æ€ä¹ˆæŠŠâ€œhelloâ€å˜æˆå‘é‡çš„å—ï¼Ÿå…ˆå˜æˆ one-hot å‘é‡ `[0,1,0,0,...]`ï¼Œå†ä¹˜ä»¥ä¸€ä¸ªå¤§çŸ©é˜µã€‚

è¿™å°±åƒï¼šä½ è¦ä»10ä¸‡ä¸ªæŠ½å±‰é‡Œæ‹¿ä¸€ä¸ªæ–‡ä»¶ï¼Œå…ˆå†™ä¸€å¼ â€œæˆ‘è¦ç¬¬2å·æŠ½å±‰â€çš„çº¸æ¡ï¼ˆone-hotï¼‰ï¼Œç„¶åè®©æœºå™¨äººæŠŠè¿™å¼ çº¸æ¡å’Œ10ä¸‡ä¸ªæŠ½å±‰çš„æ ‡ç­¾ä¸€ä¸€æ¯”å¯¹ï¼Œæœ€åæ‰æ‰“å¼€ç¬¬2å·æŠ½å±‰ã€‚

å¤ªéº»çƒ¦äº†ï¼ä¸ºä»€ä¹ˆä¸ç›´æ¥å‘Šè¯‰æœºå™¨äººï¼šâ€œå»2å·æŠ½å±‰æ‹¿æ–‡ä»¶â€ï¼Ÿ

è¿™å°±æ˜¯ `nn.Embedding` å±‚çš„æ™ºæ…§ï¼

### `nn.Embedding`ï¼šä½ çš„â€œæ™ºèƒ½æŠ½å±‰ç®¡ç†å‘˜â€

```python
self.embedding = nn.Embedding(vocab_size, hidden_size)
```

- `vocab_size=100000`ï¼šæ€»å…±æœ‰10ä¸‡ä¸ªæŠ½å±‰ã€‚
- `hidden_size=300`ï¼šæ¯ä¸ªæŠ½å±‰é‡Œæ”¾ä¸€ä¸ª300ç»´çš„å‘é‡ã€‚

**å®ƒæ€ä¹ˆå·¥ä½œï¼Ÿ**

ç›´æ¥ç»™å®ƒä¸€ä¸ªè¯çš„IDï¼ˆæ¯”å¦‚ `2`ï¼‰ï¼Œå®ƒç¬é—´å°±è¿”å›å¯¹åº”çš„è¯å‘é‡ï¼Œå°±åƒæœºå™¨äººç›´æ¥å»2å·æŠ½å±‰å–æ–‡ä»¶ï¼Œè·³è¿‡äº†ç¹ççš„æ¯”å¯¹è¿‡ç¨‹ã€‚

> **å¥½å¤„**ï¼šè®¡ç®—å¿«ã€å†…å­˜çœã€ä»£ç ç®€æ´ã€‚

æˆ‘ä»¬æŠŠä¸Šä¸€ç¯‡çš„ `nn.Linear` æ¢æˆ `nn.Embedding`ï¼Œæ¨¡å‹å°±è½»è£…ä¸Šé˜µäº†ï¼š

```python
class CBOWModel(nn.Module):
    def __init__(self, vocab_size: int, hidden_size: int):
        super().__init__()
        # æ›¿æ¢æ‰ Linear
        # Embedding å±‚çš„è¾“å…¥(*) æ˜¯ä»»æ„å½¢çŠ¶çš„ IntTensor æˆ– LongTensorï¼Œè¿™æ˜¯è¦æå–çš„ç´¢å¼•
        # è¾“å‡º(*, H) å…¶ä¸­ * æ˜¯è¾“å…¥å½¢çŠ¶
        self.embedding = nn.Embedding(vocab_size, hidden_size)  # è¾“å…¥å±‚ -> éšè—å±‚
        self.output_layer = nn.Linear(hidden_size, vocab_size)  # éšè—å±‚ -> è¾“å‡ºå±‚

    def forward(self, contexts: torch.Tensor):
        """
        è¾“å…¥: (batch_size, 2)
        è¾“å‡º: (batch_size, vocab_size)
        """
        # (batch_size, hidden_size)
        h0 = self.embedding(contexts[:, 0]) # ç›´æ¥ç´¢å¼•ï¼Œå¿«ï¼
        # (batch_size, hidden_size)
        h1 = self.embedding(contexts[:, 1])
        # (batch_size, hidden_size)
        h = (h0 + h1) * 0.5
        # (batch_size, vocab_size)
        out = self.output_layer(h)
        return out
```

**ä½†é—®é¢˜è¿˜æ²¡è§£å†³ï¼** è™½ç„¶å–å‘é‡å¿«äº†ï¼Œä½†æœ€åçš„ `output_layer` è¿˜æ˜¯è¦å¯¹10ä¸‡ä¸ªè¯åšè®¡ç®—ã€‚æ¨¡å‹è¿˜æ˜¯å¤ªç´¯ã€‚

## å‡çº§ç¬¬äºŒæ­¥ï¼šè´Ÿé‡‡æ ·â€”â€”â€œåŒ–æ•´ä¸ºé›¶â€çš„èªæ˜å­¦ä¹ æ³•

æˆ‘ä»¬å¾—æ¢ä¸ªæ€è·¯ï¼š**ä¸æâ€œ10ä¸‡é€‰1â€çš„å¤§è€ƒï¼Œæ”¹æâ€œ1å¯¹å¤šâ€çš„å°æµ‹éªŒã€‚**

è¿™å°±æ˜¯ **è´Ÿé‡‡æ ·ï¼ˆNegative Samplingï¼‰** çš„æ ¸å¿ƒæ€æƒ³ã€‚

### è´Ÿé‡‡æ ·ï¼šè€å¸ˆå‡ºé¢˜çš„æ–°æ–¹å¼

æƒ³è±¡ä½ æ˜¯è€å¸ˆï¼Œæƒ³è€ƒå­¦ç”Ÿå¯¹â€œsayâ€è¿™ä¸ªè¯çš„æŒæ¡ã€‚

**è€æ–¹æ³•ï¼ˆæ•ˆç‡ä½ï¼‰ï¼š**

> â€œyou say goodbye and I ___ helloâ€ çš„é€‰é¡¹æ˜¯ï¼š
> A. say  B. hello  C. goodbye  D. and  E. I  ...ï¼ˆå…±10ä¸‡ä¸ªé€‰é¡¹ï¼‰
> å­¦ç”Ÿè¦çœ‹å®Œæ‰€æœ‰é€‰é¡¹æ‰èƒ½é€‰Bã€‚

**æ–°æ–¹æ³•ï¼ˆè´Ÿé‡‡æ ·ï¼Œæ•ˆç‡é«˜ï¼‰ï¼š**

> â€œyou say goodbye and I ___ helloâ€ çš„é€‰é¡¹æ˜¯ï¼š
> A. sayï¼ˆæ­£ç¡®ç­”æ¡ˆï¼‰  
> B. catï¼ˆä¹±é€‰ä¸€ä¸ªé”™çš„ï¼‰  
> C. computerï¼ˆå†ä¹±é€‰ä¸€ä¸ªï¼‰  
> D. loveï¼ˆå†é€‰ä¸€ä¸ªï¼‰  
> E. runï¼ˆå†é€‰ä¸€ä¸ªï¼‰

åªéœ€è¦åˆ¤æ–­ï¼šâ€œAæ˜¯å¯¹çš„ï¼ŒBã€Cã€Dã€Eéƒ½æ˜¯é”™çš„â€ã€‚è¿™é¢˜ç®€å•å¤šäº†ï¼

- **æ­£æ ·æœ¬ï¼ˆPositive Sampleï¼‰**ï¼šçœŸå®çš„ä¸Šä¸‹æ–‡+ç›®æ ‡è¯ç»„åˆï¼Œæ¯”å¦‚ `(["you", "goodbye"], "say")`ã€‚
- **è´Ÿæ ·æœ¬ï¼ˆNegative Sampleï¼‰**ï¼šåŒä¸€ä¸ªä¸Šä¸‹æ–‡ + ä¸€ä¸ª**éšæœºé€‰çš„ã€é”™è¯¯çš„è¯**ï¼Œæ¯”å¦‚ `(["you", "goodbye"], "cat")`ã€‚

æˆ‘ä»¬çš„ç›®æ ‡å˜æˆäº†ï¼š

- è®©æ¨¡å‹è®¤ä¸ºâ€œæ­£æ ·æœ¬â€æ˜¯çœŸçš„ï¼ˆå¾—åˆ†æ¥è¿‘1ï¼‰ã€‚
- è®©æ¨¡å‹è®¤ä¸ºâ€œè´Ÿæ ·æœ¬â€æ˜¯å‡çš„ï¼ˆå¾—åˆ†æ¥è¿‘0ï¼‰ã€‚

è¿™ä¸å†æ˜¯ä¸€ä¸ªâ€œå¤šåˆ†ç±»â€é—®é¢˜ï¼Œè€Œæ˜¯ä¸€ä¸ªâ€œäºŒåˆ†ç±»â€é—®é¢˜ï¼Œä½†æ¯ä¸ª batch é‡Œæœ‰å¤šä¸ªäºŒåˆ†ç±»ä»»åŠ¡ï¼ˆ1ä¸ªæ­£æ ·æœ¬ + Kä¸ªè´Ÿæ ·æœ¬ï¼‰ã€‚

### å®ç°è´Ÿé‡‡æ ·

æˆ‘ä»¬å…ˆå†™ä¸€ä¸ªå‡½æ•°ï¼Œç»™æ¯ä¸ªç›®æ ‡è¯é‡‡æ ·å‡ºå‡ ä¸ªâ€œæ›¿èº«â€ï¼ˆè´Ÿæ ·æœ¬ï¼‰ï¼š

```python
def sample_negative_words(target_ids: torch.Tensor, vocab_size: int, num_neg_samples: int):
    """
    ä»è¯æ±‡è¡¨ä¸­é‡‡æ ·å‡ºä¸åŒ…å«ç›®æ ‡è¯çš„è´Ÿæ ·æœ¬ã€‚
    target_ids: (batch_size,)
    è¾“å‡º: (batch_size, num_neg_samples)
    num_neg_samples æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªè´Ÿé‡‡æ ·å•è¯çš„ç´¢å¼•
    """
    batch_size = target_ids.size(0)

    # # æ¯ä¸ªè¯åˆå§‹æƒé‡éƒ½æ˜¯1 (batch_size, vocab_size)
    weights = torch.ones(batch_size, vocab_size)

    # # æŠŠç›®æ ‡è¯çš„æƒé‡è®¾ä¸º0ï¼Œè®©å®ƒä¸ä¼šè¢«é€‰ä¸­
    weights[torch.arange(batch_size), target_ids] = 0

    # ä½¿ç”¨ multinomial è¿›è¡Œé‡‡æ ·ï¼ˆä¸æ”¾å›ï¼‰
    # ä»æ¯ä¸€è¡Œä¸­é€‰æ‹© num_neg_samples ä¸ªå…ƒç´ çš„ç´¢å¼•ï¼Œç”±äºç›®æ ‡è¯çš„ä½ç½®ä¸º0ï¼Œæ‰€ä»¥ä¼šè·³è¿‡ç›®æ ‡è¯ã€‚
    # (batch_zie, num_neg_samples)
    negative_word_ids = torch.multinomial(weights, num_neg_samples, replacement=False)

    return negative_word_ids
```

å°±åƒæŠ½å¥–æ—¶ï¼ŒæŠŠæ­£ç¡®ç­”æ¡ˆçš„æŠ½å¥–åˆ¸æ’•æ‰ï¼Œç¡®ä¿ä¸ä¼šæŠ½åˆ°è‡ªå·±ã€‚

### æ­å»ºæ–°æ¨¡å‹

ç°åœ¨æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªæ”¯æŒè´Ÿé‡‡æ ·çš„CBOWæ¨¡å‹ï¼š

```python
class CBOWNegativeSampling(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int):
        super().__init__()
        # ä¸¤ä¸ªåµŒå…¥å±‚ï¼šä¸€ä¸ªç»™ä¸Šä¸‹æ–‡è¯ç”¨ï¼Œä¸€ä¸ªç»™ç›®æ ‡è¯ç”¨
        self.context_embedding = nn.Embedding(vocab_size, embedding_dim)  # ä¸Šä¸‹æ–‡å‘é‡
        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)     # ç›®æ ‡è¯å‘é‡

    def forward(self, context_ids: torch.Tensor, input_ids: torch.Tensor):
        """
        context_ids: (batch_size, context_size) ä¸Šä¸‹æ–‡è¯ID
        input_ids:   (batch_size, 1 + K)        ç›®æ ‡è¯ID + Kä¸ªè´Ÿæ ·æœ¬ID
        """
        # 1. è·å–ä¸Šä¸‹æ–‡å‘é‡å¹¶å¹³å‡
        # (batch_size, context_size, embedding_size)
        context_vecs = self.context_embedding(context_ids)
        # (batch_size, embedding_size)
        context_mean = torch.mean(context_vecs, dim=1)

        # 2. è·å–ç›®æ ‡è¯å’Œè´Ÿæ ·æœ¬çš„å‘é‡
        # (batch_size, 1+K, embedding_dim)
        word_vecs = self.word_embedding(input_ids)

        # 3. è®¡ç®—ç‚¹ç§¯å¾—åˆ†ï¼ˆç›¸ä¼¼åº¦ï¼‰
        # bmm: æŠŠæ¯ä¸ªä¸Šä¸‹æ–‡å¹³å‡å‘é‡å’Œæ¯ä¸ªå€™é€‰è¯å‘é‡åšç‚¹ç§¯
        # context_mean.unsqueeze(2) æ·»åŠ ä¸€ä¸ªç»´åº¦ï¼š
        # (batch_size, embedding_size) -> (batch_size, embedding_dim, 1)
        # torch.bmm æ“ä½œï¼š
        # (batch_size, 1+K, embedding_dim) @ (batch_size, embedding_dim, 1)
        # -> (batch_size, 1+K, 1)
        scores = torch.bmm(word_vecs, context_mean.unsqueeze(2))
        # squeeze(2) -> (batch_size, 1+K)
        scores = scores.squeeze(2)

        return scores  # è¿”å›åŸå§‹å¾—åˆ†ï¼Œlossä¼šè‡ªåŠ¨åŠ sigmoid
```

> ğŸ” **ä¸ºä»€ä¹ˆæœ‰ä¸¤ä¸ªåµŒå…¥å±‚ï¼Ÿ**
> ç†è®ºä¸Šå¯ä»¥å…±äº«ï¼Œä½†å®è·µä¸­å¸¸åˆ†å¼€ã€‚ä½ å¯ä»¥ç†è§£ä¸ºâ€œä¸Šä¸‹æ–‡è§’è‰²â€å’Œâ€œç›®æ ‡è¯è§’è‰²â€ç•¥æœ‰ä¸åŒï¼Œåˆ†å¼€å­¦ä¹ æ›´çµæ´»ã€‚

> ğŸ” **ä¸ºä»€ä¹ˆè¿”å› `scores` è€Œä¸æ˜¯ `sigmoid(scores)`ï¼Ÿ**
> å› ä¸ºæˆ‘ä»¬ä¼šç”¨ `BCEWithLogitsLoss`ï¼Œå®ƒå†…éƒ¨ä¼šåš `sigmoid`ï¼Œè¿™æ ·æ•°å€¼æ›´ç¨³å®šã€‚

### æŸå¤±å‡½æ•°ï¼šäºŒåˆ†ç±»äº¤å‰ç†µ

```python
criterion = nn.BCEWithLogitsLoss()  # è‡ªå¸¦ sigmoidï¼Œæ›´ç¨³å®š
```

æ ‡ç­¾é•¿è¿™æ ·ï¼š

```python
labels = torch.zeros(batch_size, 1 + num_neg_samples)
labels[:, 0] = 1.0  # åªæœ‰ç¬¬ä¸€ä¸ªï¼ˆæ­£æ ·æœ¬ï¼‰æ˜¯1ï¼Œå…¶ä½™éƒ½æ˜¯0
```

## æ•°æ®æµæ°´çº¿ï¼š`collate_fn` çš„å¦™ç”¨

ç”¨ Dateset è¯»å–åŸå§‹æ•°æ®æä¾›ç»™ DataLoaderï¼š

```python
# è¿™ä¸ªç±»è´Ÿè´£ä»åŸå§‹è¯­æ–™ä¸­æå–ä¸Šä¸‹æ–‡å’Œç›®æ ‡è¯å¯¹ã€‚
class CBOWDataset(Dataset):
    def __init__(self, corpus: List[int], window_size: int = 1):
        self.corpus = corpus # å·²ç»è½¬æ¢ä¸º token ID çš„åˆ—è¡¨
        self.window_size = window_size
    
    def __len__(self):
        return len(self.corpus)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        if idx < self.window_size or idx >= len(self.corpus) - self.window_size:
            return {
                'context_ids': torch.tensor([], dtype=torch.long),
                'target_id': torch.tensor(-1, dtype=torch.long)
            }
        target_id = self.corpus[idx]
        context_ids: List[int] = []
        for i in range(-self.window_size, self.window_size + 1):
            if i == 0:
                continue
            context_ids.append(self.corpus[idx + i])

        return {
            'context_ids': torch.tensor(context_ids, dtype=torch.long), # shape: (window_size * 2, )
            'target_id': torch.tensor(target_id, dtype=torch.long) # shape: (1,)
        }
```

è´Ÿé‡‡æ ·éœ€è¦åœ¨æ¯ä¸ª batch ç”Ÿæˆï¼Œä½†æˆ‘ä»¬ä¸æƒ³æŠŠè¿™ç§â€œè®­ç»ƒé€»è¾‘â€å¡è¿› `Dataset`ã€‚æ€ä¹ˆåŠï¼Ÿ

PyTorch æä¾›äº† `collate_fn`â€”â€”å®ƒåƒä¸€ä¸ªâ€œæ‰“åŒ…åŠ©æ‰‹â€ï¼Œåœ¨æ•°æ®é€å…¥æ¨¡å‹å‰ï¼ŒæŠŠé›¶æ•£çš„æ ·æœ¬æ‰“åŒ…æˆ batchï¼Œå¹¶åšé¢å¤–å¤„ç†ã€‚

```python
def collate_fn(batch: List[Dict[str, torch.Tensor]], num_neg_samples: int, vocab_size: int):
    # è¿‡æ»¤æ— æ•ˆæ ·æœ¬
    batch = [item for item in batch if item['context_ids'].numel() > 0]
    
    # æå–ä¸Šä¸‹æ–‡å’Œç›®æ ‡è¯
    # (batch_size, context_size)
    # ä¾‹å¦‚ b['context_ids'] æ˜¯ [6, 2]
    # Stackåˆ° [1, 2] å¾—åˆ°
    # [[6, 2],
    #  [1, 2]]
    context_ids = torch.stack([b['context_ids'] for b in batch])
    # æŠŠ åªæœ‰ä¸€ä¸ªå€¼çš„ tensor æ„æˆçš„æ•°ç»„è½¬æ¢æˆ tensor
    # [torch.tensor(1), torch.tensor(1)] -> torch.tensor([1, 1])
    # (batch_size)
    target_ids = torch.stack([b['target_id'] for b in batch])
    
    # ç”Ÿæˆè´Ÿæ ·æœ¬
    # (batch_size, num_neg_samples)
    neg_ids = sample_negative_words(target_ids, vocab_size, num_neg_samples)
    
    # æ‹¼æ¥æ­£è´Ÿæ ·æœ¬ï¼š[æ­£, è´Ÿ1, è´Ÿ2, ...]
    # target_ids.unsqueeze(1) -> (batch_size, 1)
    # ï¼ˆbatch_size, 1 + K)
    input_ids = torch.cat([target_ids.unsqueeze(1), neg_ids], dim=1)
    
    # ç”Ÿæˆæ ‡ç­¾
    # ç¬¬ä¸€åˆ—æ˜¯æ­£æ ·æœ¬ (1)ï¼Œå…¶ä½™æ˜¯è´Ÿæ ·æœ¬ (0)
    labels = torch.zeros_like(input_ids, dtype=torch.float)
    labels[:, 0] = 1.0
    
    return {
        'context_ids': context_ids, # shape: (batch_size, 2*window_size)
        'input_ids': input_ids, # shape: (batch_size, 1 + K)
        'labels': labels # shape: (batch_size, 1 + K)
    }
```

è¿™æ ·ï¼Œ`Dataset` åªè´Ÿè´£æä¾›åŸå§‹æ ·æœ¬ï¼Œ`collate_fn` è´Ÿè´£â€œåŠ å·¥â€æˆè®­ç»ƒæ‰€éœ€æ ¼å¼ï¼ŒèŒè´£åˆ†æ˜ï¼

## å®æˆ˜ï¼šåœ¨PTBæ•°æ®é›†ä¸Šè®­ç»ƒ

æœ€åï¼Œæˆ‘ä»¬ç”¨çœŸå®æ•°æ®è®­ç»ƒè¿™ä¸ªå‡çº§ç‰ˆæ¨¡å‹ã€‚

PTB æ•°æ®é›†ä¸‹è½½åœ°å€ï¼š

- https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt
- https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.valid.txt
- https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.test.txt

### æ­¥éª¤ 1ï¼šVocabulary

```python
def default_tokenize(text: str) -> List[str]:
    text = text.lower()
    text = text.replace('\n', ' ')
    text = re.sub(r'([.,!?\'])', r' \1', text)
    return text.split()

class Vocabulary:
    def __init__(
            self,
            tokenizer: Callable[[str], List[str]] = default_tokenize,
            unk_token='<unk>'
        ):
        self.tokenizer = tokenizer
        self.word_to_id: Dict[str, int] = {
            unk_token: 0,
        }
        self.id_to_word: Dict[int, str] = {
            0: unk_token,
        }
        self.word_freq = defaultdict(int)
        self.unk_token = '<unk>'
        self.word_freq[unk_token] = 0

    def add_special_token(self, token: str):
        if token not in self.word_to_id:
            new_id = len(self.word_to_id)
            self.word_to_id[token] = new_id
            self.id_to_word[new_id] = token
            self.word_freq[token] = 0
        
    def tokenize(self, text: str):
        return self.tokenizer(text)
    
    def build(self, text: str, min_freq: int = 1):
        for word in self.tokenize(text):
            self.word_freq[word] += 1
            if self.word_freq[word] >= min_freq and word not in self.word_to_id:
                new_id = len(self.word_to_id)
                self.word_to_id[word] = new_id
                self.id_to_word[new_id] = word

    def build_from_sentences(self, sentences: List[str], min_freq: int = 1):
        for sentence in sentences:
            self.build(sentence, min_freq)

    def encode(self, text: str) -> List[int]:
        return [self.word_to_id.get(word, 0) for word in self.tokenize(text)]
    
    def decode(self, ids: List[int]) -> str:
        return ' '.join([self.id_to_word[id] for id in ids])
    
    @property
    def size(self):
        return len(self.word_to_id)
    
    def __len__(self):
        return self.size
```

### æ­¥éª¤ 2ï¼šPTBCbowDataset

è¿™ä¸ªç±»è´Ÿè´£ï¼š

- åŠ è½½æ–‡æœ¬
- ä½¿ç”¨ vocab è½¬ä¸º ID
- ä¸ºæ¯ä¸ªä¸­å¿ƒè¯ç”Ÿæˆ (context, target) æ ·æœ¬

```python
class PTBCBOWDataset(Dataset):
    def __init__(
        self, 
        file_path: Path,
        vocab: Vocabulary,
        window_size: int = 2,
    ):
        self.file_path = file_path
        self.window_size = window_size
        self.sentences = self._load_sentences()
        
        vocab.build_from_sentences(self.sentences)
        self.vocab = vocab

        self.sentences_ids = [self.vocab.encode(sent) for sent in self.sentences]
        self.corpus = self.create_contexts_target()


    def _load_sentences(self) -> List[str]:
        with open(self.file_path, 'r', encoding='utf-8') as f:
            sents = [line.strip() for line in f if line.strip()]
            return sents
        
    def create_contexts_target(self) -> List[Dict]:
        samples = []
        for sent_ids in self.sentences_ids:
            for idx in range(self.window_size, len(sent_ids) - self.window_size):
                context_ids = sent_ids[idx - self.window_size: idx] + sent_ids[idx + 1: idx + self.window_size + 1]
                target_id = sent_ids[idx]
                samples.append({
                    'context_ids': torch.tensor(context_ids, dtype=torch.long),
                    'target_id': torch.tensor(target_id, dtype=torch.long),
                })

        return samples
        
    
    def __len__(self):
        return len(self.corpus)
    
    def __getitem__(self, idx: int) -> Dict:
        return self.corpus[idx]
```

### æ­¥éª¤ 3ï¼šåˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨

```python
data_dir = Path.home() / 'datasets' / 'ptb'
train_file = data_dir / 'ptb.train.txt'

# 1. åŠ è½½æ•°æ®å’Œæ„å»ºè¯æ±‡è¡¨
vocab = Vocabulary()
train_dataset = PTBCBOWDataset(
    train_file,
    vocab=vocab, 
    window_size=2
)
print("è¯æ±‡è¡¨å¤§å°:", len(vocab))

# 2. åˆ›å»ºDataLoaderï¼Œä½¿ç”¨collate_fn
train_loader = DataLoader(
    train_dataset,
    batch_size=64,
    shuffle=True,
    collate_fn=lambda b: collate_fn(b, num_neg_samples=5, vocab_size=len(vocab))
)

# 3. åˆå§‹åŒ–æ¨¡å‹
model = CBOWNegativeSampling(vocab_size=len(vocab), embedding_dim=100)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.BCEWithLogitsLoss()

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)
```

### æ­¥éª¤ 4ï¼šè®­ç»ƒæ¨¡å‹

```python
for epoch in range(10):
    losses = []
    model.train()
    for batch in train_loader:
        ctx = batch['context_ids'].to(device)
        inp = batch['input_ids'].to(device)
        lbl = batch['labels'].to(device)

        logits = model(ctx, inp)
        loss = criterion(logits, lbl)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

    print(f"Epoch {epoch}, å¹³å‡æŸå¤±: {sum(losses)/len(losses):.4f}")
```

## æ€»ç»“ï¼šæˆ‘ä»¬åšäº†ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬ç»™Word2Vecæ¨¡å‹åšäº†ä¸¤æ¬¡å…³é”®å‡çº§ï¼š

1. **`nn.Embedding` â†’ é«˜æ•ˆæå–è¯å‘é‡** å‘Šåˆ«äº†ç¬¨é‡çš„ one-hot å’ŒçŸ©é˜µä¹˜æ³•ï¼ŒåƒæŸ¥å­—å…¸ä¸€æ ·å¿«é€Ÿè·å–è¯å‘é‡ã€‚

2. **è´Ÿé‡‡æ · â†’ åŒ–ç¹ä¸ºç®€çš„è®­ç»ƒç­–ç•¥** æŠŠâ€œå¤§æµ·æé’ˆâ€å¼çš„å¤šåˆ†ç±»ï¼Œå˜æˆäº†â€œçœŸå‡åˆ¤æ–­â€å¼çš„äºŒåˆ†ç±»ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—è´Ÿæ‹…ã€‚

---

**å‚è€ƒèµ„æ–™ï¼š**

- æ–‹è—¤åº·æ¯…ã€Šæ·±åº¦å­¦ä¹ è¿›é˜¶ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ã€‹
- PyTorchå®˜æ–¹æ–‡æ¡£


<nav class="pagination justify-content-between">
<a href="../1-word2vec">è‡ªç„¶è¯­è¨€å¤„ç†å…¥é—¨ï¼šä»ä¸€å¥è¯åˆ°è¯å‘é‡â€”â€”ç”¨PyTorchå®ç°Word2Vec</a>
<a href="../">ç›®å½•</a>
<a href="../3-rnn">RNNï¼šè®©ç¥ç»ç½‘ç»œå­¦ä¼šâ€œè®°ç¬”è®°â€</a>
</nav>

